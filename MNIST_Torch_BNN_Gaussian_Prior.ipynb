{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gg9NpFEcKJ7h",
    "outputId": "a633a5c0-6fc9-4f23-903f-bb3cd8266da8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchbnn in c:\\anaconda\\anaconda3\\envs\\py310\\lib\\site-packages (1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchbnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4x1hZB-iJ47m"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import torchbnn as bnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "hoNzE_BiKPlS"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "JMuxrQaRLhJR"
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "input_dim = 784\n",
    "\n",
    "# Hyperparameters\n",
    "lr = 1e-4\n",
    "log_sigma1 = -1\n",
    "hidden_dim = 1200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "3LmpHydwKSPW"
   },
   "outputs": [],
   "source": [
    "# iris = datasets.load_iris()\n",
    "training_data = datasets.MNIST(root='data', train=True, download=True, transform=ToTensor())\n",
    "training_set, validation_set = torch.utils.data.random_split(training_data, [50000, 10000])\n",
    "test_data = datasets.MNIST(root='data', train=False, download=True, transform=ToTensor())\n",
    "\n",
    "training_loader = DataLoader(training_set, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_set, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "1zMGuntBNrZ4"
   },
   "outputs": [],
   "source": [
    "num_trainbatches = int(len(training_set) / batch_size) + 1\n",
    "train_denom = 2 ** num_trainbatches  - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 675
    },
    "id": "W1t4gjGjZv5m",
    "outputId": "d950d8be-77dc-4938-9f2c-07216c627cb9"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x800 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the data\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img, label = training_data[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(label)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "# No need to normalize. The data values are already between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RMu3DnfuK02O",
    "outputId": "c4ff51f8-9bee-42fd-a780-d979210b85e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 10000\n"
     ]
    }
   ],
   "source": [
    "# print(img.shape)\n",
    "print(len(training_set),len(validation_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "h1zQZNEiK5Hn"
   },
   "outputs": [],
   "source": [
    "sigma1 = 10 ** log_sigma1\n",
    "\n",
    "model = nn.Sequential(\n",
    "    bnn.BayesLinear(prior_mu=0, prior_sigma=sigma1, in_features=input_dim, out_features=hidden_dim, bias=False),\n",
    "    nn.ReLU(),\n",
    "    bnn.BayesLinear(prior_mu=0, prior_sigma=sigma1, in_features=hidden_dim, out_features=hidden_dim, bias=False),\n",
    "    nn.ReLU(),\n",
    "    bnn.BayesLinear(prior_mu=0, prior_sigma=sigma1, in_features=hidden_dim, out_features=num_classes, bias=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "TKuZ_u5aK7O7"
   },
   "outputs": [],
   "source": [
    "ce_loss = nn.CrossEntropyLoss()\n",
    "kl_loss = bnn.BKLLoss(reduction='mean', last_layer_only=False)\n",
    "# kl_weight = 0.01\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "hTdPC-GWedSM"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(training_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "\n",
    "        inputs = inputs.view(-1, 784)\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        ################## WE CAN AVERAGE ACROSS MULTIPLE W SAMPLES HERE BEFORE DOING loss.backward() #########################\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        ce = ce_loss(outputs, labels)\n",
    "        kl = kl_loss(model)\n",
    "        kl_weight = 2 ** (num_trainbatches-i) / train_denom\n",
    "        loss = ce + kl_weight * kl\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            last_loss = running_loss / 100 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(training_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vKLhziHlfCyw",
    "outputId": "11f5cf4b-0ab3-41ef-ad68-8023e62c345f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 100 loss: 8.467106261253356\n",
      "  batch 200 loss: 7.729285206794739\n",
      "  batch 300 loss: 6.474286193847656\n",
      "LOSS train 6.474286193847656 valid 5.787351052972335\n",
      "valid accuracy 10.5\n",
      "test accuracy 10.46\n",
      "EPOCH 2:\n",
      "  batch 100 loss: 5.5274017667770385\n",
      "  batch 200 loss: 5.196687185764313\n",
      "  batch 300 loss: 4.735077016353607\n",
      "LOSS train 4.735077016353607 valid 4.328024903430214\n",
      "valid accuracy 10.03\n",
      "test accuracy 9.81\n",
      "EPOCH 3:\n",
      "  batch 100 loss: 4.325763711929321\n",
      "  batch 200 loss: 4.110598168373108\n",
      "  batch 300 loss: 3.863715200424194\n",
      "LOSS train 3.863715200424194 valid 3.6318890535378756\n",
      "valid accuracy 10.55\n",
      "test accuracy 10.05\n",
      "EPOCH 4:\n",
      "  batch 100 loss: 3.5319473552703857\n",
      "  batch 200 loss: 3.4836619186401365\n",
      "  batch 300 loss: 3.363242678642273\n",
      "LOSS train 3.363242678642273 valid 3.1684568230109877\n",
      "valid accuracy 11.39\n",
      "test accuracy 10.15\n",
      "EPOCH 5:\n",
      "  batch 100 loss: 3.203986644744873\n",
      "  batch 200 loss: 3.129839401245117\n",
      "  batch 300 loss: 3.0767604875564576\n",
      "LOSS train 3.0767604875564576 valid 2.9693228353427936\n",
      "valid accuracy 10.56\n",
      "test accuracy 10.36\n",
      "EPOCH 6:\n",
      "  batch 100 loss: 2.988020215034485\n",
      "  batch 200 loss: 2.9215537786483763\n",
      "  batch 300 loss: 2.894357306957245\n",
      "LOSS train 2.894357306957245 valid 2.842976745170883\n",
      "valid accuracy 10.58\n",
      "test accuracy 10.28\n",
      "EPOCH 7:\n",
      "  batch 100 loss: 2.827995057106018\n",
      "  batch 200 loss: 2.793685073852539\n",
      "  batch 300 loss: 2.7421823954582214\n",
      "LOSS train 2.7421823954582214 valid 2.6928961277008057\n",
      "valid accuracy 11.11\n",
      "test accuracy 10.64\n",
      "EPOCH 8:\n",
      "  batch 100 loss: 2.699779305458069\n",
      "  batch 200 loss: 2.689836003780365\n",
      "  batch 300 loss: 2.63756942987442\n",
      "LOSS train 2.63756942987442 valid 2.6213656437547903\n",
      "valid accuracy 10.39\n",
      "test accuracy 11.14\n",
      "EPOCH 9:\n",
      "  batch 100 loss: 2.618461775779724\n",
      "  batch 200 loss: 2.6254724264144897\n",
      "  batch 300 loss: 2.60443519115448\n",
      "LOSS train 2.60443519115448 valid 2.555206866203984\n",
      "valid accuracy 10.78\n",
      "test accuracy 11.13\n",
      "EPOCH 10:\n",
      "  batch 100 loss: 2.5477648854255674\n",
      "  batch 200 loss: 2.529967267513275\n",
      "  batch 300 loss: 2.5177092266082766\n",
      "LOSS train 2.5177092266082766 valid 2.533289978775797\n",
      "valid accuracy 10.32\n",
      "test accuracy 10.98\n",
      "EPOCH 11:\n",
      "  batch 100 loss: 2.5175018739700317\n",
      "  batch 200 loss: 2.4960289788246155\n",
      "  batch 300 loss: 2.4884116578102113\n",
      "LOSS train 2.4884116578102113 valid 2.481244337709644\n",
      "valid accuracy 10.6\n",
      "test accuracy 11.14\n",
      "EPOCH 12:\n",
      "  batch 100 loss: 2.468351676464081\n",
      "  batch 200 loss: 2.445245740413666\n",
      "  batch 300 loss: 2.4362464499473573\n",
      "LOSS train 2.4362464499473573 valid 2.4154253187058847\n",
      "valid accuracy 12.14\n",
      "test accuracy 11.27\n",
      "EPOCH 13:\n",
      "  batch 100 loss: 2.426516222953796\n",
      "  batch 200 loss: 2.404116680622101\n",
      "  batch 300 loss: 2.3964898347854615\n",
      "LOSS train 2.3964898347854615 valid 2.399771605865865\n",
      "valid accuracy 12.51\n",
      "test accuracy 12.12\n",
      "EPOCH 14:\n",
      "  batch 100 loss: 2.3692564010620116\n",
      "  batch 200 loss: 2.3654387164115906\n",
      "  batch 300 loss: 2.366726670265198\n",
      "LOSS train 2.366726670265198 valid 2.3317122006718116\n",
      "valid accuracy 14.21\n",
      "test accuracy 13.95\n",
      "EPOCH 15:\n",
      "  batch 100 loss: 2.319254286289215\n",
      "  batch 200 loss: 2.3018712401390076\n",
      "  batch 300 loss: 2.2750432562828062\n",
      "LOSS train 2.2750432562828062 valid 2.2594766978976093\n",
      "valid accuracy 16.52\n",
      "test accuracy 17.8\n",
      "EPOCH 16:\n",
      "  batch 100 loss: 2.2512689876556395\n",
      "  batch 200 loss: 2.2267661905288696\n",
      "  batch 300 loss: 2.18711985707283\n",
      "LOSS train 2.18711985707283 valid 2.1453092369852187\n",
      "valid accuracy 22.12\n",
      "test accuracy 21.47\n",
      "EPOCH 17:\n",
      "  batch 100 loss: 2.126837122440338\n",
      "  batch 200 loss: 2.0795959460735323\n",
      "  batch 300 loss: 2.022774342298508\n",
      "LOSS train 2.022774342298508 valid 1.9219198483455031\n",
      "valid accuracy 30.11\n",
      "test accuracy 31.54\n",
      "EPOCH 18:\n",
      "  batch 100 loss: 1.8793676066398621\n",
      "  batch 200 loss: 1.7985767936706543\n",
      "  batch 300 loss: 1.7506412696838378\n",
      "LOSS train 1.7506412696838378 valid 1.6750170067895818\n",
      "valid accuracy 36.91\n",
      "test accuracy 39.0\n",
      "EPOCH 19:\n",
      "  batch 100 loss: 1.659342986345291\n",
      "  batch 200 loss: 1.5647918021678924\n",
      "  batch 300 loss: 1.5557560729980469\n",
      "LOSS train 1.5557560729980469 valid 1.4413666257375404\n",
      "valid accuracy 45.17\n",
      "test accuracy 44.88\n",
      "EPOCH 20:\n",
      "  batch 100 loss: 1.4105396199226379\n",
      "  batch 200 loss: 1.3577442598342895\n",
      "  batch 300 loss: 1.3032909029722213\n",
      "LOSS train 1.3032909029722213 valid 1.2376171312754667\n",
      "valid accuracy 53.01\n",
      "test accuracy 54.21\n",
      "EPOCH 21:\n",
      "  batch 100 loss: 1.2039381712675095\n",
      "  batch 200 loss: 1.1651012724637986\n",
      "  batch 300 loss: 1.1101649528741837\n",
      "LOSS train 1.1101649528741837 valid 1.0776179930831813\n",
      "valid accuracy 59.42\n",
      "test accuracy 60.51\n",
      "EPOCH 22:\n",
      "  batch 100 loss: 1.0663515418767928\n",
      "  batch 200 loss: 1.0365257424116134\n",
      "  batch 300 loss: 1.0017592489719391\n",
      "LOSS train 1.0017592489719391 valid 0.9990656821033622\n",
      "valid accuracy 62.77\n",
      "test accuracy 63.93\n",
      "EPOCH 23:\n",
      "  batch 100 loss: 0.9573497968912125\n",
      "  batch 200 loss: 0.9278199273347855\n",
      "  batch 300 loss: 0.8910862952470779\n",
      "LOSS train 0.8910862952470779 valid 0.8958355233639101\n",
      "valid accuracy 66.82\n",
      "test accuracy 68.14\n",
      "EPOCH 24:\n",
      "  batch 100 loss: 0.8654210913181305\n",
      "  batch 200 loss: 0.8553579437732697\n",
      "  batch 300 loss: 0.8271088933944702\n",
      "LOSS train 0.8271088933944702 valid 0.8235014367707169\n",
      "valid accuracy 70.33\n",
      "test accuracy 72.06\n",
      "EPOCH 25:\n",
      "  batch 100 loss: 0.8045744159817696\n",
      "  batch 200 loss: 0.7940276539325715\n",
      "  batch 300 loss: 0.7622119414806366\n",
      "LOSS train 0.7622119414806366 valid 0.7345459219775622\n",
      "valid accuracy 74.14\n",
      "test accuracy 74.79\n",
      "EPOCH 26:\n",
      "  batch 100 loss: 0.7262353295087814\n",
      "  batch 200 loss: 0.7010642817616463\n",
      "  batch 300 loss: 0.7006699031591416\n",
      "LOSS train 0.7006699031591416 valid 0.6972414873823335\n",
      "valid accuracy 76.29\n",
      "test accuracy 77.16\n",
      "EPOCH 27:\n",
      "  batch 100 loss: 0.6764984413981437\n",
      "  batch 200 loss: 0.6518622252345085\n",
      "  batch 300 loss: 0.6423217806220055\n",
      "LOSS train 0.6423217806220055 valid 0.6343692824055877\n",
      "valid accuracy 78.73\n",
      "test accuracy 79.27\n",
      "EPOCH 28:\n",
      "  batch 100 loss: 0.6276376461982727\n",
      "  batch 200 loss: 0.6217032393813133\n",
      "  batch 300 loss: 0.5991026273369789\n",
      "LOSS train 0.5991026273369789 valid 0.5762891561924657\n",
      "valid accuracy 80.72\n",
      "test accuracy 80.96\n",
      "EPOCH 29:\n",
      "  batch 100 loss: 0.586835709810257\n",
      "  batch 200 loss: 0.5569375100731849\n",
      "  batch 300 loss: 0.530042354464531\n",
      "LOSS train 0.530042354464531 valid 0.5423470414892028\n",
      "valid accuracy 82.42\n",
      "test accuracy 83.48\n",
      "EPOCH 30:\n",
      "  batch 100 loss: 0.5150617837905884\n",
      "  batch 200 loss: 0.5242828980088234\n",
      "  batch 300 loss: 0.5143486505746842\n",
      "LOSS train 0.5143486505746842 valid 0.5008870800839195\n",
      "valid accuracy 83.98\n",
      "test accuracy 84.15\n",
      "EPOCH 31:\n",
      "  batch 100 loss: 0.49210732251405714\n",
      "  batch 200 loss: 0.49186802059412005\n",
      "  batch 300 loss: 0.49758401691913606\n",
      "LOSS train 0.49758401691913606 valid 0.481741343495212\n",
      "valid accuracy 84.71\n",
      "test accuracy 85.91\n",
      "EPOCH 32:\n",
      "  batch 100 loss: 0.4677382344007492\n",
      "  batch 200 loss: 0.46178627967834474\n",
      "  batch 300 loss: 0.46529205948114394\n",
      "LOSS train 0.46529205948114394 valid 0.4532830522784704\n",
      "valid accuracy 86.27\n",
      "test accuracy 86.18\n",
      "EPOCH 33:\n",
      "  batch 100 loss: 0.4642804569005966\n",
      "  batch 200 loss: 0.44067158058285716\n",
      "  batch 300 loss: 0.4320376744866371\n",
      "LOSS train 0.4320376744866371 valid 0.44412370535391793\n",
      "valid accuracy 86.28\n",
      "test accuracy 87.01\n",
      "EPOCH 34:\n",
      "  batch 100 loss: 0.42854432955384253\n",
      "  batch 200 loss: 0.4037574638426304\n",
      "  batch 300 loss: 0.41127571403980256\n",
      "LOSS train 0.41127571403980256 valid 0.41524217041987405\n",
      "valid accuracy 87.5\n",
      "test accuracy 87.56\n",
      "EPOCH 35:\n",
      "  batch 100 loss: 0.402964725792408\n",
      "  batch 200 loss: 0.4002625839412212\n",
      "  batch 300 loss: 0.38270970731973647\n",
      "LOSS train 0.38270970731973647 valid 0.39664882893049264\n",
      "valid accuracy 88.22\n",
      "test accuracy 88.4\n",
      "EPOCH 36:\n",
      "  batch 100 loss: 0.3937715676426887\n",
      "  batch 200 loss: 0.38263023883104325\n",
      "  batch 300 loss: 0.37854940608143806\n",
      "LOSS train 0.37854940608143806 valid 0.3652581631005565\n",
      "valid accuracy 89.0\n",
      "test accuracy 88.26\n",
      "EPOCH 37:\n",
      "  batch 100 loss: 0.3586979222297668\n",
      "  batch 200 loss: 0.35199264287948606\n",
      "  batch 300 loss: 0.34951883777976034\n",
      "LOSS train 0.34951883777976034 valid 0.3500606881666787\n",
      "valid accuracy 89.45\n",
      "test accuracy 89.73\n",
      "EPOCH 38:\n",
      "  batch 100 loss: 0.35255842208862304\n",
      "  batch 200 loss: 0.35397676229476926\n",
      "  batch 300 loss: 0.3327064102888107\n",
      "LOSS train 0.3327064102888107 valid 0.33077998685685894\n",
      "valid accuracy 89.95\n",
      "test accuracy 89.64\n",
      "EPOCH 39:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 100 loss: 0.3405824040621519\n",
      "  batch 200 loss: 0.32515482932329176\n",
      "  batch 300 loss: 0.3469628955423832\n",
      "LOSS train 0.3469628955423832 valid 0.34347862816309627\n",
      "valid accuracy 89.61\n",
      "test accuracy 90.05\n",
      "EPOCH 40:\n",
      "  batch 100 loss: 0.31439870908856393\n",
      "  batch 200 loss: 0.31381992556154725\n",
      "  batch 300 loss: 0.3244291864335537\n",
      "LOSS train 0.3244291864335537 valid 0.32172788981395434\n",
      "valid accuracy 90.27\n",
      "test accuracy 90.59\n",
      "EPOCH 41:\n",
      "  batch 100 loss: 0.31904579684138296\n",
      "  batch 200 loss: 0.3089143507182598\n",
      "  batch 300 loss: 0.3157603707164526\n",
      "LOSS train 0.3157603707164526 valid 0.3115287959764275\n",
      "valid accuracy 90.59\n",
      "test accuracy 90.83\n",
      "EPOCH 42:\n",
      "  batch 100 loss: 0.2899281112104654\n",
      "  batch 200 loss: 0.3026174530386925\n",
      "  batch 300 loss: 0.30889287203550336\n",
      "LOSS train 0.30889287203550336 valid 0.29761605851257905\n",
      "valid accuracy 91.21\n",
      "test accuracy 91.02\n",
      "EPOCH 43:\n",
      "  batch 100 loss: 0.3015387736260891\n",
      "  batch 200 loss: 0.2971583868563175\n",
      "  batch 300 loss: 0.2884974719583988\n",
      "LOSS train 0.2884974719583988 valid 0.29723529102681556\n",
      "valid accuracy 91.4\n",
      "test accuracy 91.47\n",
      "EPOCH 44:\n",
      "  batch 100 loss: 0.29887463472783565\n",
      "  batch 200 loss: 0.27742114551365377\n",
      "  batch 300 loss: 0.2758399403095245\n",
      "LOSS train 0.2758399403095245 valid 0.2862364016944849\n",
      "valid accuracy 91.63\n",
      "test accuracy 91.28\n",
      "EPOCH 45:\n",
      "  batch 100 loss: 0.29143232822418214\n",
      "  batch 200 loss: 0.26063493855297565\n",
      "  batch 300 loss: 0.2735494442284107\n",
      "LOSS train 0.2735494442284107 valid 0.2846865762449518\n",
      "valid accuracy 91.8\n",
      "test accuracy 91.75\n",
      "EPOCH 46:\n",
      "  batch 100 loss: 0.2742087845504284\n",
      "  batch 200 loss: 0.24389284878969192\n",
      "  batch 300 loss: 0.26495583705604075\n",
      "LOSS train 0.26495583705604075 valid 0.27415668728608117\n",
      "valid accuracy 91.85\n",
      "test accuracy 92.11\n",
      "EPOCH 47:\n",
      "  batch 100 loss: 0.2533370108157396\n",
      "  batch 200 loss: 0.25053641229867935\n",
      "  batch 300 loss: 0.24754915349185466\n",
      "LOSS train 0.24754915349185466 valid 0.26150415950938116\n",
      "valid accuracy 92.38\n",
      "test accuracy 92.41\n",
      "EPOCH 48:\n",
      "  batch 100 loss: 0.259760464578867\n",
      "  batch 200 loss: 0.24484063923358917\n",
      "  batch 300 loss: 0.24734070606529712\n",
      "LOSS train 0.24734070606529712 valid 0.25412894709955286\n",
      "valid accuracy 92.54\n",
      "test accuracy 92.73\n",
      "EPOCH 49:\n",
      "  batch 100 loss: 0.24757377311587334\n",
      "  batch 200 loss: 0.22373734220862387\n",
      "  batch 300 loss: 0.24679047510027885\n",
      "LOSS train 0.24679047510027885 valid 0.2520834349944622\n",
      "valid accuracy 92.78\n",
      "test accuracy 92.71\n",
      "EPOCH 50:\n",
      "  batch 100 loss: 0.22974520146846772\n",
      "  batch 200 loss: 0.22185529232025147\n",
      "  batch 300 loss: 0.23957710091024637\n",
      "LOSS train 0.23957710091024637 valid 0.24881950011358986\n",
      "valid accuracy 92.8\n",
      "test accuracy 92.87\n",
      "EPOCH 51:\n",
      "  batch 100 loss: 0.23927010126411916\n",
      "  batch 200 loss: 0.227386789098382\n",
      "  batch 300 loss: 0.2248650236800313\n",
      "LOSS train 0.2248650236800313 valid 0.24376037047256396\n",
      "valid accuracy 93.15\n",
      "test accuracy 93.17\n",
      "EPOCH 52:\n",
      "  batch 100 loss: 0.23055927485227584\n",
      "  batch 200 loss: 0.2172206800431013\n",
      "  batch 300 loss: 0.22896783381700517\n",
      "LOSS train 0.22896783381700517 valid 0.23617511120023607\n",
      "valid accuracy 93.31\n",
      "test accuracy 93.35\n",
      "EPOCH 53:\n",
      "  batch 100 loss: 0.2182790818065405\n",
      "  batch 200 loss: 0.21878240890800954\n",
      "  batch 300 loss: 0.21444912061095237\n",
      "LOSS train 0.21444912061095237 valid 0.2350525837155837\n",
      "valid accuracy 93.29\n",
      "test accuracy 93.48\n",
      "EPOCH 54:\n",
      "  batch 100 loss: 0.2328341194242239\n",
      "  batch 200 loss: 0.2088908213376999\n",
      "  batch 300 loss: 0.2096692045778036\n",
      "LOSS train 0.2096692045778036 valid 0.22245845179769058\n",
      "valid accuracy 93.21\n",
      "test accuracy 93.46\n",
      "EPOCH 55:\n",
      "  batch 100 loss: 0.21685332916676997\n",
      "  batch 200 loss: 0.20474829133599998\n",
      "  batch 300 loss: 0.20799614012241363\n",
      "LOSS train 0.20799614012241363 valid 0.2309705980499334\n",
      "valid accuracy 93.49\n",
      "test accuracy 93.74\n",
      "EPOCH 56:\n",
      "  batch 100 loss: 0.2057562617957592\n",
      "  batch 200 loss: 0.1950385558977723\n",
      "  batch 300 loss: 0.1959999728947878\n",
      "LOSS train 0.1959999728947878 valid 0.2122075464340705\n",
      "valid accuracy 93.77\n",
      "test accuracy 93.86\n",
      "EPOCH 57:\n",
      "  batch 100 loss: 0.20551572054624556\n",
      "  batch 200 loss: 0.19878160282969476\n",
      "  batch 300 loss: 0.18142341569066048\n",
      "LOSS train 0.18142341569066048 valid 0.21257817151048516\n",
      "valid accuracy 93.73\n",
      "test accuracy 93.87\n",
      "EPOCH 58:\n",
      "  batch 100 loss: 0.19745315805077554\n",
      "  batch 200 loss: 0.18958317749202253\n",
      "  batch 300 loss: 0.1808831039816141\n",
      "LOSS train 0.1808831039816141 valid 0.2145508530113516\n",
      "valid accuracy 93.73\n",
      "test accuracy 93.79\n",
      "EPOCH 59:\n",
      "  batch 100 loss: 0.18497127436101438\n",
      "  batch 200 loss: 0.1927508854866028\n",
      "  batch 300 loss: 0.18900742888450622\n",
      "LOSS train 0.18900742888450622 valid 0.20976303192444995\n",
      "valid accuracy 93.82\n",
      "test accuracy 93.87\n",
      "EPOCH 60:\n",
      "  batch 100 loss: 0.181727407425642\n",
      "  batch 200 loss: 0.17472372304648162\n",
      "  batch 300 loss: 0.18954826284199952\n",
      "LOSS train 0.18954826284199952 valid 0.1971342080756079\n",
      "valid accuracy 94.22\n",
      "test accuracy 94.14\n",
      "EPOCH 61:\n",
      "  batch 100 loss: 0.18247440356761216\n",
      "  batch 200 loss: 0.17865599475800992\n",
      "  batch 300 loss: 0.1794711822271347\n",
      "LOSS train 0.1794711822271347 valid 0.20368509725490702\n",
      "valid accuracy 93.77\n",
      "test accuracy 94.5\n",
      "EPOCH 62:\n",
      "  batch 100 loss: 0.17473866924643516\n",
      "  batch 200 loss: 0.17253664609044791\n",
      "  batch 300 loss: 0.16653905555605888\n",
      "LOSS train 0.16653905555605888 valid 0.20632991430502903\n",
      "valid accuracy 93.97\n",
      "test accuracy 94.42\n",
      "EPOCH 63:\n",
      "  batch 100 loss: 0.17679383374750615\n",
      "  batch 200 loss: 0.16659916292876006\n",
      "  batch 300 loss: 0.15618692088872194\n",
      "LOSS train 0.15618692088872194 valid 0.19504215063739427\n",
      "valid accuracy 94.39\n",
      "test accuracy 94.49\n",
      "EPOCH 64:\n",
      "  batch 100 loss: 0.18016177892684937\n",
      "  batch 200 loss: 0.16245453825220466\n",
      "  batch 300 loss: 0.16298746906220912\n",
      "LOSS train 0.16298746906220912 valid 0.18771810145883622\n",
      "valid accuracy 94.56\n",
      "test accuracy 94.62\n",
      "EPOCH 65:\n",
      "  batch 100 loss: 0.17047480247914792\n",
      "  batch 200 loss: 0.16507125977426768\n",
      "  batch 300 loss: 0.16309895109385253\n",
      "LOSS train 0.16309895109385253 valid 0.18960731283207483\n",
      "valid accuracy 94.61\n",
      "test accuracy 94.76\n",
      "EPOCH 66:\n",
      "  batch 100 loss: 0.16426168598234653\n",
      "  batch 200 loss: 0.16396985329687597\n",
      "  batch 300 loss: 0.16240451615303755\n",
      "LOSS train 0.16240451615303755 valid 0.19048377998833416\n",
      "valid accuracy 94.33\n",
      "test accuracy 94.77\n",
      "EPOCH 67:\n",
      "  batch 100 loss: 0.16801818706095217\n",
      "  batch 200 loss: 0.15863330652937294\n",
      "  batch 300 loss: 0.15883612133562564\n",
      "LOSS train 0.15883612133562564 valid 0.18324968431003485\n",
      "valid accuracy 94.55\n",
      "test accuracy 94.81\n",
      "EPOCH 68:\n",
      "  batch 100 loss: 0.16092735823243856\n",
      "  batch 200 loss: 0.1482179418578744\n",
      "  batch 300 loss: 0.15717624433338642\n",
      "LOSS train 0.15717624433338642 valid 0.18753848583260668\n",
      "valid accuracy 94.45\n",
      "test accuracy 95.02\n",
      "EPOCH 69:\n",
      "  batch 100 loss: 0.15057378083467485\n",
      "  batch 200 loss: 0.15867484197020532\n",
      "  batch 300 loss: 0.1555038571357727\n",
      "LOSS train 0.1555038571357727 valid 0.18646181049414828\n",
      "valid accuracy 94.7\n",
      "test accuracy 95.1\n",
      "EPOCH 70:\n",
      "  batch 100 loss: 0.14594653625041246\n",
      "  batch 200 loss: 0.13781490683555603\n",
      "  batch 300 loss: 0.15952833782881498\n",
      "LOSS train 0.15952833782881498 valid 0.1732301298481754\n",
      "valid accuracy 94.87\n",
      "test accuracy 95.0\n",
      "EPOCH 71:\n",
      "  batch 100 loss: 0.13852198109030722\n",
      "  batch 200 loss: 0.1484192118421197\n",
      "  batch 300 loss: 0.13816757209599018\n",
      "LOSS train 0.13816757209599018 valid 0.18318066665832\n",
      "valid accuracy 94.69\n",
      "test accuracy 95.21\n",
      "EPOCH 72:\n",
      "  batch 100 loss: 0.13565015867352487\n",
      "  batch 200 loss: 0.1449632941558957\n",
      "  batch 300 loss: 0.15788102224469186\n",
      "LOSS train 0.15788102224469186 valid 0.17624191114608245\n",
      "valid accuracy 94.95\n",
      "test accuracy 95.2\n",
      "EPOCH 73:\n",
      "  batch 100 loss: 0.137331698872149\n",
      "  batch 200 loss: 0.14671051617711783\n",
      "  batch 300 loss: 0.1332084308564663\n",
      "LOSS train 0.1332084308564663 valid 0.16539256152095674\n",
      "valid accuracy 95.05\n",
      "test accuracy 95.24\n",
      "EPOCH 74:\n",
      "  batch 100 loss: 0.14575214920565485\n",
      "  batch 200 loss: 0.13363125830888747\n",
      "  batch 300 loss: 0.12780490513890982\n",
      "LOSS train 0.12780490513890982 valid 0.1720456721001788\n",
      "valid accuracy 95.01\n",
      "test accuracy 95.56\n",
      "EPOCH 75:\n",
      "  batch 100 loss: 0.1352653113566339\n",
      "  batch 200 loss: 0.13757484391331673\n",
      "  batch 300 loss: 0.12748919639736414\n",
      "LOSS train 0.12748919639736414 valid 0.16703110768259327\n",
      "valid accuracy 95.28\n",
      "test accuracy 95.34\n",
      "EPOCH 76:\n",
      "  batch 100 loss: 0.12619690378196538\n",
      "  batch 200 loss: 0.13238897982984782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 300 loss: 0.13597683217376472\n",
      "LOSS train 0.13597683217376472 valid 0.16532308657806885\n",
      "valid accuracy 95.25\n",
      "test accuracy 95.44\n",
      "EPOCH 77:\n",
      "  batch 100 loss: 0.1261585591547191\n",
      "  batch 200 loss: 0.12387018680572509\n",
      "  batch 300 loss: 0.13945111490786075\n",
      "LOSS train 0.13945111490786075 valid 0.15942684136614016\n",
      "valid accuracy 95.26\n",
      "test accuracy 95.48\n",
      "EPOCH 78:\n",
      "  batch 100 loss: 0.1327340155467391\n",
      "  batch 200 loss: 0.128977462798357\n",
      "  batch 300 loss: 0.12854004051536322\n",
      "LOSS train 0.12854004051536322 valid 0.1633692593608476\n",
      "valid accuracy 95.23\n",
      "test accuracy 95.51\n",
      "EPOCH 79:\n",
      "  batch 100 loss: 0.12759467300027608\n",
      "  batch 200 loss: 0.11765244707465172\n",
      "  batch 300 loss: 0.12784759737551213\n",
      "LOSS train 0.12784759737551213 valid 0.16759626065156882\n",
      "valid accuracy 95.27\n",
      "test accuracy 95.68\n",
      "EPOCH 80:\n",
      "  batch 100 loss: 0.12276397300884127\n",
      "  batch 200 loss: 0.12479426080361009\n",
      "  batch 300 loss: 0.12020181640982627\n",
      "LOSS train 0.12020181640982627 valid 0.16250184607468074\n",
      "valid accuracy 95.38\n",
      "test accuracy 95.85\n",
      "EPOCH 81:\n",
      "  batch 100 loss: 0.12055385764688253\n",
      "  batch 200 loss: 0.12034870993345975\n",
      "  batch 300 loss: 0.12151675932109356\n",
      "LOSS train 0.12151675932109356 valid 0.15653978788022754\n",
      "valid accuracy 95.72\n",
      "test accuracy 95.7\n",
      "EPOCH 82:\n",
      "  batch 100 loss: 0.1260637944377959\n",
      "  batch 200 loss: 0.11955849042162299\n",
      "  batch 300 loss: 0.11807506062090396\n",
      "LOSS train 0.11807506062090396 valid 0.15574029828362826\n",
      "valid accuracy 95.81\n",
      "test accuracy 95.57\n",
      "EPOCH 83:\n",
      "  batch 100 loss: 0.12085397224873304\n",
      "  batch 200 loss: 0.11278701443225145\n",
      "  batch 300 loss: 0.1251962548494339\n",
      "LOSS train 0.1251962548494339 valid 0.14915150130474114\n",
      "valid accuracy 95.78\n",
      "test accuracy 95.78\n",
      "EPOCH 84:\n",
      "  batch 100 loss: 0.1238021034002304\n",
      "  batch 200 loss: 0.11945100624114274\n",
      "  batch 300 loss: 0.11223083372227848\n",
      "LOSS train 0.11223083372227848 valid 0.1467284355076808\n",
      "valid accuracy 95.84\n",
      "test accuracy 96.31\n",
      "EPOCH 85:\n",
      "  batch 100 loss: 0.11164066338911653\n",
      "  batch 200 loss: 0.11365366162732243\n",
      "  batch 300 loss: 0.10957066783681511\n",
      "LOSS train 0.10957066783681511 valid 0.14943936783113057\n",
      "valid accuracy 95.79\n",
      "test accuracy 96.15\n",
      "EPOCH 86:\n",
      "  batch 100 loss: 0.113880656901747\n",
      "  batch 200 loss: 0.10981108825653792\n",
      "  batch 300 loss: 0.11685197923332452\n",
      "LOSS train 0.11685197923332452 valid 0.1513877993190213\n",
      "valid accuracy 95.83\n",
      "test accuracy 95.8\n",
      "EPOCH 87:\n",
      "  batch 100 loss: 0.10881256675347686\n",
      "  batch 200 loss: 0.11393812570720911\n",
      "  batch 300 loss: 0.10516315625980496\n",
      "LOSS train 0.10516315625980496 valid 0.14413661342349987\n",
      "valid accuracy 95.85\n",
      "test accuracy 96.03\n",
      "EPOCH 88:\n",
      "  batch 100 loss: 0.10971571203321219\n",
      "  batch 200 loss: 0.11785507570952176\n",
      "  batch 300 loss: 0.11020746795460581\n",
      "LOSS train 0.11020746795460581 valid 0.15236288233648373\n",
      "valid accuracy 95.81\n",
      "test accuracy 95.9\n",
      "EPOCH 89:\n",
      "  batch 100 loss: 0.10505272477865218\n",
      "  batch 200 loss: 0.11551929568871856\n",
      "  batch 300 loss: 0.10759785564616323\n",
      "LOSS train 0.10759785564616323 valid 0.14566054850628105\n",
      "valid accuracy 95.91\n",
      "test accuracy 96.19\n",
      "EPOCH 90:\n",
      "  batch 100 loss: 0.09844111114740371\n",
      "  batch 200 loss: 0.10366510979831219\n",
      "  batch 300 loss: 0.10632252523675562\n",
      "LOSS train 0.10632252523675562 valid 0.1486751178067319\n",
      "valid accuracy 95.73\n",
      "test accuracy 96.37\n",
      "EPOCH 91:\n",
      "  batch 100 loss: 0.10099636967293918\n",
      "  batch 200 loss: 0.09835516745224596\n",
      "  batch 300 loss: 0.11187150562182069\n",
      "LOSS train 0.11187150562182069 valid 0.14565708705141575\n",
      "valid accuracy 95.93\n",
      "test accuracy 96.04\n",
      "EPOCH 92:\n",
      "  batch 100 loss: 0.10317296888679266\n",
      "  batch 200 loss: 0.10463334634900093\n",
      "  batch 300 loss: 0.0942549907322973\n",
      "LOSS train 0.0942549907322973 valid 0.14423191152465872\n",
      "valid accuracy 95.84\n",
      "test accuracy 96.27\n",
      "EPOCH 93:\n",
      "  batch 100 loss: 0.10419694323092699\n",
      "  batch 200 loss: 0.10098693151026965\n",
      "  batch 300 loss: 0.10526336960494519\n",
      "LOSS train 0.10526336960494519 valid 0.1441219858589429\n",
      "valid accuracy 95.76\n",
      "test accuracy 96.34\n",
      "EPOCH 94:\n",
      "  batch 100 loss: 0.09935349704697728\n",
      "  batch 200 loss: 0.09952221282757819\n",
      "  batch 300 loss: 0.09657284934073687\n",
      "LOSS train 0.09657284934073687 valid 0.1463764336290239\n",
      "valid accuracy 95.9\n",
      "test accuracy 96.53\n",
      "EPOCH 95:\n",
      "  batch 100 loss: 0.08893052374944091\n",
      "  batch 200 loss: 0.09647125309333206\n",
      "  batch 300 loss: 0.09211424550041557\n",
      "LOSS train 0.09211424550041557 valid 0.14891141928921017\n",
      "valid accuracy 96.1\n",
      "test accuracy 96.55\n",
      "EPOCH 96:\n",
      "  batch 100 loss: 0.0964410306327045\n",
      "  batch 200 loss: 0.09639419417828321\n",
      "  batch 300 loss: 0.08961131802527234\n",
      "LOSS train 0.08961131802527234 valid 0.14078680331571192\n",
      "valid accuracy 95.89\n",
      "test accuracy 96.04\n",
      "EPOCH 97:\n",
      "  batch 100 loss: 0.09423745328560472\n",
      "  batch 200 loss: 0.09094980695284903\n",
      "  batch 300 loss: 0.08186751345172524\n",
      "LOSS train 0.08186751345172524 valid 0.145689974527193\n",
      "valid accuracy 95.91\n",
      "test accuracy 96.46\n",
      "EPOCH 98:\n",
      "  batch 100 loss: 0.09590286804363131\n",
      "  batch 200 loss: 0.09002376958727837\n",
      "  batch 300 loss: 0.09480077845044434\n",
      "LOSS train 0.09480077845044434 valid 0.13559770690206485\n",
      "valid accuracy 96.3\n",
      "test accuracy 96.31\n",
      "EPOCH 99:\n",
      "  batch 100 loss: 0.09379209046252072\n",
      "  batch 200 loss: 0.09234330520033836\n",
      "  batch 300 loss: 0.09234920254908502\n",
      "LOSS train 0.09234920254908502 valid 0.13607170823018386\n",
      "valid accuracy 96.3\n",
      "test accuracy 96.54\n",
      "EPOCH 100:\n",
      "  batch 100 loss: 0.09332292845472694\n",
      "  batch 200 loss: 0.0908535448461771\n",
      "  batch 300 loss: 0.08722481271252036\n",
      "LOSS train 0.08722481271252036 valid 0.14277621371600824\n",
      "valid accuracy 96.03\n",
      "test accuracy 96.71\n",
      "EPOCH 101:\n",
      "  batch 100 loss: 0.09218123149126768\n",
      "  batch 200 loss: 0.09090458745136858\n",
      "  batch 300 loss: 0.09035551513545215\n",
      "LOSS train 0.09035551513545215 valid 0.13821606366317482\n",
      "valid accuracy 96.2\n",
      "test accuracy 96.36\n",
      "EPOCH 102:\n",
      "  batch 100 loss: 0.08900631416589022\n",
      "  batch 200 loss: 0.08698058622889221\n",
      "  batch 300 loss: 0.09303679946810008\n",
      "LOSS train 0.09303679946810008 valid 0.1344086917329438\n",
      "valid accuracy 96.16\n",
      "test accuracy 96.62\n",
      "EPOCH 103:\n",
      "  batch 100 loss: 0.08707110149785877\n",
      "  batch 200 loss: 0.08463837202638387\n",
      "  batch 300 loss: 0.08067613693885506\n",
      "LOSS train 0.08067613693885506 valid 0.1315957605013553\n",
      "valid accuracy 96.36\n",
      "test accuracy 96.38\n",
      "EPOCH 104:\n",
      "  batch 100 loss: 0.0870189464930445\n",
      "  batch 200 loss: 0.07882477257400751\n",
      "  batch 300 loss: 0.07454696227796376\n",
      "LOSS train 0.07454696227796376 valid 0.1425121858055833\n",
      "valid accuracy 96.02\n",
      "test accuracy 96.69\n",
      "EPOCH 105:\n",
      "  batch 100 loss: 0.07940891867969185\n",
      "  batch 200 loss: 0.07792996651493013\n",
      "  batch 300 loss: 0.09044197969138622\n",
      "LOSS train 0.09044197969138622 valid 0.13193609004345122\n",
      "valid accuracy 96.34\n",
      "test accuracy 96.75\n",
      "EPOCH 106:\n",
      "  batch 100 loss: 0.07661130543332546\n",
      "  batch 200 loss: 0.0914071486517787\n",
      "  batch 300 loss: 0.08133148336783051\n",
      "LOSS train 0.08133148336783051 valid 0.13703724878686893\n",
      "valid accuracy 96.37\n",
      "test accuracy 96.55\n",
      "EPOCH 107:\n",
      "  batch 100 loss: 0.08409617975354194\n",
      "  batch 200 loss: 0.08320078982040285\n",
      "  batch 300 loss: 0.0766367944329977\n",
      "LOSS train 0.0766367944329977 valid 0.12880134028441545\n",
      "valid accuracy 96.34\n",
      "test accuracy 96.49\n",
      "EPOCH 108:\n",
      "  batch 100 loss: 0.07675596632063389\n",
      "  batch 200 loss: 0.07708971256390214\n",
      "  batch 300 loss: 0.08157345314510167\n",
      "LOSS train 0.08157345314510167 valid 0.13465808907264395\n",
      "valid accuracy 96.35\n",
      "test accuracy 96.58\n",
      "EPOCH 109:\n",
      "  batch 100 loss: 0.08407333992421627\n",
      "  batch 200 loss: 0.07410065119154752\n",
      "  batch 300 loss: 0.07741460805060342\n",
      "LOSS train 0.07741460805060342 valid 0.13147700164184162\n",
      "valid accuracy 96.46\n",
      "test accuracy 96.48\n",
      "EPOCH 110:\n",
      "  batch 100 loss: 0.07564230447635055\n",
      "  batch 200 loss: 0.07450008418643847\n",
      "  batch 300 loss: 0.08014633255079388\n",
      "LOSS train 0.08014633255079388 valid 0.13132449406894703\n",
      "valid accuracy 96.36\n",
      "test accuracy 96.65\n",
      "EPOCH 111:\n",
      "  batch 100 loss: 0.07187380714341998\n",
      "  batch 200 loss: 0.07586875258944929\n",
      "  batch 300 loss: 0.07660992298275232\n",
      "LOSS train 0.07660992298275232 valid 0.12260138805767026\n",
      "valid accuracy 96.71\n",
      "test accuracy 96.77\n",
      "EPOCH 112:\n",
      "  batch 100 loss: 0.07850545179098845\n",
      "  batch 200 loss: 0.07549947303254158\n",
      "  batch 300 loss: 0.0733191491290927\n",
      "LOSS train 0.0733191491290927 valid 0.12597936005154742\n",
      "valid accuracy 96.63\n",
      "test accuracy 96.8\n",
      "EPOCH 113:\n",
      "  batch 100 loss: 0.07454243429936469\n",
      "  batch 200 loss: 0.06779026756063104\n",
      "  batch 300 loss: 0.07042460255324841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 0.07042460255324841 valid 0.12676653382663108\n",
      "valid accuracy 96.49\n",
      "test accuracy 96.65\n",
      "EPOCH 114:\n",
      "  batch 100 loss: 0.07287027838174254\n",
      "  batch 200 loss: 0.0692017442267388\n",
      "  batch 300 loss: 0.07165088157635183\n",
      "LOSS train 0.07165088157635183 valid 0.12910244830212073\n",
      "valid accuracy 96.61\n",
      "test accuracy 96.91\n",
      "EPOCH 115:\n",
      "  batch 100 loss: 0.07003947709687054\n",
      "  batch 200 loss: 0.0721826409175992\n",
      "  batch 300 loss: 0.06971828592941165\n",
      "LOSS train 0.06971828592941165 valid 0.13109097528401054\n",
      "valid accuracy 96.5\n",
      "test accuracy 96.5\n",
      "EPOCH 116:\n",
      "  batch 100 loss: 0.07010542198084295\n",
      "  batch 200 loss: 0.06810183549299836\n",
      "  batch 300 loss: 0.07507767162285744\n",
      "LOSS train 0.07507767162285744 valid 0.13080612600698502\n",
      "valid accuracy 96.54\n",
      "test accuracy 96.86\n",
      "EPOCH 117:\n",
      "  batch 100 loss: 0.07851764380931854\n",
      "  batch 200 loss: 0.055548345828428866\n",
      "  batch 300 loss: 0.06312727817334235\n",
      "LOSS train 0.06312727817334235 valid 0.13040458021944837\n",
      "valid accuracy 96.62\n",
      "test accuracy 96.94\n",
      "EPOCH 118:\n",
      "  batch 100 loss: 0.07295939253643155\n",
      "  batch 200 loss: 0.06450458658626303\n",
      "  batch 300 loss: 0.06678039304912091\n",
      "LOSS train 0.06678039304912091 valid 0.12303623151552828\n",
      "valid accuracy 96.61\n",
      "test accuracy 97.01\n",
      "EPOCH 119:\n",
      "  batch 100 loss: 0.06959882403723895\n",
      "  batch 200 loss: 0.06604049854911864\n",
      "  batch 300 loss: 0.07113529660739004\n",
      "LOSS train 0.07113529660739004 valid 0.12639458218730892\n",
      "valid accuracy 96.64\n",
      "test accuracy 96.81\n",
      "EPOCH 120:\n",
      "  batch 100 loss: 0.061195845697075126\n",
      "  batch 200 loss: 0.06863413440063595\n",
      "  batch 300 loss: 0.06355071515776217\n",
      "LOSS train 0.06355071515776217 valid 0.12665866856475042\n",
      "valid accuracy 96.84\n",
      "test accuracy 96.93\n",
      "EPOCH 121:\n",
      "  batch 100 loss: 0.05822005558758974\n",
      "  batch 200 loss: 0.0637003087811172\n",
      "  batch 300 loss: 0.06243266419041902\n",
      "LOSS train 0.06243266419041902 valid 0.12457746654583872\n",
      "valid accuracy 96.67\n",
      "test accuracy 97.19\n",
      "EPOCH 122:\n",
      "  batch 100 loss: 0.06204810771159828\n",
      "  batch 200 loss: 0.06746183296199888\n",
      "  batch 300 loss: 0.054838241608813405\n",
      "LOSS train 0.054838241608813405 valid 0.12129508876064911\n",
      "valid accuracy 96.7\n",
      "test accuracy 97.11\n",
      "EPOCH 123:\n",
      "  batch 100 loss: 0.06552925321506337\n",
      "  batch 200 loss: 0.06730277457507328\n",
      "  batch 300 loss: 0.06635959311388433\n",
      "LOSS train 0.06635959311388433 valid 0.12146054382459555\n",
      "valid accuracy 96.81\n",
      "test accuracy 96.92\n",
      "EPOCH 124:\n",
      "  batch 100 loss: 0.06306935630738736\n",
      "  batch 200 loss: 0.05743649152573198\n",
      "  batch 300 loss: 0.060925637343898414\n",
      "LOSS train 0.060925637343898414 valid 0.12339239347065929\n",
      "valid accuracy 96.76\n",
      "test accuracy 96.84\n",
      "EPOCH 125:\n",
      "  batch 100 loss: 0.06436497529037297\n",
      "  batch 200 loss: 0.06974486852064729\n",
      "  batch 300 loss: 0.05653925075661391\n",
      "LOSS train 0.05653925075661391 valid 0.11611943704347256\n",
      "valid accuracy 96.9\n",
      "test accuracy 96.98\n",
      "EPOCH 126:\n",
      "  batch 100 loss: 0.06003316333051771\n",
      "  batch 200 loss: 0.059443927807733415\n",
      "  batch 300 loss: 0.05978769658133388\n",
      "LOSS train 0.05978769658133388 valid 0.12267784832658458\n",
      "valid accuracy 96.77\n",
      "test accuracy 97.05\n",
      "EPOCH 127:\n",
      "  batch 100 loss: 0.06016860598232597\n",
      "  batch 200 loss: 0.06010350029915571\n",
      "  batch 300 loss: 0.06236362313386053\n",
      "LOSS train 0.06236362313386053 valid 0.11828706965817115\n",
      "valid accuracy 96.81\n",
      "test accuracy 97.01\n",
      "EPOCH 128:\n",
      "  batch 100 loss: 0.06260313024744392\n",
      "  batch 200 loss: 0.057742190258577464\n",
      "  batch 300 loss: 0.060753019424155355\n",
      "LOSS train 0.060753019424155355 valid 0.11448995752519445\n",
      "valid accuracy 97.01\n",
      "test accuracy 97.13\n",
      "EPOCH 129:\n",
      "  batch 100 loss: 0.05319138477323577\n",
      "  batch 200 loss: 0.06947217916604131\n",
      "  batch 300 loss: 0.05628879284719005\n",
      "LOSS train 0.05628879284719005 valid 0.12122879568723184\n",
      "valid accuracy 96.87\n",
      "test accuracy 97.19\n",
      "EPOCH 130:\n",
      "  batch 100 loss: 0.06001300655771047\n",
      "  batch 200 loss: 0.048923478107899425\n",
      "  batch 300 loss: 0.058505065515637396\n",
      "LOSS train 0.058505065515637396 valid 0.12274158998286422\n",
      "valid accuracy 96.9\n",
      "test accuracy 96.99\n",
      "EPOCH 131:\n",
      "  batch 100 loss: 0.05447387620806694\n",
      "  batch 200 loss: 0.0578051742631942\n",
      "  batch 300 loss: 0.05684873883612454\n",
      "LOSS train 0.05684873883612454 valid 0.1181888038976283\n",
      "valid accuracy 96.75\n",
      "test accuracy 97.2\n",
      "EPOCH 132:\n",
      "  batch 100 loss: 0.05785009799525142\n",
      "  batch 200 loss: 0.05728148999158293\n",
      "  batch 300 loss: 0.05225404491648078\n",
      "LOSS train 0.05225404491648078 valid 0.1212245857201611\n",
      "valid accuracy 96.91\n",
      "test accuracy 97.15\n",
      "EPOCH 133:\n",
      "  batch 100 loss: 0.05722110639791936\n",
      "  batch 200 loss: 0.056992286331951615\n",
      "  batch 300 loss: 0.05801918924320489\n",
      "LOSS train 0.05801918924320489 valid 0.11841611185441195\n",
      "valid accuracy 97.05\n",
      "test accuracy 97.05\n",
      "EPOCH 134:\n",
      "  batch 100 loss: 0.056358334263786676\n",
      "  batch 200 loss: 0.05405263918451965\n",
      "  batch 300 loss: 0.05093049454968423\n",
      "LOSS train 0.05093049454968423 valid 0.1212719893672421\n",
      "valid accuracy 96.8\n",
      "test accuracy 97.18\n",
      "EPOCH 135:\n",
      "  batch 100 loss: 0.05157335200579837\n",
      "  batch 200 loss: 0.05206504246219992\n",
      "  batch 300 loss: 0.051333273248746994\n",
      "LOSS train 0.051333273248746994 valid 0.12585346550456708\n",
      "valid accuracy 96.91\n",
      "test accuracy 97.12\n",
      "EPOCH 136:\n",
      "  batch 100 loss: 0.0520663494837936\n",
      "  batch 200 loss: 0.05790696586016566\n",
      "  batch 300 loss: 0.05756312309997156\n",
      "LOSS train 0.05756312309997156 valid 0.12127191591064763\n",
      "valid accuracy 96.99\n",
      "test accuracy 97.51\n",
      "EPOCH 137:\n",
      "  batch 100 loss: 0.05341617168858647\n",
      "  batch 200 loss: 0.04694986692979\n",
      "  batch 300 loss: 0.057376916417852045\n",
      "LOSS train 0.057376916417852045 valid 0.12404849455114218\n",
      "valid accuracy 96.73\n",
      "test accuracy 97.44\n",
      "EPOCH 138:\n",
      "  batch 100 loss: 0.048694349057041106\n",
      "  batch 200 loss: 0.0524243870517239\n",
      "  batch 300 loss: 0.05080987489782274\n",
      "LOSS train 0.05080987489782274 valid 0.1198611630574812\n",
      "valid accuracy 96.95\n",
      "test accuracy 97.37\n",
      "EPOCH 139:\n",
      "  batch 100 loss: 0.04824812829727307\n",
      "  batch 200 loss: 0.05289789270143956\n",
      "  batch 300 loss: 0.052065923484042285\n",
      "LOSS train 0.052065923484042285 valid 0.11511823932840666\n",
      "valid accuracy 97.04\n",
      "test accuracy 97.27\n",
      "EPOCH 140:\n",
      "  batch 100 loss: 0.05235824360977858\n",
      "  batch 200 loss: 0.04494468430289999\n",
      "  batch 300 loss: 0.05178268976509571\n",
      "LOSS train 0.05178268976509571 valid 0.12000634122191917\n",
      "valid accuracy 96.85\n",
      "test accuracy 97.05\n",
      "EPOCH 141:\n",
      "  batch 100 loss: 0.04558286728337407\n",
      "  batch 200 loss: 0.05539016991970129\n",
      "  batch 300 loss: 0.04817694093217142\n",
      "LOSS train 0.04817694093217142 valid 0.12093294817988195\n",
      "valid accuracy 97.0\n",
      "test accuracy 97.42\n",
      "EPOCH 142:\n",
      "  batch 100 loss: 0.05219541621627286\n",
      "  batch 200 loss: 0.04928318283520639\n",
      "  batch 300 loss: 0.047144024800509214\n",
      "LOSS train 0.047144024800509214 valid 0.11844053523780047\n",
      "valid accuracy 96.91\n",
      "test accuracy 97.39\n",
      "EPOCH 143:\n",
      "  batch 100 loss: 0.0544566680979915\n",
      "  batch 200 loss: 0.04407766302814707\n",
      "  batch 300 loss: 0.042620790884830055\n",
      "LOSS train 0.042620790884830055 valid 0.12102805318523056\n",
      "valid accuracy 96.89\n",
      "test accuracy 97.4\n",
      "EPOCH 144:\n",
      "  batch 100 loss: 0.049462288068607446\n",
      "  batch 200 loss: 0.049175547035411\n",
      "  batch 300 loss: 0.04858639000216499\n",
      "LOSS train 0.04858639000216499 valid 0.11900519480622268\n",
      "valid accuracy 97.17\n",
      "test accuracy 97.12\n",
      "EPOCH 145:\n",
      "  batch 100 loss: 0.05152724726125598\n",
      "  batch 200 loss: 0.042029144957195966\n",
      "  batch 300 loss: 0.043608450517058375\n",
      "LOSS train 0.043608450517058375 valid 0.11589713109055935\n",
      "valid accuracy 97.01\n",
      "test accuracy 97.38\n",
      "EPOCH 146:\n",
      "  batch 100 loss: 0.05092955292202532\n",
      "  batch 200 loss: 0.04750006218906492\n",
      "  batch 300 loss: 0.04626406235154718\n",
      "LOSS train 0.04626406235154718 valid 0.11976437603088119\n",
      "valid accuracy 96.99\n",
      "test accuracy 97.22\n",
      "EPOCH 147:\n",
      "  batch 100 loss: 0.045852174798492344\n",
      "  batch 200 loss: 0.04804692259058356\n",
      "  batch 300 loss: 0.0431701306020841\n",
      "LOSS train 0.0431701306020841 valid 0.11826838564740706\n",
      "valid accuracy 97.06\n",
      "test accuracy 97.05\n",
      "EPOCH 148:\n",
      "  batch 100 loss: 0.05068135529756546\n",
      "  batch 200 loss: 0.035730437166057526\n",
      "  batch 300 loss: 0.052573486398905515\n",
      "LOSS train 0.052573486398905515 valid 0.12444524951957024\n",
      "valid accuracy 96.99\n",
      "test accuracy 97.42\n",
      "EPOCH 149:\n",
      "  batch 100 loss: 0.03483105389983393\n",
      "  batch 200 loss: 0.046708643729798494\n",
      "  batch 300 loss: 0.04525493881199509\n",
      "LOSS train 0.04525493881199509 valid 0.1137227309054306\n",
      "valid accuracy 97.05\n",
      "test accuracy 97.48\n",
      "EPOCH 150:\n",
      "  batch 100 loss: 0.048552153432974594\n",
      "  batch 200 loss: 0.04223876080708578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 300 loss: 0.03862566792406142\n",
      "LOSS train 0.03862566792406142 valid 0.11879793745837663\n",
      "valid accuracy 96.97\n",
      "test accuracy 97.35\n",
      "EPOCH 151:\n",
      "  batch 100 loss: 0.04889386388240382\n",
      "  batch 200 loss: 0.0392458225809969\n",
      "  batch 300 loss: 0.046558720956090836\n",
      "LOSS train 0.046558720956090836 valid 0.122127410109874\n",
      "valid accuracy 96.95\n",
      "test accuracy 97.36\n",
      "EPOCH 152:\n",
      "  batch 100 loss: 0.04344709153054282\n",
      "  batch 200 loss: 0.04356561078922823\n",
      "  batch 300 loss: 0.04755607866449282\n",
      "LOSS train 0.04755607866449282 valid 0.11424533219394993\n",
      "valid accuracy 97.09\n",
      "test accuracy 97.42\n",
      "EPOCH 153:\n",
      "  batch 100 loss: 0.03859241610392928\n",
      "  batch 200 loss: 0.04635518222581595\n",
      "  batch 300 loss: 0.04024861407931894\n",
      "LOSS train 0.04024861407931894 valid 0.11777339664087454\n",
      "valid accuracy 97.18\n",
      "test accuracy 97.19\n",
      "EPOCH 154:\n",
      "  batch 100 loss: 0.044843039107508954\n",
      "  batch 200 loss: 0.04282302498817444\n",
      "  batch 300 loss: 0.040593755857553336\n",
      "LOSS train 0.040593755857553336 valid 0.12138438420930193\n",
      "valid accuracy 97.19\n",
      "test accuracy 97.28\n",
      "EPOCH 155:\n",
      "  batch 100 loss: 0.0406166410073638\n",
      "  batch 200 loss: 0.039178505431627854\n",
      "  batch 300 loss: 0.03965981469722465\n",
      "LOSS train 0.03965981469722465 valid 0.11103947147066835\n",
      "valid accuracy 97.1\n",
      "test accuracy 97.45\n",
      "EPOCH 156:\n",
      "  batch 100 loss: 0.04235645132139325\n",
      "  batch 200 loss: 0.03969886849634349\n",
      "  batch 300 loss: 0.040015696662594566\n",
      "LOSS train 0.040015696662594566 valid 0.11379889282103203\n",
      "valid accuracy 97.26\n",
      "test accuracy 97.44\n",
      "EPOCH 157:\n",
      "  batch 100 loss: 0.04142099168384448\n",
      "  batch 200 loss: 0.04112821878516115\n",
      "  batch 300 loss: 0.039591911004390565\n",
      "LOSS train 0.039591911004390565 valid 0.12255641393941132\n",
      "valid accuracy 97.24\n",
      "test accuracy 97.3\n",
      "EPOCH 158:\n",
      "  batch 100 loss: 0.040043866102350875\n",
      "  batch 200 loss: 0.03582921243854798\n",
      "  batch 300 loss: 0.036382749883923676\n",
      "LOSS train 0.036382749883923676 valid 0.11701135880526013\n",
      "valid accuracy 97.11\n",
      "test accuracy 97.38\n",
      "EPOCH 159:\n",
      "  batch 100 loss: 0.040202240354847166\n",
      "  batch 200 loss: 0.039141982139553874\n",
      "  batch 300 loss: 0.039751076338579876\n",
      "LOSS train 0.039751076338579876 valid 0.12204246498832974\n",
      "valid accuracy 97.24\n",
      "test accuracy 97.38\n",
      "EPOCH 160:\n",
      "  batch 100 loss: 0.03817644942260813\n",
      "  batch 200 loss: 0.0387203587195836\n",
      "  batch 300 loss: 0.038329533794894816\n",
      "LOSS train 0.038329533794894816 valid 0.12559969722266792\n",
      "valid accuracy 97.2\n",
      "test accuracy 97.42\n",
      "EPOCH 161:\n",
      "  batch 100 loss: 0.04031502025900409\n",
      "  batch 200 loss: 0.03824290878954344\n",
      "  batch 300 loss: 0.04089421955402941\n",
      "LOSS train 0.04089421955402941 valid 0.1121068403012078\n",
      "valid accuracy 97.28\n",
      "test accuracy 97.35\n",
      "EPOCH 162:\n",
      "  batch 100 loss: 0.040199892853852365\n",
      "  batch 200 loss: 0.03267965641920455\n",
      "  batch 300 loss: 0.03918787813046947\n",
      "LOSS train 0.03918787813046947 valid 0.12746607745361008\n",
      "valid accuracy 97.16\n",
      "test accuracy 97.39\n",
      "EPOCH 163:\n",
      "  batch 100 loss: 0.036109374539228155\n",
      "  batch 200 loss: 0.03327957529458217\n",
      "  batch 300 loss: 0.034568074857234024\n",
      "LOSS train 0.034568074857234024 valid 0.12249486715403161\n",
      "valid accuracy 97.26\n",
      "test accuracy 97.46\n",
      "EPOCH 164:\n",
      "  batch 100 loss: 0.03631027661729604\n",
      "  batch 200 loss: 0.03516229623695835\n",
      "  batch 300 loss: 0.0383251000859309\n",
      "LOSS train 0.0383251000859309 valid 0.12049569009767869\n",
      "valid accuracy 97.23\n",
      "test accuracy 97.47\n",
      "EPOCH 165:\n",
      "  batch 100 loss: 0.03445931078982539\n",
      "  batch 200 loss: 0.03176003924687393\n",
      "  batch 300 loss: 0.03386794061982073\n",
      "LOSS train 0.03386794061982073 valid 0.12693753912367067\n",
      "valid accuracy 97.34\n",
      "test accuracy 97.47\n",
      "EPOCH 166:\n",
      "  batch 100 loss: 0.0362056789197959\n",
      "  batch 200 loss: 0.03179199085803702\n",
      "  batch 300 loss: 0.03321582376142032\n",
      "LOSS train 0.03321582376142032 valid 0.12322463750061166\n",
      "valid accuracy 97.33\n",
      "test accuracy 97.27\n",
      "EPOCH 167:\n",
      "  batch 100 loss: 0.03414427359210095\n",
      "  batch 200 loss: 0.033511293428600764\n",
      "  batch 300 loss: 0.03782490602694452\n",
      "LOSS train 0.03782490602694452 valid 0.1257026482460419\n",
      "valid accuracy 97.22\n",
      "test accuracy 97.54\n",
      "EPOCH 168:\n",
      "  batch 100 loss: 0.03314970620791428\n",
      "  batch 200 loss: 0.03815589161356911\n",
      "  batch 300 loss: 0.035443177865818146\n",
      "LOSS train 0.035443177865818146 valid 0.11689779881566256\n",
      "valid accuracy 97.39\n",
      "test accuracy 97.37\n",
      "EPOCH 169:\n",
      "  batch 100 loss: 0.032111175541067495\n",
      "  batch 200 loss: 0.03739427823922597\n",
      "  batch 300 loss: 0.03309486608020961\n",
      "LOSS train 0.03309486608020961 valid 0.12750644174911366\n",
      "valid accuracy 96.97\n",
      "test accuracy 97.37\n",
      "EPOCH 170:\n",
      "  batch 100 loss: 0.032094706111820415\n",
      "  batch 200 loss: 0.037404423989064524\n",
      "  batch 300 loss: 0.03134125794749707\n",
      "LOSS train 0.03134125794749707 valid 0.12680757869347412\n",
      "valid accuracy 97.13\n",
      "test accuracy 97.23\n",
      "EPOCH 171:\n",
      "  batch 100 loss: 0.03488366856414359\n",
      "  batch 200 loss: 0.03219570494256914\n",
      "  batch 300 loss: 0.035071039999602364\n",
      "LOSS train 0.035071039999602364 valid 0.11876055822247945\n",
      "valid accuracy 97.53\n",
      "test accuracy 97.55\n",
      "EPOCH 172:\n",
      "  batch 100 loss: 0.0358185518195387\n",
      "  batch 200 loss: 0.036115858593257145\n",
      "  batch 300 loss: 0.03210813329846132\n",
      "LOSS train 0.03210813329846132 valid 0.12544703710370378\n",
      "valid accuracy 97.28\n",
      "test accuracy 97.56\n",
      "EPOCH 173:\n",
      "  batch 100 loss: 0.03295400088245515\n",
      "  batch 200 loss: 0.032109508322319014\n",
      "  batch 300 loss: 0.031002436775015667\n",
      "LOSS train 0.031002436775015667 valid 0.12095732603051193\n",
      "valid accuracy 97.37\n",
      "test accuracy 97.82\n",
      "EPOCH 174:\n",
      "  batch 100 loss: 0.03753753403936571\n",
      "  batch 200 loss: 0.03153845099644968\n",
      "  batch 300 loss: 0.028059703335165977\n",
      "LOSS train 0.028059703335165977 valid 0.12564190851798565\n",
      "valid accuracy 97.18\n",
      "test accuracy 97.51\n",
      "EPOCH 175:\n",
      "  batch 100 loss: 0.032944092712423295\n",
      "  batch 200 loss: 0.03579614704940468\n",
      "  batch 300 loss: 0.029860696439864113\n",
      "LOSS train 0.029860696439864113 valid 0.12121504587283405\n",
      "valid accuracy 97.34\n",
      "test accuracy 97.52\n",
      "EPOCH 176:\n",
      "  batch 100 loss: 0.03249566131620668\n",
      "  batch 200 loss: 0.02942560916650109\n",
      "  batch 300 loss: 0.035991391048301014\n",
      "LOSS train 0.035991391048301014 valid 0.12531308097694116\n",
      "valid accuracy 97.33\n",
      "test accuracy 97.44\n",
      "EPOCH 177:\n",
      "  batch 100 loss: 0.03525796838570386\n",
      "  batch 200 loss: 0.030315321165835483\n",
      "  batch 300 loss: 0.027448151602293365\n",
      "LOSS train 0.027448151602293365 valid 0.11613072572744605\n",
      "valid accuracy 97.27\n",
      "test accuracy 97.57\n",
      "EPOCH 178:\n",
      "  batch 100 loss: 0.029586056623375042\n",
      "  batch 200 loss: 0.02977311786962673\n",
      "  batch 300 loss: 0.02906076311133802\n",
      "LOSS train 0.02906076311133802 valid 0.12317723700705963\n",
      "valid accuracy 97.3\n",
      "test accuracy 97.54\n",
      "EPOCH 179:\n",
      "  batch 100 loss: 0.030127399454941042\n",
      "  batch 200 loss: 0.031629014243371785\n",
      "  batch 300 loss: 0.028354913641232996\n",
      "LOSS train 0.028354913641232996 valid 0.11560223592501252\n",
      "valid accuracy 97.41\n",
      "test accuracy 97.74\n",
      "EPOCH 180:\n",
      "  batch 100 loss: 0.027093181267846374\n",
      "  batch 200 loss: 0.028436907564755528\n",
      "  batch 300 loss: 0.026805891601543407\n",
      "LOSS train 0.026805891601543407 valid 0.11340409742235029\n",
      "valid accuracy 97.34\n",
      "test accuracy 97.74\n",
      "EPOCH 181:\n",
      "  batch 100 loss: 0.03290459528507199\n",
      "  batch 200 loss: 0.02657856940757483\n",
      "  batch 300 loss: 0.030446066653821616\n",
      "LOSS train 0.030446066653821616 valid 0.11763943683519433\n",
      "valid accuracy 97.33\n",
      "test accuracy 97.52\n",
      "EPOCH 182:\n",
      "  batch 100 loss: 0.0338564771017991\n",
      "  batch 200 loss: 0.02651699684705818\n",
      "  batch 300 loss: 0.02903450029552914\n",
      "LOSS train 0.02903450029552914 valid 0.11985492732234393\n",
      "valid accuracy 97.3\n",
      "test accuracy 97.6\n",
      "EPOCH 183:\n",
      "  batch 100 loss: 0.029766234897469986\n",
      "  batch 200 loss: 0.023122730478062296\n",
      "  batch 300 loss: 0.030510305616335245\n",
      "LOSS train 0.030510305616335245 valid 0.12023367249713385\n",
      "valid accuracy 97.25\n",
      "test accuracy 97.7\n",
      "EPOCH 184:\n",
      "  batch 100 loss: 0.03082404200860765\n",
      "  batch 200 loss: 0.02812774302670732\n",
      "  batch 300 loss: 0.02588576000969624\n",
      "LOSS train 0.02588576000969624 valid 0.11324790202720751\n",
      "valid accuracy 97.33\n",
      "test accuracy 97.61\n",
      "EPOCH 185:\n",
      "  batch 100 loss: 0.02730850499588996\n",
      "  batch 200 loss: 0.02747251998720458\n",
      "  batch 300 loss: 0.024981093446258457\n",
      "LOSS train 0.024981093446258457 valid 0.12789376278973738\n",
      "valid accuracy 97.27\n",
      "test accuracy 97.53\n",
      "EPOCH 186:\n",
      "  batch 100 loss: 0.027234852840192617\n",
      "  batch 200 loss: 0.029084336884552614\n",
      "  batch 300 loss: 0.024448586024809628\n",
      "LOSS train 0.024448586024809628 valid 0.11631673387538412\n",
      "valid accuracy 97.42\n",
      "test accuracy 97.41\n",
      "EPOCH 187:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 100 loss: 0.02789062978408765\n",
      "  batch 200 loss: 0.030922537503938655\n",
      "  batch 300 loss: 0.02383736637595575\n",
      "LOSS train 0.02383736637595575 valid 0.11890871333815983\n",
      "valid accuracy 97.62\n",
      "test accuracy 97.67\n",
      "EPOCH 188:\n",
      "  batch 100 loss: 0.027032606811844745\n",
      "  batch 200 loss: 0.024224137876590248\n",
      "  batch 300 loss: 0.034947552585508676\n",
      "LOSS train 0.034947552585508676 valid 0.12030868937962842\n",
      "valid accuracy 97.33\n",
      "test accuracy 97.73\n",
      "EPOCH 189:\n",
      "  batch 100 loss: 0.024361093290499413\n",
      "  batch 200 loss: 0.02625310037401505\n",
      "  batch 300 loss: 0.026626540545839815\n",
      "LOSS train 0.026626540545839815 valid 0.12839458719532512\n",
      "valid accuracy 97.32\n",
      "test accuracy 97.75\n",
      "EPOCH 190:\n",
      "  batch 100 loss: 0.026768781228456648\n",
      "  batch 200 loss: 0.02178896101715509\n",
      "  batch 300 loss: 0.02563429149799049\n",
      "LOSS train 0.02563429149799049 valid 0.1200233684709779\n",
      "valid accuracy 97.35\n",
      "test accuracy 97.72\n",
      "EPOCH 191:\n",
      "  batch 100 loss: 0.02565916083753109\n",
      "  batch 200 loss: 0.023346080401097424\n",
      "  batch 300 loss: 0.020034351604554104\n",
      "LOSS train 0.020034351604554104 valid 0.12472248575583843\n",
      "valid accuracy 97.35\n",
      "test accuracy 97.69\n",
      "EPOCH 192:\n",
      "  batch 100 loss: 0.021021164518315344\n",
      "  batch 200 loss: 0.02351855164873996\n",
      "  batch 300 loss: 0.029855060306726956\n",
      "LOSS train 0.029855060306726956 valid 0.12497395829756633\n",
      "valid accuracy 97.46\n",
      "test accuracy 97.62\n",
      "EPOCH 193:\n",
      "  batch 100 loss: 0.02696545104845427\n",
      "  batch 200 loss: 0.023144867013324983\n",
      "  batch 300 loss: 0.024489637983206195\n",
      "LOSS train 0.024489637983206195 valid 0.12210075362052038\n",
      "valid accuracy 97.39\n",
      "test accuracy 97.69\n",
      "EPOCH 194:\n",
      "  batch 100 loss: 0.02799261454551015\n",
      "  batch 200 loss: 0.02736182947846828\n",
      "  batch 300 loss: 0.02483779364731163\n",
      "LOSS train 0.02483779364731163 valid 0.1231147687363842\n",
      "valid accuracy 97.38\n",
      "test accuracy 97.5\n",
      "EPOCH 195:\n",
      "  batch 100 loss: 0.025327590262750162\n",
      "  batch 200 loss: 0.023540101350517942\n",
      "  batch 300 loss: 0.026137019229063297\n",
      "LOSS train 0.026137019229063297 valid 0.11867305763136549\n",
      "valid accuracy 97.51\n",
      "test accuracy 97.74\n",
      "EPOCH 196:\n",
      "  batch 100 loss: 0.026181622321601025\n",
      "  batch 200 loss: 0.022478868250036612\n",
      "  batch 300 loss: 0.02215544993930962\n",
      "LOSS train 0.02215544993930962 valid 0.125885937489732\n",
      "valid accuracy 97.43\n",
      "test accuracy 97.68\n",
      "EPOCH 197:\n",
      "  batch 100 loss: 0.02348107424462796\n",
      "  batch 200 loss: 0.02088453369564377\n",
      "  batch 300 loss: 0.0219336794395349\n",
      "LOSS train 0.0219336794395349 valid 0.12272840450992889\n",
      "valid accuracy 97.64\n",
      "test accuracy 97.72\n",
      "EPOCH 198:\n",
      "  batch 100 loss: 0.02701799827365903\n",
      "  batch 200 loss: 0.025247908222372644\n",
      "  batch 300 loss: 0.021314057655690704\n",
      "LOSS train 0.021314057655690704 valid 0.12058632079875863\n",
      "valid accuracy 97.35\n",
      "test accuracy 97.61\n",
      "EPOCH 199:\n",
      "  batch 100 loss: 0.02449584413203411\n",
      "  batch 200 loss: 0.023939082655415403\n",
      "  batch 300 loss: 0.022463820667617256\n",
      "LOSS train 0.022463820667617256 valid 0.12647450446314032\n",
      "valid accuracy 97.58\n",
      "test accuracy 97.77\n",
      "EPOCH 200:\n",
      "  batch 100 loss: 0.026600107529084198\n",
      "  batch 200 loss: 0.024747240701981355\n",
      "  batch 300 loss: 0.01822960181161761\n",
      "LOSS train 0.01822960181161761 valid 0.12156169299224887\n",
      "valid accuracy 97.53\n",
      "test accuracy 97.63\n",
      "EPOCH 201:\n",
      "  batch 100 loss: 0.021093869450851344\n",
      "  batch 200 loss: 0.021288120934041217\n",
      "  batch 300 loss: 0.020872840065567288\n",
      "LOSS train 0.020872840065567288 valid 0.12171000508662266\n",
      "valid accuracy 97.68\n",
      "test accuracy 97.7\n",
      "EPOCH 202:\n",
      "  batch 100 loss: 0.020977905607433057\n",
      "  batch 200 loss: 0.020531345360504927\n",
      "  batch 300 loss: 0.019595854714134477\n",
      "LOSS train 0.019595854714134477 valid 0.12376004435443261\n",
      "valid accuracy 97.54\n",
      "test accuracy 97.49\n",
      "EPOCH 203:\n",
      "  batch 100 loss: 0.022717220964113948\n",
      "  batch 200 loss: 0.018328459746844602\n",
      "  batch 300 loss: 0.023033361443231114\n",
      "LOSS train 0.023033361443231114 valid 0.13173675862961756\n",
      "valid accuracy 97.51\n",
      "test accuracy 97.97\n",
      "EPOCH 204:\n",
      "  batch 100 loss: 0.025113074440450873\n",
      "  batch 200 loss: 0.016920662642660317\n",
      "  batch 300 loss: 0.02110770832572598\n",
      "LOSS train 0.02110770832572598 valid 0.1211921921153877\n",
      "valid accuracy 97.46\n",
      "test accuracy 97.76\n",
      "EPOCH 205:\n",
      "  batch 100 loss: 0.027086070117366034\n",
      "  batch 200 loss: 0.01728396340426116\n",
      "  batch 300 loss: 0.017508428535657004\n",
      "LOSS train 0.017508428535657004 valid 0.1262082877764891\n",
      "valid accuracy 97.39\n",
      "test accuracy 97.74\n",
      "EPOCH 206:\n",
      "  batch 100 loss: 0.02543817503086757\n",
      "  batch 200 loss: 0.02033849885352538\n",
      "  batch 300 loss: 0.01992652845394332\n",
      "LOSS train 0.01992652845394332 valid 0.1323101065516519\n",
      "valid accuracy 97.43\n",
      "test accuracy 97.64\n",
      "EPOCH 207:\n",
      "  batch 100 loss: 0.020629851437406616\n",
      "  batch 200 loss: 0.02023161399629316\n",
      "  batch 300 loss: 0.02297641582088545\n",
      "LOSS train 0.02297641582088545 valid 0.13242641761222976\n",
      "valid accuracy 97.44\n",
      "test accuracy 97.78\n",
      "EPOCH 208:\n",
      "  batch 100 loss: 0.020700378150504548\n",
      "  batch 200 loss: 0.019322450982581358\n",
      "  batch 300 loss: 0.02136974438210018\n",
      "LOSS train 0.02136974438210018 valid 0.13024357517322274\n",
      "valid accuracy 97.55\n",
      "test accuracy 97.69\n",
      "EPOCH 209:\n",
      "  batch 100 loss: 0.017905298658442915\n",
      "  batch 200 loss: 0.01952270783498534\n",
      "  batch 300 loss: 0.017686908541509182\n",
      "LOSS train 0.017686908541509182 valid 0.13054395069444075\n",
      "valid accuracy 97.39\n",
      "test accuracy 97.71\n",
      "EPOCH 210:\n",
      "  batch 100 loss: 0.023344891365704824\n",
      "  batch 200 loss: 0.01907187439064728\n",
      "  batch 300 loss: 0.018776976864610332\n",
      "LOSS train 0.018776976864610332 valid 0.1297602639997767\n",
      "valid accuracy 97.56\n",
      "test accuracy 97.75\n",
      "EPOCH 211:\n",
      "  batch 100 loss: 0.02150420790698263\n",
      "  batch 200 loss: 0.019135739559715148\n",
      "  batch 300 loss: 0.019112195437846823\n",
      "LOSS train 0.019112195437846823 valid 0.13319498800046858\n",
      "valid accuracy 97.42\n",
      "test accuracy 97.8\n",
      "EPOCH 212:\n",
      "  batch 100 loss: 0.02158996687619947\n",
      "  batch 200 loss: 0.019515967191982782\n",
      "  batch 300 loss: 0.017566088469466196\n",
      "LOSS train 0.017566088469466196 valid 0.1365121378581028\n",
      "valid accuracy 97.48\n",
      "test accuracy 97.85\n",
      "EPOCH 213:\n",
      "  batch 100 loss: 0.018366581243317342\n",
      "  batch 200 loss: 0.020523981374135473\n",
      "  batch 300 loss: 0.017403609838220292\n",
      "LOSS train 0.017403609838220292 valid 0.122922592103051\n",
      "valid accuracy 97.69\n",
      "test accuracy 97.73\n",
      "EPOCH 214:\n",
      "  batch 100 loss: 0.02171891681937268\n",
      "  batch 200 loss: 0.020419584676274097\n",
      "  batch 300 loss: 0.018019646559259854\n",
      "LOSS train 0.018019646559259854 valid 0.12271398904768727\n",
      "valid accuracy 97.63\n",
      "test accuracy 97.86\n",
      "EPOCH 215:\n",
      "  batch 100 loss: 0.022811213326931465\n",
      "  batch 200 loss: 0.01575724354792328\n",
      "  batch 300 loss: 0.015483975823735818\n",
      "LOSS train 0.015483975823735818 valid 0.12412447907926538\n",
      "valid accuracy 97.55\n",
      "test accuracy 97.67\n",
      "EPOCH 216:\n",
      "  batch 100 loss: 0.0180614801550837\n",
      "  batch 200 loss: 0.015024231855932157\n",
      "  batch 300 loss: 0.017530409230130318\n",
      "LOSS train 0.017530409230130318 valid 0.1279680779862477\n",
      "valid accuracy 97.53\n",
      "test accuracy 97.8\n",
      "EPOCH 217:\n",
      "  batch 100 loss: 0.021591587985603837\n",
      "  batch 200 loss: 0.015911001741442304\n",
      "  batch 300 loss: 0.018853698502207406\n",
      "LOSS train 0.018853698502207406 valid 0.12802103207088547\n",
      "valid accuracy 97.52\n",
      "test accuracy 97.7\n",
      "EPOCH 218:\n",
      "  batch 100 loss: 0.020273091260169167\n",
      "  batch 200 loss: 0.017506146059313322\n",
      "  batch 300 loss: 0.019754960025238688\n",
      "LOSS train 0.019754960025238688 valid 0.12417190032716416\n",
      "valid accuracy 97.66\n",
      "test accuracy 97.82\n",
      "EPOCH 219:\n",
      "  batch 100 loss: 0.021863439386361278\n",
      "  batch 200 loss: 0.015042058268154506\n",
      "  batch 300 loss: 0.015839732712775004\n",
      "LOSS train 0.015839732712775004 valid 0.11577933911269875\n",
      "valid accuracy 97.67\n",
      "test accuracy 97.73\n",
      "EPOCH 220:\n",
      "  batch 100 loss: 0.018413492852341732\n",
      "  batch 200 loss: 0.019162202095612882\n",
      "  batch 300 loss: 0.01632839177909773\n",
      "LOSS train 0.01632839177909773 valid 0.1361090717203216\n",
      "valid accuracy 97.56\n",
      "test accuracy 97.7\n",
      "EPOCH 221:\n",
      "  batch 100 loss: 0.02029775415248878\n",
      "  batch 200 loss: 0.017906775499795913\n",
      "  batch 300 loss: 0.01950213151445496\n",
      "LOSS train 0.01950213151445496 valid 0.1309552797428745\n",
      "valid accuracy 97.65\n",
      "test accuracy 97.69\n",
      "EPOCH 222:\n",
      "  batch 100 loss: 0.019205150218767814\n",
      "  batch 200 loss: 0.016753270475601312\n",
      "  batch 300 loss: 0.018644000787317053\n",
      "LOSS train 0.018644000787317053 valid 0.13829437322336943\n",
      "valid accuracy 97.67\n",
      "test accuracy 97.58\n",
      "EPOCH 223:\n",
      "  batch 100 loss: 0.017758579644287238\n",
      "  batch 200 loss: 0.015388337050681003\n",
      "  batch 300 loss: 0.016741883042377594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 0.016741883042377594 valid 0.1349902709741941\n",
      "valid accuracy 97.61\n",
      "test accuracy 97.87\n",
      "EPOCH 224:\n",
      "  batch 100 loss: 0.021434359091435908\n",
      "  batch 200 loss: 0.01644268317257229\n",
      "  batch 300 loss: 0.014239302127280098\n",
      "LOSS train 0.014239302127280098 valid 0.1418218489884048\n",
      "valid accuracy 97.58\n",
      "test accuracy 97.82\n",
      "EPOCH 225:\n",
      "  batch 100 loss: 0.016266395353159168\n",
      "  batch 200 loss: 0.015108378098229878\n",
      "  batch 300 loss: 0.015512403858083416\n",
      "LOSS train 0.015512403858083416 valid 0.13709681013619668\n",
      "valid accuracy 97.52\n",
      "test accuracy 97.92\n",
      "EPOCH 226:\n",
      "  batch 100 loss: 0.015886069845801102\n",
      "  batch 200 loss: 0.015318604106287239\n",
      "  batch 300 loss: 0.015067060420624329\n",
      "LOSS train 0.015067060420624329 valid 0.13087207472830115\n",
      "valid accuracy 97.59\n",
      "test accuracy 97.93\n",
      "EPOCH 227:\n",
      "  batch 100 loss: 0.016642736066423823\n",
      "  batch 200 loss: 0.014646279224834871\n",
      "  batch 300 loss: 0.011670379953357042\n",
      "LOSS train 0.011670379953357042 valid 0.13042485817903623\n",
      "valid accuracy 97.5\n",
      "test accuracy 97.85\n",
      "EPOCH 228:\n",
      "  batch 100 loss: 0.019123285383975598\n",
      "  batch 200 loss: 0.016751383618684487\n",
      "  batch 300 loss: 0.015857001956828754\n",
      "LOSS train 0.015857001956828754 valid 0.1417230015762056\n",
      "valid accuracy 97.72\n",
      "test accuracy 97.72\n",
      "EPOCH 229:\n",
      "  batch 100 loss: 0.019325966518954373\n",
      "  batch 200 loss: 0.01619498363485036\n",
      "  batch 300 loss: 0.01907426247766125\n",
      "LOSS train 0.01907426247766125 valid 0.1371642704036299\n",
      "valid accuracy 97.58\n",
      "test accuracy 97.86\n",
      "EPOCH 230:\n",
      "  batch 100 loss: 0.013590223864666769\n",
      "  batch 200 loss: 0.01451478034912725\n",
      "  batch 300 loss: 0.016950510681490415\n",
      "LOSS train 0.016950510681490415 valid 0.12219535007695519\n",
      "valid accuracy 97.77\n",
      "test accuracy 97.75\n",
      "EPOCH 231:\n",
      "  batch 100 loss: 0.01563877074600896\n",
      "  batch 200 loss: 0.018536104470549618\n",
      "  batch 300 loss: 0.015460914174473145\n",
      "LOSS train 0.015460914174473145 valid 0.12964773824610865\n",
      "valid accuracy 97.64\n",
      "test accuracy 97.8\n",
      "EPOCH 232:\n",
      "  batch 100 loss: 0.020059094765201735\n",
      "  batch 200 loss: 0.017585829868621658\n",
      "  batch 300 loss: 0.014002256023668452\n",
      "LOSS train 0.014002256023668452 valid 0.14168954063044814\n",
      "valid accuracy 97.69\n",
      "test accuracy 97.86\n",
      "EPOCH 233:\n",
      "  batch 100 loss: 0.018667211737047183\n",
      "  batch 200 loss: 0.018282164629199543\n",
      "  batch 300 loss: 0.01466034986806335\n",
      "LOSS train 0.01466034986806335 valid 0.12817403573404315\n",
      "valid accuracy 97.74\n",
      "test accuracy 97.72\n",
      "EPOCH 234:\n",
      "  batch 100 loss: 0.01829973071347922\n",
      "  batch 200 loss: 0.012400811671104748\n",
      "  batch 300 loss: 0.013965598760514696\n",
      "LOSS train 0.013965598760514696 valid 0.13988120699392725\n",
      "valid accuracy 97.74\n",
      "test accuracy 97.84\n",
      "EPOCH 235:\n",
      "  batch 100 loss: 0.015015992208209355\n",
      "  batch 200 loss: 0.018225642405595863\n",
      "  batch 300 loss: 0.013672159227280645\n",
      "LOSS train 0.013672159227280645 valid 0.12511473079110125\n",
      "valid accuracy 97.7\n",
      "test accuracy 97.68\n",
      "EPOCH 236:\n",
      "  batch 100 loss: 0.01639212838737876\n",
      "  batch 200 loss: 0.012089535656268708\n",
      "  batch 300 loss: 0.01307175047139026\n",
      "LOSS train 0.01307175047139026 valid 0.13505182023643872\n",
      "valid accuracy 97.48\n",
      "test accuracy 97.83\n",
      "EPOCH 237:\n",
      "  batch 100 loss: 0.019635283461611835\n",
      "  batch 200 loss: 0.011405622592574218\n",
      "  batch 300 loss: 0.01694590763268934\n",
      "LOSS train 0.01694590763268934 valid 0.12869166858977626\n",
      "valid accuracy 97.66\n",
      "test accuracy 97.87\n",
      "EPOCH 238:\n",
      "  batch 100 loss: 0.015146952919094474\n",
      "  batch 200 loss: 0.013937588782646344\n",
      "  batch 300 loss: 0.013746147682832088\n",
      "LOSS train 0.013746147682832088 valid 0.1409247097061653\n",
      "valid accuracy 97.63\n",
      "test accuracy 97.74\n",
      "EPOCH 239:\n",
      "  batch 100 loss: 0.015365761384273355\n",
      "  batch 200 loss: 0.01398169043033704\n",
      "  batch 300 loss: 0.014127361515420489\n",
      "LOSS train 0.014127361515420489 valid 0.1356424125201729\n",
      "valid accuracy 97.62\n",
      "test accuracy 97.99\n",
      "EPOCH 240:\n",
      "  batch 100 loss: 0.0164118230407621\n",
      "  batch 200 loss: 0.012217094956358778\n",
      "  batch 300 loss: 0.011861356951776543\n",
      "LOSS train 0.011861356951776543 valid 0.1331049679760833\n",
      "valid accuracy 97.63\n",
      "test accuracy 97.83\n",
      "EPOCH 241:\n",
      "  batch 100 loss: 0.014773414878836775\n",
      "  batch 200 loss: 0.012961450955499458\n",
      "  batch 300 loss: 0.014100081304495688\n",
      "LOSS train 0.014100081304495688 valid 0.14438754531348894\n",
      "valid accuracy 97.68\n",
      "test accuracy 97.86\n",
      "EPOCH 242:\n",
      "  batch 100 loss: 0.015650204919074893\n",
      "  batch 200 loss: 0.012748046661290573\n",
      "  batch 300 loss: 0.011824698111558974\n",
      "LOSS train 0.011824698111558974 valid 0.12441056968107866\n",
      "valid accuracy 97.77\n",
      "test accuracy 97.9\n",
      "EPOCH 243:\n",
      "  batch 100 loss: 0.015894584881170886\n",
      "  batch 200 loss: 0.014391625664648017\n",
      "  batch 300 loss: 0.01347112514951732\n",
      "LOSS train 0.01347112514951732 valid 0.14397653760918533\n",
      "valid accuracy 97.66\n",
      "test accuracy 97.79\n",
      "EPOCH 244:\n",
      "  batch 100 loss: 0.013831147221244464\n",
      "  batch 200 loss: 0.010797793663850825\n",
      "  batch 300 loss: 0.01214444452416501\n",
      "LOSS train 0.01214444452416501 valid 0.12513701726835952\n",
      "valid accuracy 97.86\n",
      "test accuracy 97.83\n",
      "EPOCH 245:\n",
      "  batch 100 loss: 0.01702926820871653\n",
      "  batch 200 loss: 0.016633157593423674\n",
      "  batch 300 loss: 0.010840458236525592\n",
      "LOSS train 0.010840458236525592 valid 0.13888799384724884\n",
      "valid accuracy 97.75\n",
      "test accuracy 97.86\n",
      "EPOCH 246:\n",
      "  batch 100 loss: 0.016638264169596368\n",
      "  batch 200 loss: 0.014720103844883852\n",
      "  batch 300 loss: 0.010751374483606923\n",
      "LOSS train 0.010751374483606923 valid 0.1338365556951465\n",
      "valid accuracy 97.61\n",
      "test accuracy 97.78\n",
      "EPOCH 247:\n",
      "  batch 100 loss: 0.014483192377156229\n",
      "  batch 200 loss: 0.011774661460949574\n",
      "  batch 300 loss: 0.00937279074707476\n",
      "LOSS train 0.00937279074707476 valid 0.1407752849388285\n",
      "valid accuracy 97.61\n",
      "test accuracy 97.8\n",
      "EPOCH 248:\n",
      "  batch 100 loss: 0.011897866820372655\n",
      "  batch 200 loss: 0.013539652347390074\n",
      "  batch 300 loss: 0.012974896139930935\n",
      "LOSS train 0.012974896139930935 valid 0.138276136881831\n",
      "valid accuracy 97.72\n",
      "test accuracy 97.94\n",
      "EPOCH 249:\n",
      "  batch 100 loss: 0.014457287947916484\n",
      "  batch 200 loss: 0.011489309261996823\n",
      "  batch 300 loss: 0.013747943607613706\n",
      "LOSS train 0.013747943607613706 valid 0.14294124550747545\n",
      "valid accuracy 97.71\n",
      "test accuracy 97.85\n",
      "EPOCH 250:\n",
      "  batch 100 loss: 0.014688615615668824\n",
      "  batch 200 loss: 0.009687926228398282\n",
      "  batch 300 loss: 0.012491579938141513\n",
      "LOSS train 0.012491579938141513 valid 0.13395601922270917\n",
      "valid accuracy 97.77\n",
      "test accuracy 97.79\n",
      "EPOCH 251:\n",
      "  batch 100 loss: 0.013356576606765884\n",
      "  batch 200 loss: 0.014417764537938638\n",
      "  batch 300 loss: 0.015613644008826668\n",
      "LOSS train 0.015613644008826668 valid 0.1477497100166009\n",
      "valid accuracy 97.72\n",
      "test accuracy 97.9\n",
      "EPOCH 252:\n",
      "  batch 100 loss: 0.015077848701839685\n",
      "  batch 200 loss: 0.011732774351694389\n",
      "  batch 300 loss: 0.010553225156863846\n",
      "LOSS train 0.010553225156863846 valid 0.12759546551627218\n",
      "valid accuracy 97.75\n",
      "test accuracy 97.9\n",
      "EPOCH 253:\n",
      "  batch 100 loss: 0.014963235828836331\n",
      "  batch 200 loss: 0.011053521672438364\n",
      "  batch 300 loss: 0.010095511264298694\n",
      "LOSS train 0.010095511264298694 valid 0.14164037339393376\n",
      "valid accuracy 97.74\n",
      "test accuracy 98.04\n",
      "EPOCH 254:\n",
      "  batch 100 loss: 0.011728126865054946\n",
      "  batch 200 loss: 0.012180991612112849\n",
      "  batch 300 loss: 0.014018889924955147\n",
      "LOSS train 0.014018889924955147 valid 0.13815865092070867\n",
      "valid accuracy 97.62\n",
      "test accuracy 98.03\n",
      "EPOCH 255:\n",
      "  batch 100 loss: 0.012904798105737428\n",
      "  batch 200 loss: 0.01144838132751829\n",
      "  batch 300 loss: 0.011005878315809242\n",
      "LOSS train 0.011005878315809242 valid 0.13809183641274966\n",
      "valid accuracy 97.66\n",
      "test accuracy 97.86\n",
      "EPOCH 256:\n",
      "  batch 100 loss: 0.014149210061295889\n",
      "  batch 200 loss: 0.011448271270492114\n",
      "  batch 300 loss: 0.011441193979517266\n",
      "LOSS train 0.011441193979517266 valid 0.14061221669524032\n",
      "valid accuracy 97.63\n",
      "test accuracy 97.95\n",
      "EPOCH 257:\n",
      "  batch 100 loss: 0.015105423981312925\n",
      "  batch 200 loss: 0.011026007012933405\n",
      "  batch 300 loss: 0.011379606003688423\n",
      "LOSS train 0.011379606003688423 valid 0.12944259994649857\n",
      "valid accuracy 97.78\n",
      "test accuracy 97.7\n",
      "EPOCH 258:\n",
      "  batch 100 loss: 0.012778381321404596\n",
      "  batch 200 loss: 0.010401314747177821\n",
      "  batch 300 loss: 0.011746164495270932\n",
      "LOSS train 0.011746164495270932 valid 0.15994236579224808\n",
      "valid accuracy 97.65\n",
      "test accuracy 98.0\n",
      "EPOCH 259:\n",
      "  batch 100 loss: 0.012660169001246686\n",
      "  batch 200 loss: 0.013151579242185107\n",
      "  batch 300 loss: 0.011586425161076476\n",
      "LOSS train 0.011586425161076476 valid 0.13586280153953614\n",
      "valid accuracy 97.74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 97.86\n",
      "EPOCH 260:\n",
      "  batch 100 loss: 0.013380739711901696\n",
      "  batch 200 loss: 0.008070026140521804\n",
      "  batch 300 loss: 0.011907575091227045\n",
      "LOSS train 0.011907575091227045 valid 0.15869677343752173\n",
      "valid accuracy 97.7\n",
      "test accuracy 97.93\n",
      "EPOCH 261:\n",
      "  batch 100 loss: 0.01323948924909928\n",
      "  batch 200 loss: 0.01097346875558287\n",
      "  batch 300 loss: 0.010190687365629857\n",
      "LOSS train 0.010190687365629857 valid 0.14106485450086242\n",
      "valid accuracy 97.67\n",
      "test accuracy 97.98\n",
      "EPOCH 262:\n",
      "  batch 100 loss: 0.012263726055186908\n",
      "  batch 200 loss: 0.009554695661136066\n",
      "  batch 300 loss: 0.009093683268570203\n",
      "LOSS train 0.009093683268570203 valid 0.14619689835091798\n",
      "valid accuracy 97.61\n",
      "test accuracy 97.99\n",
      "EPOCH 263:\n",
      "  batch 100 loss: 0.013273737924391753\n",
      "  batch 200 loss: 0.009368504121539445\n",
      "  batch 300 loss: 0.00749862463100726\n",
      "LOSS train 0.00749862463100726 valid 0.14860389638108498\n",
      "valid accuracy 97.86\n",
      "test accuracy 97.82\n",
      "EPOCH 264:\n",
      "  batch 100 loss: 0.012661930501867574\n",
      "  batch 200 loss: 0.011101293609335698\n",
      "  batch 300 loss: 0.01047630383300202\n",
      "LOSS train 0.01047630383300202 valid 0.14095201320728099\n",
      "valid accuracy 97.87\n",
      "test accuracy 97.88\n",
      "EPOCH 265:\n",
      "  batch 100 loss: 0.011157323737461412\n",
      "  batch 200 loss: 0.009197161683587182\n",
      "  batch 300 loss: 0.010236473457844113\n",
      "LOSS train 0.010236473457844113 valid 0.1364630948209811\n",
      "valid accuracy 97.88\n",
      "test accuracy 97.87\n",
      "EPOCH 266:\n",
      "  batch 100 loss: 0.012089952545702545\n",
      "  batch 200 loss: 0.009801150194139154\n",
      "  batch 300 loss: 0.012472106491477461\n",
      "LOSS train 0.012472106491477461 valid 0.1476949575905645\n",
      "valid accuracy 97.74\n",
      "test accuracy 97.88\n",
      "EPOCH 267:\n",
      "  batch 100 loss: 0.013441334634480882\n",
      "  batch 200 loss: 0.011257317894232984\n",
      "  batch 300 loss: 0.011833463603152268\n",
      "LOSS train 0.011833463603152268 valid 0.14531697137080518\n",
      "valid accuracy 97.74\n",
      "test accuracy 97.9\n",
      "EPOCH 268:\n",
      "  batch 100 loss: 0.011828261289247166\n",
      "  batch 200 loss: 0.00934270979869325\n",
      "  batch 300 loss: 0.009435894661191924\n",
      "LOSS train 0.009435894661191924 valid 0.14282967584059877\n",
      "valid accuracy 97.83\n",
      "test accuracy 97.91\n",
      "EPOCH 269:\n",
      "  batch 100 loss: 0.013898672186587645\n",
      "  batch 200 loss: 0.0069277978735772195\n",
      "  batch 300 loss: 0.009805623567954171\n",
      "LOSS train 0.009805623567954171 valid 0.14331684470002812\n",
      "valid accuracy 97.89\n",
      "test accuracy 97.85\n",
      "EPOCH 270:\n",
      "  batch 100 loss: 0.011313113199703366\n",
      "  batch 200 loss: 0.00980417566439428\n",
      "  batch 300 loss: 0.010305241753339942\n",
      "LOSS train 0.010305241753339942 valid 0.156151186939715\n",
      "valid accuracy 97.76\n",
      "test accuracy 98.1\n",
      "EPOCH 271:\n",
      "  batch 100 loss: 0.00787934007448257\n",
      "  batch 200 loss: 0.01006249797752389\n",
      "  batch 300 loss: 0.00875740664807381\n",
      "LOSS train 0.00875740664807381 valid 0.15145046011247923\n",
      "valid accuracy 97.78\n",
      "test accuracy 97.94\n",
      "EPOCH 272:\n",
      "  batch 100 loss: 0.011285012724401895\n",
      "  batch 200 loss: 0.008203957575333334\n",
      "  batch 300 loss: 0.011073338045716809\n",
      "LOSS train 0.011073338045716809 valid 0.15445562203616578\n",
      "valid accuracy 97.73\n",
      "test accuracy 98.11\n",
      "EPOCH 273:\n",
      "  batch 100 loss: 0.010350356145590923\n",
      "  batch 200 loss: 0.008139664689260827\n",
      "  batch 300 loss: 0.007162616917539708\n",
      "LOSS train 0.007162616917539708 valid 0.14581757450033836\n",
      "valid accuracy 97.79\n",
      "test accuracy 98.1\n",
      "EPOCH 274:\n",
      "  batch 100 loss: 0.010437803344048006\n",
      "  batch 200 loss: 0.008212936489931053\n",
      "  batch 300 loss: 0.008689113809841728\n",
      "LOSS train 0.008689113809841728 valid 0.15065036619029043\n",
      "valid accuracy 97.71\n",
      "test accuracy 97.75\n",
      "EPOCH 275:\n",
      "  batch 100 loss: 0.009609878001101605\n",
      "  batch 200 loss: 0.013331637456194584\n",
      "  batch 300 loss: 0.008438054385896976\n",
      "LOSS train 0.008438054385896976 valid 0.15085111216559224\n",
      "valid accuracy 97.78\n",
      "test accuracy 97.96\n",
      "EPOCH 276:\n",
      "  batch 100 loss: 0.011287478212461793\n",
      "  batch 200 loss: 0.009285366446019907\n",
      "  batch 300 loss: 0.01114247632790466\n",
      "LOSS train 0.01114247632790466 valid 0.14284381381518896\n",
      "valid accuracy 97.71\n",
      "test accuracy 97.9\n",
      "EPOCH 277:\n",
      "  batch 100 loss: 0.011761542155534243\n",
      "  batch 200 loss: 0.007157697296206606\n",
      "  batch 300 loss: 0.00822979589949682\n",
      "LOSS train 0.00822979589949682 valid 0.1473947762433454\n",
      "valid accuracy 97.77\n",
      "test accuracy 98.01\n",
      "EPOCH 278:\n",
      "  batch 100 loss: 0.011615101436452733\n",
      "  batch 200 loss: 0.008437339671836526\n",
      "  batch 300 loss: 0.008962012758024685\n",
      "LOSS train 0.008962012758024685 valid 0.15866132641651026\n",
      "valid accuracy 97.7\n",
      "test accuracy 97.89\n",
      "EPOCH 279:\n",
      "  batch 100 loss: 0.010561908313501362\n",
      "  batch 200 loss: 0.007491318611082534\n",
      "  batch 300 loss: 0.0071867352376648345\n",
      "LOSS train 0.0071867352376648345 valid 0.14709642532271355\n",
      "valid accuracy 97.84\n",
      "test accuracy 97.88\n",
      "EPOCH 280:\n",
      "  batch 100 loss: 0.009585199813993767\n",
      "  batch 200 loss: 0.01004757700200571\n",
      "  batch 300 loss: 0.009361974278208436\n",
      "LOSS train 0.009361974278208436 valid 0.15464388528251427\n",
      "valid accuracy 97.81\n",
      "test accuracy 98.07\n",
      "EPOCH 281:\n",
      "  batch 100 loss: 0.009321180024489877\n",
      "  batch 200 loss: 0.008100336419211089\n",
      "  batch 300 loss: 0.009405296657314465\n",
      "LOSS train 0.009405296657314465 valid 0.1653033844073388\n",
      "valid accuracy 97.59\n",
      "test accuracy 97.99\n",
      "EPOCH 282:\n",
      "  batch 100 loss: 0.008763891212820453\n",
      "  batch 200 loss: 0.00981974091297161\n",
      "  batch 300 loss: 0.009291633043508227\n",
      "LOSS train 0.009291633043508227 valid 0.15616273252967\n",
      "valid accuracy 97.74\n",
      "test accuracy 97.95\n",
      "EPOCH 283:\n",
      "  batch 100 loss: 0.00924158169367729\n",
      "  batch 200 loss: 0.00765900521405456\n",
      "  batch 300 loss: 0.008378456626096521\n",
      "LOSS train 0.008378456626096521 valid 0.15419405134132497\n",
      "valid accuracy 97.8\n",
      "test accuracy 98.05\n",
      "EPOCH 284:\n",
      "  batch 100 loss: 0.010474998346958274\n",
      "  batch 200 loss: 0.007672888845431772\n",
      "  batch 300 loss: 0.008514269327861257\n",
      "LOSS train 0.008514269327861257 valid 0.14506381198561594\n",
      "valid accuracy 97.77\n",
      "test accuracy 98.01\n",
      "EPOCH 285:\n",
      "  batch 100 loss: 0.013058179750478302\n",
      "  batch 200 loss: 0.00935555629602277\n",
      "  batch 300 loss: 0.011387995429286092\n",
      "LOSS train 0.011387995429286092 valid 0.16303665868782688\n",
      "valid accuracy 97.72\n",
      "test accuracy 97.94\n",
      "EPOCH 286:\n",
      "  batch 100 loss: 0.01093687537118683\n",
      "  batch 200 loss: 0.007495759868443201\n",
      "  batch 300 loss: 0.00875995819274749\n",
      "LOSS train 0.00875995819274749 valid 0.15505278949725385\n",
      "valid accuracy 97.75\n",
      "test accuracy 98.04\n",
      "EPOCH 287:\n",
      "  batch 100 loss: 0.011763620052929582\n",
      "  batch 200 loss: 0.009994809720938065\n",
      "  batch 300 loss: 0.008986363164403884\n",
      "LOSS train 0.008986363164403884 valid 0.1531538287394303\n",
      "valid accuracy 97.78\n",
      "test accuracy 97.97\n",
      "EPOCH 288:\n",
      "  batch 100 loss: 0.010963782370345143\n",
      "  batch 200 loss: 0.008037934756014237\n",
      "  batch 300 loss: 0.008563604940104597\n",
      "LOSS train 0.008563604940104597 valid 0.14685114447486658\n",
      "valid accuracy 97.83\n",
      "test accuracy 97.88\n",
      "EPOCH 289:\n",
      "  batch 100 loss: 0.010138640701634359\n",
      "  batch 200 loss: 0.0069805752041168035\n",
      "  batch 300 loss: 0.007396179307352213\n",
      "LOSS train 0.007396179307352213 valid 0.1548209062635993\n",
      "valid accuracy 97.73\n",
      "test accuracy 98.05\n",
      "EPOCH 290:\n",
      "  batch 100 loss: 0.01120174830466567\n",
      "  batch 200 loss: 0.007280499276948831\n",
      "  batch 300 loss: 0.0049342817573960925\n",
      "LOSS train 0.0049342817573960925 valid 0.16073308216262008\n",
      "valid accuracy 97.71\n",
      "test accuracy 97.97\n",
      "EPOCH 291:\n",
      "  batch 100 loss: 0.011384452457896259\n",
      "  batch 200 loss: 0.008774281536079797\n",
      "  batch 300 loss: 0.0072317249610273394\n",
      "LOSS train 0.0072317249610273394 valid 0.14600585943420963\n",
      "valid accuracy 97.75\n",
      "test accuracy 98.03\n",
      "EPOCH 292:\n",
      "  batch 100 loss: 0.009030843401960737\n",
      "  batch 200 loss: 0.008777254140566128\n",
      "  batch 300 loss: 0.006293129549894729\n",
      "LOSS train 0.006293129549894729 valid 0.15798342513816419\n",
      "valid accuracy 97.83\n",
      "test accuracy 97.72\n",
      "EPOCH 293:\n",
      "  batch 100 loss: 0.00984478580834093\n",
      "  batch 200 loss: 0.0055182840449742795\n",
      "  batch 300 loss: 0.008296280671570457\n",
      "LOSS train 0.008296280671570457 valid 0.13921648578860382\n",
      "valid accuracy 97.96\n",
      "test accuracy 97.96\n",
      "EPOCH 294:\n",
      "  batch 100 loss: 0.008545949918616316\n",
      "  batch 200 loss: 0.010998146920392173\n",
      "  batch 300 loss: 0.008741002163460507\n",
      "LOSS train 0.008741002163460507 valid 0.1645265011068118\n",
      "valid accuracy 97.88\n",
      "test accuracy 97.98\n",
      "EPOCH 295:\n",
      "  batch 100 loss: 0.011247723429138433\n",
      "  batch 200 loss: 0.0058197721981355244\n",
      "  batch 300 loss: 0.005803983138821423\n",
      "LOSS train 0.005803983138821423 valid 0.15506167825911094\n",
      "valid accuracy 97.82\n",
      "test accuracy 98.11\n",
      "EPOCH 296:\n",
      "  batch 100 loss: 0.008062944255264028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 200 loss: 0.005632230749506561\n",
      "  batch 300 loss: 0.006867610244893285\n",
      "LOSS train 0.006867610244893285 valid 0.15670995286199044\n",
      "valid accuracy 97.6\n",
      "test accuracy 97.97\n",
      "EPOCH 297:\n",
      "  batch 100 loss: 0.00863911656711025\n",
      "  batch 200 loss: 0.006031689003548308\n",
      "  batch 300 loss: 0.007441860556816664\n",
      "LOSS train 0.007441860556816664 valid 0.1570693970529128\n",
      "valid accuracy 97.96\n",
      "test accuracy 98.03\n",
      "EPOCH 298:\n",
      "  batch 100 loss: 0.012071734133182872\n",
      "  batch 200 loss: 0.006522710011795425\n",
      "  batch 300 loss: 0.006069924478342727\n",
      "LOSS train 0.006069924478342727 valid 0.14720213603180163\n",
      "valid accuracy 97.83\n",
      "test accuracy 97.92\n",
      "EPOCH 299:\n",
      "  batch 100 loss: 0.011837149854227392\n",
      "  batch 200 loss: 0.0067846789538543815\n",
      "  batch 300 loss: 0.006285742032448524\n",
      "LOSS train 0.006285742032448524 valid 0.14483487131433775\n",
      "valid accuracy 97.92\n",
      "test accuracy 97.92\n",
      "EPOCH 300:\n",
      "  batch 100 loss: 0.007569253519914127\n",
      "  batch 200 loss: 0.004809390626555796\n",
      "  batch 300 loss: 0.005687654467583343\n",
      "LOSS train 0.005687654467583343 valid 0.15891865503733496\n",
      "valid accuracy 97.82\n",
      "test accuracy 98.12\n"
     ]
    }
   ],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/MNIST_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "val_loss = []\n",
    "val_accuracy = []\n",
    "\n",
    "test_loss = []\n",
    "test_accuracy = []\n",
    "\n",
    "EPOCHS = 300\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "num_vbatches = int(len(validation_set) / batch_size) + 1\n",
    "vdenom = 2 ** num_vbatches - 1\n",
    "\n",
    "num_tbatches = int(len(test_data) / batch_size) + 1\n",
    "tdenom = 2 ** num_tbatches - 1\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number, writer)\n",
    "\n",
    "    # We don't need gradients on to do reporting\n",
    "    model.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    correct = 0\n",
    "    for i, vdata in enumerate(validation_loader):\n",
    "        vinputs, vlabels = vdata\n",
    "        vinputs = vinputs.view(-1, 784)\n",
    "        voutputs = model(vinputs)\n",
    "        _, predicted = torch.max(voutputs, 1)\n",
    "        correct += torch.sum(vlabels == predicted)\n",
    "        vce = ce_loss(voutputs, vlabels)\n",
    "        vkl = kl_loss(model)\n",
    "        kl_weight = 2**(num_vbatches-i) / vdenom\n",
    "        vloss = vce + kl_weight * vkl\n",
    "        running_vloss += vloss.item()\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    val_loss.append(avg_vloss)\n",
    "    val_accuracy.append(100 * float(correct)/ len(validation_set))\n",
    "\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "    print('valid accuracy {}'.format(val_accuracy[-1]))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = './models/gaussian-prior/model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    running_tloss = 0.0\n",
    "    correct = 0\n",
    "\n",
    "    for i, tdata in enumerate(test_loader):\n",
    "        tinputs, tlabels = tdata\n",
    "        tinputs = tinputs.view(-1, 784)\n",
    "        toutputs = model(tinputs)\n",
    "        _, predicted = torch.max(toutputs, 1)\n",
    "        correct += torch.sum(tlabels == predicted)\n",
    "        tloss = ce_loss(voutputs, vlabels)\n",
    "        running_tloss += tloss.item()\n",
    "\n",
    "    avg_tloss = running_tloss / (i + 1)\n",
    "\n",
    "    test_loss.append(avg_tloss)\n",
    "    test_accuracy.append(100 * float(correct)/ len(test_data))\n",
    "    print('test accuracy {}'.format(test_accuracy[-1]))\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "data['test-loss'] = test_loss\n",
    "data['test-accuracy'] = test_accuracy\n",
    "data['validation-loss'] = val_loss\n",
    "data['validation-accuracy'] = val_accuracy\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('./models/gaussian-prior/test-loss', 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "miTxSzr8jk1R"
   },
   "source": [
    "# Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uaDECpUpjmuw",
    "outputId": "93d368c0-0e90-4cbf-9dee-668c53e84209"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS test 0.14686702648890415\n",
      "Number of correct predictions 9789\n",
      "Accuracy: 97.89\n"
     ]
    }
   ],
   "source": [
    "model.train(False)\n",
    "\n",
    "running_tloss = 0.0\n",
    "correct = 0\n",
    "\n",
    "num_tbatches = int(len(test_data) / batch_size) + 1\n",
    "tdenom = 2 ** num_tbatches - 1\n",
    "\n",
    "for i, tdata in enumerate(test_loader):\n",
    "    tinputs, tlabels = tdata\n",
    "    tinputs = tinputs.view(-1, 784)\n",
    "    toutputs = model(tinputs)\n",
    "    _, predicted = torch.max(toutputs, 1)\n",
    "    correct += torch.sum(tlabels == predicted)\n",
    "    tce = ce_loss(toutputs, tlabels)\n",
    "    tkl = kl_loss(model)\n",
    "    kl_weight = 2**(num_tbatches - i) / tdenom\n",
    "    tloss = tce + kl_weight * tkl\n",
    "    running_tloss += tloss.item()\n",
    "\n",
    "avg_tloss = running_tloss / (i + 1)\n",
    "print('LOSS test {}'.format(avg_tloss))\n",
    "print(\"Number of correct predictions {}\".format(correct))\n",
    "\n",
    "print(\"Accuracy: {:.2f}\".format(100 * float(correct)/ len(test_data)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './models/gaussian-prior/model_{}_{}'.format(timestamp, epoch_number)\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OHh7JGU9K_ZE"
   },
   "outputs": [],
   "source": [
    "# for step in range(3000):\n",
    "#     pre = model(x)\n",
    "#     ce = ce_loss(pre, y)\n",
    "#     kl = kl_loss(model)\n",
    "#     cost = ce + kl_weight*kl\n",
    "    \n",
    "#     optimizer.zero_grad()\n",
    "#     cost.backward()\n",
    "#     optimizer.step()\n",
    "    \n",
    "# _, predicted = torch.max(pre.data, 1)\n",
    "# total = y.size(0)\n",
    "# correct = (predicted == y).sum()\n",
    "# print('- Accuracy: %f %%' % (100 * float(correct) / total))\n",
    "# print('- CE : %2.2f, KL : %2.2f' % (ce.item(), kl.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HUKaRHNaLWPq"
   },
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cA6ecKiZLBwG"
   },
   "outputs": [],
   "source": [
    "# def draw_plot(predicted) :\n",
    "#     fig = plt.figure(figsize = (16, 5))\n",
    "\n",
    "#     ax1 = fig.add_subplot(1, 2, 1)\n",
    "#     ax2 = fig.add_subplot(1, 2, 2)\n",
    "\n",
    "#     z1_plot = ax1.scatter(X[:, 0], X[:, 1], c = Y)\n",
    "#     z2_plot = ax2.scatter(X[:, 0], X[:, 1], c = predicted)\n",
    "\n",
    "#     plt.colorbar(z1_plot,ax=ax1)\n",
    "#     plt.colorbar(z2_plot,ax=ax2)\n",
    "\n",
    "#     ax1.set_title(\"REAL\")\n",
    "#     ax2.set_title(\"PREDICT\")\n",
    "\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JDz-mKj0LGZz"
   },
   "outputs": [],
   "source": [
    "# pre = model(x)\n",
    "# _, predicted = torch.max(pre.data, 1)\n",
    "# draw_plot(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gUnK7u3ZLJBy"
   },
   "outputs": [],
   "source": [
    "# Bayesian Neural Network will return different outputs even if inputs are same.\n",
    "# In other words, different plots will be shown every time forward method is called.\n",
    "# pre = model(x)\n",
    "# _, predicted = torch.max(pre.data, 1)\n",
    "# draw_plot(predicted)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
