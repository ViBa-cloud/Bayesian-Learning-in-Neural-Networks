{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gg9NpFEcKJ7h",
    "outputId": "75f81b97-98e2-4ec4-f33e-92a2bd789cda"
   },
   "outputs": [],
   "source": [
    "# !pip install torchbnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "4x1hZB-iJ47m"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import torchbnn as bnn\n",
    "from torchbnn.modules.loss import _Loss\n",
    "\n",
    "import math\n",
    "\n",
    "from torch.nn import Module, Parameter\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "hoNzE_BiKPlS"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "JMuxrQaRLhJR"
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "input_dim = 784\n",
    "\n",
    "# Hyperparameters\n",
    "num_samples = 2\n",
    "lr = 1e-3\n",
    "log_sigma1 = -3\n",
    "log_sigma2 = -8\n",
    "pi = 0.5\n",
    "hidden_dim = 1200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3LmpHydwKSPW",
    "outputId": "948f9706-0545-4a74-ee30-6b2958cac7e5"
   },
   "outputs": [],
   "source": [
    "# iris = datasets.load_iris()\n",
    "training_data = datasets.MNIST(root='data', train=True, download=True, transform=ToTensor())\n",
    "training_set, validation_set = torch.utils.data.random_split(training_data, [50000, 10000])\n",
    "test_data = datasets.MNIST(root='data', train=False, download=True, transform=ToTensor())\n",
    "\n",
    "training_loader = DataLoader(training_set, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_set, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "1zMGuntBNrZ4"
   },
   "outputs": [],
   "source": [
    "num_trainbatches = int(len(training_set) / batch_size) + 1\n",
    "train_denom = 2 ** num_trainbatches  - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 675
    },
    "id": "W1t4gjGjZv5m",
    "outputId": "f6bf628e-5e8f-4027-f5f8-45a506b48a9b"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x800 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the data\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img, label = training_data[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(label)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "# No need to normalize. The data values are already between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RMu3DnfuK02O",
    "outputId": "9297cf37-70bf-42d3-e4de-dd26e6dc169f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 10000\n"
     ]
    }
   ],
   "source": [
    "# print(img.shape)\n",
    "print(len(training_set),len(validation_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "Y8E9kyv3kk_C"
   },
   "outputs": [],
   "source": [
    "class BayesLinearMixture(Module):\n",
    "    r\"\"\"\n",
    "    Applies Bayesian Linear\n",
    "    Arguments:\n",
    "        prior_mu (Float): mean of prior normal distribution.\n",
    "        prior_sigma (Float): sigma of prior normal distribution.\n",
    "    .. note:: other arguments are following linear of pytorch 1.2.0.\n",
    "    https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/linear.py\n",
    "    \n",
    "    \"\"\"\n",
    "    __constants__ = ['prior_mu', 'prior_sigma', 'bias', 'in_features', 'out_features']\n",
    "\n",
    "    def __init__(self, prior_mu1, prior_sigma1, prior_mu2, prior_sigma2, pi, in_features, out_features):\n",
    "        super(BayesLinearMixture, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        self.prior_mu1 = prior_mu1\n",
    "        self.prior_sigma1 = prior_sigma1\n",
    "        self.prior_log_sigma1 = math.log(prior_sigma1)\n",
    "\n",
    "        self.prior_mu2 = prior_mu2\n",
    "        self.prior_sigma2 = prior_sigma2\n",
    "        self.prior_log_sigma2 = math.log(prior_sigma2)\n",
    "\n",
    "        self.pi = pi\n",
    "\n",
    "        self.weight_mu = Parameter(torch.Tensor(out_features, in_features))\n",
    "#         self.weight_log_sigma = Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.weight_rho = Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.register_buffer('weight_eps', None)\n",
    "\n",
    "#         if bias is None or bias is False :\n",
    "#             self.bias = False\n",
    "#         else :\n",
    "#             self.bias = True\n",
    "\n",
    "#         if self.bias:\n",
    "#             self.bias_mu = Parameter(torch.Tensor(out_features))\n",
    "#             self.bias_log_sigma = Parameter(torch.Tensor(out_features))\n",
    "#             self.register_buffer('bias_eps', None)\n",
    "#         else:\n",
    "#             self.register_parameter('bias_mu', None)\n",
    "#             self.register_parameter('bias_log_sigma', None)\n",
    "#             self.register_buffer('bias_eps', None)\n",
    "            \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Initialization method of Adv-BNN\n",
    "        stdv = 1. / math.sqrt(self.weight_mu.size(1))\n",
    "        self.weight_mu.data.uniform_(-stdv, stdv)\n",
    "        self.weight_rho.data.fill_(self.prior_log_sigma1)\n",
    "#         if self.bias :\n",
    "#             self.bias_mu.data.uniform_(-stdv, stdv)\n",
    "#             self.bias_log_sigma.data.fill_(self.prior_log_sigma1)\n",
    "  \n",
    "    def freeze(self) :\n",
    "        self.weight_eps = torch.randn_like(self.weight_log_sigma)\n",
    "#         if self.bias :\n",
    "#             self.bias_eps = torch.randn_like(self.bias_log_sigma)\n",
    "        \n",
    "    def unfreeze(self) :\n",
    "        self.weight_eps = None\n",
    "#         if self.bias :\n",
    "#             self.bias_eps = None \n",
    "            \n",
    "    def forward(self, input):\n",
    "        r\"\"\"\n",
    "        Overriden.\n",
    "        \"\"\"\n",
    "#         if self.weight_eps is None :\n",
    "        eps = torch.torch.randn_like(self.weight_rho)\n",
    "        self.weight = self.weight_mu + torch.log1p(torch.exp(self.weight_rho)) * eps\n",
    "#         else :\n",
    "#             self.weight = self.weight_mu + torch.exp(self.weight_log_sigma) * self.weight_eps\n",
    "        \n",
    "#         if self.bias:\n",
    "#             if self.bias_eps is None :\n",
    "#                 bias = self.bias_mu + torch.exp(self.bias_log_sigma) * torch.randn_like(self.bias_log_sigma)\n",
    "#             else :\n",
    "#                 bias = self.bias_mu + torch.exp(self.bias_log_sigma) * self.bias_eps                \n",
    "#         else :\n",
    "#             bias = None\n",
    "\n",
    "        return F.linear(input, self.weight, bias=None)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        r\"\"\"\n",
    "        Overriden.\n",
    "        \"\"\"\n",
    "        return 'prior_mu1={}, prior_sigma1={}, prior_mu2={}, prior_sigma2={}, in_features={}, out_features={}'.format(self.prior_mu1, self.prior_sigma1, self.prior_mu2, self.prior_sigma2, self.in_features, self.out_features)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "xcy4K2S4ilma"
   },
   "outputs": [],
   "source": [
    "def _kl_loss_mixture(weights, weight_mu, weight_rho, sigma_1, sigma_2, pi):\n",
    "    n = len(weight_mu.view(-1))\n",
    "    weight_sigma = torch.log1p(torch.exp(weight_rho))\n",
    "#   kl = -n/2*math.log(2*math.pi) - weight_log_sigma - torch.square(torch.linalg.norm(weights - weight_mu))/(2*weight_sigma**2)\n",
    "    kl = -torch.log(weight_sigma) - (weights - weight_mu) ** 2/(2 * weight_sigma**2)\n",
    "#   N1 = torch.distributions.Normal(0, scale=sigma_1)\n",
    "#   N2 = torch.distributions.Normal(0, scale=sigma_2)\n",
    "#   pdf1 = torch.exp(N1.log_prob(weights))\n",
    "#   pdf2 = torch.exp(N2.log_prob(weights))\n",
    "#   pdf1 = 1 / sigma_1 * torch.exp(-torch.dot(weights.view(-1), weights.view(-1))/(2*sigma_1**2))\n",
    "#   pdf2 = 1 / sigma_2 * torch.exp(-torch.dot(weights.view(-1), weights.view(-1))/(2*sigma_2**2))\n",
    "    pdf1 = 1 / sigma_1 * torch.exp(-(weights ** 2)/(2*sigma_1**2))\n",
    "    pdf2 = 1 / sigma_2 * torch.exp(-(weights ** 2)/(2*sigma_2**2))\n",
    "\n",
    "#   import pdb; pdb.set_trace()\n",
    "  \n",
    "    prior_log_likelihood = (torch.log(pi*pdf1 + (1-pi)*pdf2))\n",
    "\n",
    "    kl = kl - prior_log_likelihood\n",
    "\n",
    "    return kl.sum()\n",
    "\n",
    "def _bayesian_kl_loss_mixture(model, reduction='mean', last_layer_only=False):\n",
    "    device = torch.device(\"cuda\" if next(model.parameters()).is_cuda else \"cpu\")\n",
    "    kl = torch.Tensor([0]).to(device)\n",
    "    kl_sum = torch.Tensor([0]).to(device)\n",
    "    n = torch.Tensor([0]).to(device)\n",
    "\n",
    "    for m in model.modules() :\n",
    "        if isinstance(m, BayesLinearMixture):\n",
    "            kl = _kl_loss_mixture(m.weight, m.weight_mu, m.weight_rho, m.prior_sigma1, m.prior_sigma2, m.pi)\n",
    "            kl_sum += kl\n",
    "            n += len(m.weight_mu.view(-1))\n",
    "\n",
    "        if last_layer_only or n == 0 :\n",
    "            return kl\n",
    "\n",
    "        if reduction == 'mean' :\n",
    "            return kl_sum/n\n",
    "        elif reduction == 'sum' :\n",
    "            return kl_sum\n",
    "        else :\n",
    "            raise ValueError(reduction + \" is not valid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "zIYQHeWatX1g"
   },
   "outputs": [],
   "source": [
    "class BKLLossMixture(_Loss):\n",
    "    \"\"\"\n",
    "    Loss for calculating KL divergence of baysian neural network model.\n",
    "    Arguments:\n",
    "        reduction (string, optional): Specifies the reduction to apply to the output:\n",
    "            ``'mean'``: the sum of the output will be divided by the number of\n",
    "            elements of the output.\n",
    "            ``'sum'``: the output will be summed.\n",
    "        last_layer_only (Bool): True for return only the last layer's KL divergence.    \n",
    "    \"\"\"\n",
    "    __constants__ = ['reduction']\n",
    "\n",
    "    def __init__(self, reduction='mean', last_layer_only=False):\n",
    "        super(BKLLossMixture, self).__init__(reduction)\n",
    "        self.last_layer_only = last_layer_only\n",
    "\n",
    "    def forward(self, model):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            model (nn.Module): a model to be calculated for KL-divergence.\n",
    "        \"\"\"\n",
    "        return _bayesian_kl_loss_mixture(model, reduction=self.reduction, last_layer_only=self.last_layer_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "h1zQZNEiK5Hn"
   },
   "outputs": [],
   "source": [
    "sigma1 = 10 ** log_sigma1\n",
    "sigma2 = 10 ** log_sigma2\n",
    "\n",
    "model = nn.Sequential(\n",
    "    BayesLinearMixture(prior_mu1=0, prior_sigma1=sigma1, prior_mu2=0, prior_sigma2=sigma2, pi=pi, in_features=input_dim, out_features=hidden_dim),\n",
    "    nn.ReLU(),\n",
    "    BayesLinearMixture(prior_mu1=0, prior_sigma1=sigma1, prior_mu2=0, prior_sigma2=sigma2, pi=pi, in_features=hidden_dim, out_features=hidden_dim),\n",
    "    nn.ReLU(),\n",
    "    BayesLinearMixture(prior_mu1=0, prior_sigma1=sigma1, prior_mu2=0, prior_sigma2=sigma2, pi=pi, in_features=hidden_dim, out_features=num_classes),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "TKuZ_u5aK7O7"
   },
   "outputs": [],
   "source": [
    "ce_loss = nn.CrossEntropyLoss()\n",
    "kl_loss = BKLLossMixture(reduction='mean', last_layer_only=False)\n",
    "# kl_weight = 0.01\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "device = torch.device(\"cuda\" if next(model.parameters()).is_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "hTdPC-GWedSM"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(training_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "\n",
    "        inputs = inputs.view(-1, 784)\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        ################## WE CAN AVERAGE ACROSS MULTIPLE W SAMPLES HERE BEFORE DOING loss.backward() #########################\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Compute the loss and its gradients\n",
    "        loss = torch.Tensor([0]).to(device)\n",
    "        for _ in range(num_samples):\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Compute the loss and its gradients\n",
    "            ce = ce_loss(outputs, labels)\n",
    "            kl = kl_loss(model)\n",
    "            kl_weight = 2 ** (num_trainbatches-i) / train_denom\n",
    "            loss += (ce + kl_weight * kl) / num_samples\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            last_loss = running_loss / 100 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(training_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vKLhziHlfCyw",
    "outputId": "b9a4fc32-451a-4c52-f563-66fb46fdfcf5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 100 loss: 0.4638574641942978\n",
      "  batch 200 loss: 0.19144128061830998\n",
      "  batch 300 loss: 0.13972640791907906\n",
      "LOSS train 0.13972640791907906 valid 0.11967141803684114\n",
      "valid accuracy 96.32\n",
      "test accuracy 96.68\n",
      "EPOCH 2:\n",
      "  batch 100 loss: 0.09318956527858972\n",
      "  batch 200 loss: 0.08214469512924552\n",
      "  batch 300 loss: 0.08633777890354395\n",
      "LOSS train 0.08633777890354395 valid 0.10002257346317078\n",
      "valid accuracy 96.99\n",
      "test accuracy 97.3\n",
      "EPOCH 3:\n",
      "  batch 100 loss: 0.048344376310706136\n",
      "  batch 200 loss: 0.05518466750625521\n",
      "  batch 300 loss: 0.05994015044067055\n",
      "LOSS train 0.05994015044067055 valid 0.09462209396017125\n",
      "valid accuracy 97.25\n",
      "test accuracy 97.35\n",
      "EPOCH 4:\n",
      "  batch 100 loss: 0.03632703804876655\n",
      "  batch 200 loss: 0.035426440350711345\n",
      "  batch 300 loss: 0.039114915903192014\n",
      "LOSS train 0.039114915903192014 valid 0.0947382135664502\n",
      "valid accuracy 97.44\n",
      "test accuracy 97.65\n",
      "EPOCH 5:\n",
      "  batch 100 loss: 0.021952035996946506\n",
      "  batch 200 loss: 0.03615982187911868\n",
      "  batch 300 loss: 0.03689312732662074\n",
      "LOSS train 0.03689312732662074 valid 0.09264995367487279\n",
      "valid accuracy 97.48\n",
      "test accuracy 97.93\n",
      "EPOCH 6:\n",
      "  batch 100 loss: 0.02138819177635014\n",
      "  batch 200 loss: 0.019687154406274204\n",
      "  batch 300 loss: 0.02777614811435342\n",
      "LOSS train 0.02777614811435342 valid 0.10155750663269125\n",
      "valid accuracy 97.34\n",
      "test accuracy 97.74\n",
      "EPOCH 7:\n",
      "  batch 100 loss: 0.013269533081620466\n",
      "  batch 200 loss: 0.018479122369317338\n",
      "  batch 300 loss: 0.024358997988165355\n",
      "LOSS train 0.024358997988165355 valid 0.10314682908643544\n",
      "valid accuracy 97.58\n",
      "test accuracy 97.53\n",
      "EPOCH 8:\n",
      "  batch 100 loss: 0.01563143545819912\n",
      "  batch 200 loss: 0.022323724592570215\n",
      "  batch 300 loss: 0.014027836619643495\n",
      "LOSS train 0.014027836619643495 valid 0.11268466233013896\n",
      "valid accuracy 97.29\n",
      "test accuracy 97.68\n",
      "EPOCH 9:\n",
      "  batch 100 loss: 0.019208660895819775\n",
      "  batch 200 loss: 0.013712158103007823\n",
      "  batch 300 loss: 0.015426769208279439\n",
      "LOSS train 0.015426769208279439 valid 0.1153596126197401\n",
      "valid accuracy 97.62\n",
      "test accuracy 97.86\n",
      "EPOCH 10:\n",
      "  batch 100 loss: 0.012184590630058665\n",
      "  batch 200 loss: 0.011138224581081886\n",
      "  batch 300 loss: 0.018011265447712504\n",
      "LOSS train 0.018011265447712504 valid 0.11233635984631349\n",
      "valid accuracy 97.58\n",
      "test accuracy 97.69\n",
      "EPOCH 11:\n",
      "  batch 100 loss: 0.012535324479104019\n",
      "  batch 200 loss: 0.010257327895160415\n",
      "  batch 300 loss: 0.013185783340304624\n",
      "LOSS train 0.013185783340304624 valid 0.11816835538997388\n",
      "valid accuracy 97.54\n",
      "test accuracy 97.83\n",
      "EPOCH 12:\n",
      "  batch 100 loss: 0.011317857666072087\n",
      "  batch 200 loss: 0.01307162149299984\n",
      "  batch 300 loss: 0.007247088348303805\n",
      "LOSS train 0.007247088348303805 valid 0.14927585279083014\n",
      "valid accuracy 97.31\n",
      "test accuracy 97.64\n",
      "EPOCH 13:\n",
      "  batch 100 loss: 0.015354366097817547\n",
      "  batch 200 loss: 0.012140092281188118\n",
      "  batch 300 loss: 0.009743668441224144\n",
      "LOSS train 0.009743668441224144 valid 0.13662869847683862\n",
      "valid accuracy 97.6\n",
      "test accuracy 97.72\n",
      "EPOCH 14:\n",
      "  batch 100 loss: 0.009828309539079783\n",
      "  batch 200 loss: 0.01542713620030554\n",
      "  batch 300 loss: 0.027077184499212308\n",
      "LOSS train 0.027077184499212308 valid 0.12298527451326315\n",
      "valid accuracy 97.76\n",
      "test accuracy 97.99\n",
      "EPOCH 15:\n",
      "  batch 100 loss: 0.007214097613614285\n",
      "  batch 200 loss: 0.00575577370937026\n",
      "  batch 300 loss: 0.008798982343723765\n",
      "LOSS train 0.008798982343723765 valid 0.12611407704121178\n",
      "valid accuracy 97.83\n",
      "test accuracy 98.02\n",
      "EPOCH 16:\n",
      "  batch 100 loss: 0.004707894970624693\n",
      "  batch 200 loss: 0.005096236460822183\n",
      "  batch 300 loss: 0.009472253167623422\n",
      "LOSS train 0.009472253167623422 valid 0.1620506725638492\n",
      "valid accuracy 97.12\n",
      "test accuracy 97.35\n",
      "EPOCH 17:\n",
      "  batch 100 loss: 0.01071308679654976\n",
      "  batch 200 loss: 0.010318423039316259\n",
      "  batch 300 loss: 0.010780068338644923\n",
      "LOSS train 0.010780068338644923 valid 0.12953587114803952\n",
      "valid accuracy 97.86\n",
      "test accuracy 98.21\n",
      "EPOCH 18:\n",
      "  batch 100 loss: 0.012462221796740778\n",
      "  batch 200 loss: 0.010661050143271495\n",
      "  batch 300 loss: 0.011362419972429053\n",
      "LOSS train 0.011362419972429053 valid 0.12703328564927646\n",
      "valid accuracy 97.75\n",
      "test accuracy 98.04\n",
      "EPOCH 19:\n",
      "  batch 100 loss: 0.007911612615462218\n",
      "  batch 200 loss: 0.006438722704915563\n",
      "  batch 300 loss: 0.01157605666670861\n",
      "LOSS train 0.01157605666670861 valid 0.13549978791992476\n",
      "valid accuracy 97.83\n",
      "test accuracy 98.04\n",
      "EPOCH 20:\n",
      "  batch 100 loss: 0.010423493384910216\n",
      "  batch 200 loss: 0.012473310117366054\n",
      "  batch 300 loss: 0.009617087289461779\n",
      "LOSS train 0.009617087289461779 valid 0.13310784854249644\n",
      "valid accuracy 97.74\n",
      "test accuracy 98.24\n",
      "EPOCH 21:\n",
      "  batch 100 loss: 0.0042553402781891235\n",
      "  batch 200 loss: 0.009421435226249742\n",
      "  batch 300 loss: 0.007315691446474375\n",
      "LOSS train 0.007315691446474375 valid 0.12771529570440918\n",
      "valid accuracy 97.9\n",
      "test accuracy 98.2\n",
      "EPOCH 22:\n",
      "  batch 100 loss: 0.009303359876585092\n",
      "  batch 200 loss: 0.008817034967451036\n",
      "  batch 300 loss: 0.005945454692123348\n",
      "LOSS train 0.005945454692123348 valid 0.12218986730216715\n",
      "valid accuracy 97.94\n",
      "test accuracy 98.2\n",
      "EPOCH 23:\n",
      "  batch 100 loss: 0.004623325039942756\n",
      "  batch 200 loss: 0.009918244179916656\n",
      "  batch 300 loss: 0.012732761765855685\n",
      "LOSS train 0.012732761765855685 valid 0.12719967697755893\n",
      "valid accuracy 97.91\n",
      "test accuracy 97.97\n",
      "EPOCH 24:\n",
      "  batch 100 loss: 0.0077413359023148585\n",
      "  batch 200 loss: 0.006613159515377447\n",
      "  batch 300 loss: 0.007774591071670329\n",
      "LOSS train 0.007774591071670329 valid 0.13701266294193845\n",
      "valid accuracy 97.94\n",
      "test accuracy 98.17\n",
      "EPOCH 25:\n",
      "  batch 100 loss: 0.0035463127197613176\n",
      "  batch 200 loss: 0.0047658156587294795\n",
      "  batch 300 loss: 0.0034765242409673645\n",
      "LOSS train 0.0034765242409673645 valid 0.14302555645581724\n",
      "valid accuracy 97.96\n",
      "test accuracy 98.06\n",
      "EPOCH 26:\n",
      "  batch 100 loss: 0.006119891931507482\n",
      "  batch 200 loss: 0.00140360001266572\n",
      "  batch 300 loss: 0.0035148426974888026\n",
      "LOSS train 0.0035148426974888026 valid 0.1283870315676181\n",
      "valid accuracy 98.11\n",
      "test accuracy 98.25\n",
      "EPOCH 27:\n",
      "  batch 100 loss: 0.00437668517057773\n",
      "  batch 200 loss: 0.0038271223934415845\n",
      "  batch 300 loss: 0.01086801063471171\n",
      "LOSS train 0.01086801063471171 valid 0.1707629106902436\n",
      "valid accuracy 97.64\n",
      "test accuracy 97.92\n",
      "EPOCH 28:\n",
      "  batch 100 loss: 0.015792236731481352\n",
      "  batch 200 loss: 0.015974518415996498\n",
      "  batch 300 loss: 0.007614856771524501\n",
      "LOSS train 0.007614856771524501 valid 0.13266371808672778\n",
      "valid accuracy 98.07\n",
      "test accuracy 97.93\n",
      "EPOCH 29:\n",
      "  batch 100 loss: 0.012331332356322946\n",
      "  batch 200 loss: 0.005972734516414562\n",
      "  batch 300 loss: 0.009436144089024765\n",
      "LOSS train 0.009436144089024765 valid 0.1500829907907453\n",
      "valid accuracy 98.0\n",
      "test accuracy 98.23\n",
      "EPOCH 30:\n",
      "  batch 100 loss: 0.0044349562663333585\n",
      "  batch 200 loss: 0.006139669837790507\n",
      "  batch 300 loss: 0.007277279637132778\n",
      "LOSS train 0.007277279637132778 valid 0.1624420768583357\n",
      "valid accuracy 97.86\n",
      "test accuracy 97.94\n",
      "EPOCH 31:\n",
      "  batch 100 loss: 0.0028280856525402954\n",
      "  batch 200 loss: 0.00506016284621353\n",
      "  batch 300 loss: 0.004272528876856541\n",
      "LOSS train 0.004272528876856541 valid 0.17762576342182554\n",
      "valid accuracy 97.68\n",
      "test accuracy 98.13\n",
      "EPOCH 32:\n",
      "  batch 100 loss: 0.0038339115286999004\n",
      "  batch 200 loss: 0.005825506100870257\n",
      "  batch 300 loss: 0.008204134818605838\n",
      "LOSS train 0.008204134818605838 valid 0.15773181636582415\n",
      "valid accuracy 97.92\n",
      "test accuracy 98.02\n",
      "EPOCH 33:\n",
      "  batch 100 loss: 0.005918800678666685\n",
      "  batch 200 loss: 0.01213782542541594\n",
      "  batch 300 loss: 0.012455907522615349\n",
      "LOSS train 0.012455907522615349 valid 0.19304913459764292\n",
      "valid accuracy 97.67\n",
      "test accuracy 97.79\n",
      "EPOCH 34:\n",
      "  batch 100 loss: 0.010120526821604017\n",
      "  batch 200 loss: 0.005438831608271358\n",
      "  batch 300 loss: 0.0039794749968075395\n",
      "LOSS train 0.0039794749968075395 valid 0.15082529725448315\n",
      "valid accuracy 98.06\n",
      "test accuracy 98.29\n",
      "EPOCH 35:\n",
      "  batch 100 loss: 0.003840426447250138\n",
      "  batch 200 loss: 0.0019396376382718472\n",
      "  batch 300 loss: 0.004101520063331918\n",
      "LOSS train 0.004101520063331918 valid 0.1624643744519894\n",
      "valid accuracy 98.0\n",
      "test accuracy 97.71\n",
      "EPOCH 36:\n",
      "  batch 100 loss: 0.009247197880086447\n",
      "  batch 200 loss: 0.003658227523913524\n",
      "  batch 300 loss: 0.011913625812018722\n",
      "LOSS train 0.011913625812018722 valid 0.16895207428742537\n",
      "valid accuracy 97.93\n",
      "test accuracy 98.14\n",
      "EPOCH 37:\n",
      "  batch 100 loss: 0.004547974902732221\n",
      "  batch 200 loss: 0.007214833534286953\n",
      "  batch 300 loss: 0.0075774896098016594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 0.0075774896098016594 valid 0.16738797156223034\n",
      "valid accuracy 98.09\n",
      "test accuracy 98.34\n",
      "EPOCH 38:\n",
      "  batch 100 loss: 0.0022180961523550024\n",
      "  batch 200 loss: 0.0032199835631198325\n",
      "  batch 300 loss: 0.0019884047622349323\n",
      "LOSS train 0.0019884047622349323 valid 0.1691542980161448\n",
      "valid accuracy 97.99\n",
      "test accuracy 98.13\n",
      "EPOCH 39:\n",
      "  batch 100 loss: 0.003560162098497983\n",
      "  batch 200 loss: 0.0073972963313534024\n",
      "  batch 300 loss: 0.0018473853014586439\n",
      "LOSS train 0.0018473853014586439 valid 0.15566921943602635\n",
      "valid accuracy 98.11\n",
      "test accuracy 98.4\n",
      "EPOCH 40:\n",
      "  batch 100 loss: 0.004325006869437615\n",
      "  batch 200 loss: 0.0070823123103017594\n",
      "  batch 300 loss: 0.008401792069429348\n",
      "LOSS train 0.008401792069429348 valid 0.19518348054194415\n",
      "valid accuracy 98.05\n",
      "test accuracy 98.29\n",
      "EPOCH 41:\n",
      "  batch 100 loss: 0.0010763668249876446\n",
      "  batch 200 loss: 0.005665572122248559\n",
      "  batch 300 loss: 0.007012058591246841\n",
      "LOSS train 0.007012058591246841 valid 0.20205999622706983\n",
      "valid accuracy 97.74\n",
      "test accuracy 97.78\n",
      "EPOCH 42:\n",
      "  batch 100 loss: 0.010945626802432287\n",
      "  batch 200 loss: 0.013645178708153481\n",
      "  batch 300 loss: 0.013217463103974864\n",
      "LOSS train 0.013217463103974864 valid 0.18124909801734843\n",
      "valid accuracy 98.09\n",
      "test accuracy 98.2\n",
      "EPOCH 43:\n",
      "  batch 100 loss: 0.00982604221671238\n",
      "  batch 200 loss: 0.006218230705371042\n",
      "  batch 300 loss: 0.005227284782309951\n",
      "LOSS train 0.005227284782309951 valid 0.19352250189769873\n",
      "valid accuracy 97.96\n",
      "test accuracy 98.07\n",
      "EPOCH 44:\n",
      "  batch 100 loss: 0.005747525263465931\n",
      "  batch 200 loss: 0.002386130403183344\n",
      "  batch 300 loss: 0.0031824431181814816\n",
      "LOSS train 0.0031824431181814816 valid 0.18369049386669659\n",
      "valid accuracy 98.0\n",
      "test accuracy 98.0\n",
      "EPOCH 45:\n",
      "  batch 100 loss: 0.005151756851206191\n",
      "  batch 200 loss: 0.004525160794131579\n",
      "  batch 300 loss: 0.00199667117654144\n",
      "LOSS train 0.00199667117654144 valid 0.16086174821306418\n",
      "valid accuracy 98.09\n",
      "test accuracy 98.05\n",
      "EPOCH 46:\n",
      "  batch 100 loss: 0.0036921074767940353\n",
      "  batch 200 loss: 0.0030806282012863662\n",
      "  batch 300 loss: 0.004916358308847748\n",
      "LOSS train 0.004916358308847748 valid 0.1629865336104512\n",
      "valid accuracy 98.28\n",
      "test accuracy 98.3\n",
      "EPOCH 47:\n",
      "  batch 100 loss: 0.00031772750767590453\n",
      "  batch 200 loss: 0.003063333612285426\n",
      "  batch 300 loss: 0.002009692369431342\n",
      "LOSS train 0.002009692369431342 valid 0.16011131531278816\n",
      "valid accuracy 98.31\n",
      "test accuracy 98.23\n",
      "EPOCH 48:\n",
      "  batch 100 loss: 0.0004948806906723036\n",
      "  batch 200 loss: 0.002485997030484306\n",
      "  batch 300 loss: 0.0015494503048112307\n",
      "LOSS train 0.0015494503048112307 valid 0.18291277574603038\n",
      "valid accuracy 98.06\n",
      "test accuracy 98.21\n",
      "EPOCH 49:\n",
      "  batch 100 loss: 0.005501518286306464\n",
      "  batch 200 loss: 0.013128015123173924\n",
      "  batch 300 loss: 0.011388256540200814\n",
      "LOSS train 0.011388256540200814 valid 0.24318564938723192\n",
      "valid accuracy 97.33\n",
      "test accuracy 97.68\n",
      "EPOCH 50:\n",
      "  batch 100 loss: 0.016697179278107797\n",
      "  batch 200 loss: 0.009743321503610786\n",
      "  batch 300 loss: 0.005340094198227234\n",
      "LOSS train 0.005340094198227234 valid 0.15582046942673985\n",
      "valid accuracy 98.22\n",
      "test accuracy 98.22\n",
      "EPOCH 51:\n",
      "  batch 100 loss: 0.00014940738076873572\n",
      "  batch 200 loss: 0.0010027926805619813\n",
      "  batch 300 loss: 0.00027359457523758125\n",
      "LOSS train 0.00027359457523758125 valid 0.16374999322988582\n",
      "valid accuracy 98.21\n",
      "test accuracy 98.24\n",
      "EPOCH 52:\n",
      "  batch 100 loss: 0.0021539032376261245\n",
      "  batch 200 loss: 0.0018892592036979926\n",
      "  batch 300 loss: 0.0024234445780198397\n",
      "LOSS train 0.0024234445780198397 valid 0.16454423348941247\n",
      "valid accuracy 98.24\n",
      "test accuracy 98.38\n",
      "EPOCH 53:\n",
      "  batch 100 loss: 3.218628610866858e-05\n",
      "  batch 200 loss: 9.2054742775467e-05\n",
      "  batch 300 loss: 0.0002878861260381527\n",
      "LOSS train 0.0002878861260381527 valid 0.1711686510216744\n",
      "valid accuracy 98.21\n",
      "test accuracy 98.43\n",
      "EPOCH 54:\n",
      "  batch 100 loss: 0.00019444510285314463\n",
      "  batch 200 loss: 0.0006111956359955007\n",
      "  batch 300 loss: 5.544239844153864e-05\n",
      "LOSS train 5.544239844153864e-05 valid 0.15603047599129174\n",
      "valid accuracy 98.29\n",
      "test accuracy 98.39\n",
      "EPOCH 55:\n",
      "  batch 100 loss: 0.0001024901292392677\n",
      "  batch 200 loss: 0.004519334334472376\n",
      "  batch 300 loss: 0.0127147765532969\n",
      "LOSS train 0.0127147765532969 valid 0.2891208382494323\n",
      "valid accuracy 97.2\n",
      "test accuracy 97.4\n",
      "EPOCH 56:\n",
      "  batch 100 loss: 0.022324794677280407\n",
      "  batch 200 loss: 0.015346774609454314\n",
      "  batch 300 loss: 0.016323823642940452\n",
      "LOSS train 0.016323823642940452 valid 0.20088326015048794\n",
      "valid accuracy 97.7\n",
      "test accuracy 97.81\n",
      "EPOCH 57:\n",
      "  batch 100 loss: 0.008451783826712784\n",
      "  batch 200 loss: 0.006415094772847851\n",
      "  batch 300 loss: 0.0018733136552802955\n",
      "LOSS train 0.0018733136552802955 valid 0.20890592962806998\n",
      "valid accuracy 97.84\n",
      "test accuracy 98.32\n",
      "EPOCH 58:\n",
      "  batch 100 loss: 0.000625870475169279\n",
      "  batch 200 loss: 0.000141255897149426\n",
      "  batch 300 loss: 0.001696789585149787\n",
      "LOSS train 0.001696789585149787 valid 0.1991284014650923\n",
      "valid accuracy 98.09\n",
      "test accuracy 98.15\n",
      "EPOCH 59:\n",
      "  batch 100 loss: 0.0010359113887850797\n",
      "  batch 200 loss: 0.0005810238936488865\n",
      "  batch 300 loss: 0.003613856874880962\n",
      "LOSS train 0.003613856874880962 valid 0.20265140299128073\n",
      "valid accuracy 97.97\n",
      "test accuracy 97.99\n",
      "EPOCH 60:\n",
      "  batch 100 loss: 0.002369831538695948\n",
      "  batch 200 loss: 0.00315423458001554\n",
      "  batch 300 loss: 0.00472644656725123\n",
      "LOSS train 0.00472644656725123 valid 0.1850742330046885\n",
      "valid accuracy 98.17\n",
      "test accuracy 98.25\n",
      "EPOCH 61:\n",
      "  batch 100 loss: 0.000730584471835618\n",
      "  batch 200 loss: 0.00019578151152734591\n",
      "  batch 300 loss: 0.004554730854611744\n",
      "LOSS train 0.004554730854611744 valid 0.20009293477566725\n",
      "valid accuracy 98.01\n",
      "test accuracy 98.09\n",
      "EPOCH 62:\n",
      "  batch 100 loss: 0.002639079105645479\n",
      "  batch 200 loss: 0.010173810720323058\n",
      "  batch 300 loss: 0.002851558000431318\n",
      "LOSS train 0.002851558000431318 valid 0.2189695860811212\n",
      "valid accuracy 98.19\n",
      "test accuracy 98.13\n",
      "EPOCH 63:\n",
      "  batch 100 loss: 0.00118942203467985\n",
      "  batch 200 loss: 0.0022150776089898904\n",
      "  batch 300 loss: 0.014956902484642286\n",
      "LOSS train 0.014956902484642286 valid 0.1946629697530038\n",
      "valid accuracy 98.15\n",
      "test accuracy 98.21\n",
      "EPOCH 64:\n",
      "  batch 100 loss: 0.003843526489234801\n",
      "  batch 200 loss: 0.003693135234500282\n",
      "  batch 300 loss: 0.0019736875967210745\n",
      "LOSS train 0.0019736875967210745 valid 0.21430314703509426\n",
      "valid accuracy 98.18\n",
      "test accuracy 98.25\n",
      "EPOCH 65:\n",
      "  batch 100 loss: 0.0016627159014813708\n",
      "  batch 200 loss: 0.013829803912956842\n",
      "  batch 300 loss: 0.014511336516029\n",
      "LOSS train 0.014511336516029 valid 0.24652854192043125\n",
      "valid accuracy 97.77\n",
      "test accuracy 98.03\n",
      "EPOCH 66:\n",
      "  batch 100 loss: 0.0015014100131419106\n",
      "  batch 200 loss: 0.003191303341667542\n",
      "  batch 300 loss: 0.0016806181465788583\n",
      "LOSS train 0.0016806181465788583 valid 0.22224449238838365\n",
      "valid accuracy 98.1\n",
      "test accuracy 98.19\n",
      "EPOCH 67:\n",
      "  batch 100 loss: 0.0015756507882768035\n",
      "  batch 200 loss: 0.001862309061902659\n",
      "  batch 300 loss: 0.004054121275927253\n",
      "LOSS train 0.004054121275927253 valid 0.24812348960109903\n",
      "valid accuracy 98.0\n",
      "test accuracy 98.42\n",
      "EPOCH 68:\n",
      "  batch 100 loss: 0.0055146917038761244\n",
      "  batch 200 loss: 0.0038238530817227435\n",
      "  batch 300 loss: 0.004752821168600058\n",
      "LOSS train 0.004752821168600058 valid 0.2411840973290964\n",
      "valid accuracy 97.99\n",
      "test accuracy 98.33\n",
      "EPOCH 69:\n",
      "  batch 100 loss: 0.004055989994869277\n",
      "  batch 200 loss: 0.007732841689306938\n",
      "  batch 300 loss: 0.008556527656637093\n",
      "LOSS train 0.008556527656637093 valid 0.22874469265309616\n",
      "valid accuracy 97.97\n",
      "test accuracy 98.02\n",
      "EPOCH 70:\n",
      "  batch 100 loss: 0.010792314926947126\n",
      "  batch 200 loss: 0.004771567790921516\n",
      "  batch 300 loss: 0.0019679881315546755\n",
      "LOSS train 0.0019679881315546755 valid 0.2326958732266138\n",
      "valid accuracy 98.01\n",
      "test accuracy 98.31\n",
      "EPOCH 71:\n",
      "  batch 100 loss: 0.005169164456876179\n",
      "  batch 200 loss: 0.0025984769142643536\n",
      "  batch 300 loss: 0.005021116891754609\n",
      "LOSS train 0.005021116891754609 valid 0.23559140185808525\n",
      "valid accuracy 97.94\n",
      "test accuracy 98.36\n",
      "EPOCH 72:\n",
      "  batch 100 loss: 0.006140335165929806\n",
      "  batch 200 loss: 0.002203290051594227\n",
      "  batch 300 loss: 0.009587937477499935\n",
      "LOSS train 0.009587937477499935 valid 0.209981384054693\n",
      "valid accuracy 97.99\n",
      "test accuracy 98.24\n",
      "EPOCH 73:\n",
      "  batch 100 loss: 0.0013572946590954516\n",
      "  batch 200 loss: 0.0015842901906055684\n",
      "  batch 300 loss: 0.00017209069053212957\n",
      "LOSS train 0.00017209069053212957 valid 0.2147326134598856\n",
      "valid accuracy 98.13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 98.24\n",
      "EPOCH 74:\n",
      "  batch 100 loss: 0.00029815037194845127\n",
      "  batch 200 loss: 7.758395877204005e-05\n",
      "  batch 300 loss: 8.352874137843358e-05\n",
      "LOSS train 8.352874137843358e-05 valid 0.22633730610721592\n",
      "valid accuracy 98.23\n",
      "test accuracy 98.38\n",
      "EPOCH 75:\n",
      "  batch 100 loss: 0.001234389225626616\n",
      "  batch 200 loss: 9.667603475291831e-05\n",
      "  batch 300 loss: 0.0004733748151278572\n",
      "LOSS train 0.0004733748151278572 valid 0.20579688518196695\n",
      "valid accuracy 98.31\n",
      "test accuracy 98.43\n",
      "EPOCH 76:\n",
      "  batch 100 loss: 1.382438121423496e-05\n",
      "  batch 200 loss: 2.3555928673379966e-06\n",
      "  batch 300 loss: 0.0008136034354040439\n",
      "LOSS train 0.0008136034354040439 valid 0.253154102331941\n",
      "valid accuracy 97.98\n",
      "test accuracy 98.24\n",
      "EPOCH 77:\n",
      "  batch 100 loss: 0.002306683894372612\n",
      "  batch 200 loss: 0.009954855597749147\n",
      "  batch 300 loss: 0.014438124876887741\n",
      "LOSS train 0.014438124876887741 valid 0.26991722422071346\n",
      "valid accuracy 97.62\n",
      "test accuracy 97.75\n",
      "EPOCH 78:\n",
      "  batch 100 loss: 0.017008486639658107\n",
      "  batch 200 loss: 0.015619296618389953\n",
      "  batch 300 loss: 0.004779649401109447\n",
      "LOSS train 0.004779649401109447 valid 0.23392769415564152\n",
      "valid accuracy 97.97\n",
      "test accuracy 98.18\n",
      "EPOCH 79:\n",
      "  batch 100 loss: 0.0023456224738548936\n",
      "  batch 200 loss: 0.005206969494167657\n",
      "  batch 300 loss: 0.005235398425833902\n",
      "LOSS train 0.005235398425833902 valid 0.23206080398514028\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.11\n",
      "EPOCH 80:\n",
      "  batch 100 loss: 0.003525784567241525\n",
      "  batch 200 loss: 0.0012861230388524264\n",
      "  batch 300 loss: 0.006686087908941718\n",
      "LOSS train 0.006686087908941718 valid 0.19942596644149133\n",
      "valid accuracy 98.38\n",
      "test accuracy 98.25\n",
      "EPOCH 81:\n",
      "  batch 100 loss: 0.0010966404022636222\n",
      "  batch 200 loss: 0.0021712134702951413\n",
      "  batch 300 loss: 0.001427408893244177\n",
      "LOSS train 0.001427408893244177 valid 0.19032510636862418\n",
      "valid accuracy 98.27\n",
      "test accuracy 98.26\n",
      "EPOCH 82:\n",
      "  batch 100 loss: 0.0004970511444611375\n",
      "  batch 200 loss: 0.009853390114368145\n",
      "  batch 300 loss: 0.006932198313570939\n",
      "LOSS train 0.006932198313570939 valid 0.2475026622931353\n",
      "valid accuracy 97.93\n",
      "test accuracy 98.13\n",
      "EPOCH 83:\n",
      "  batch 100 loss: 0.007678213704505552\n",
      "  batch 200 loss: 0.0004496910897174894\n",
      "  batch 300 loss: 0.006064740355189828\n",
      "LOSS train 0.006064740355189828 valid 0.21730343901005272\n",
      "valid accuracy 98.01\n",
      "test accuracy 98.18\n",
      "EPOCH 84:\n",
      "  batch 100 loss: 0.001013882976688797\n",
      "  batch 200 loss: 0.00027572948984277035\n",
      "  batch 300 loss: 0.00010498053081222481\n",
      "LOSS train 0.00010498053081222481 valid 0.22593278708420625\n",
      "valid accuracy 98.15\n",
      "test accuracy 98.24\n",
      "EPOCH 85:\n",
      "  batch 100 loss: 0.006428570059003869\n",
      "  batch 200 loss: 0.0031678479650804827\n",
      "  batch 300 loss: 0.0051185707140025995\n",
      "LOSS train 0.0051185707140025995 valid 0.2519281335919027\n",
      "valid accuracy 97.84\n",
      "test accuracy 98.06\n",
      "EPOCH 86:\n",
      "  batch 100 loss: 0.005809802648633831\n",
      "  batch 200 loss: 0.0022811929813229413\n",
      "  batch 300 loss: 0.009952107709307071\n",
      "LOSS train 0.009952107709307071 valid 0.22642650047287768\n",
      "valid accuracy 97.94\n",
      "test accuracy 98.04\n",
      "EPOCH 87:\n",
      "  batch 100 loss: 0.003993450455806257\n",
      "  batch 200 loss: 0.012553987318004825\n",
      "  batch 300 loss: 0.007461549239513357\n",
      "LOSS train 0.007461549239513357 valid 0.20866897203396098\n",
      "valid accuracy 98.07\n",
      "test accuracy 98.26\n",
      "EPOCH 88:\n",
      "  batch 100 loss: 0.0006990575576517555\n",
      "  batch 200 loss: 0.001839388020776222\n",
      "  batch 300 loss: 0.0037360423506402145\n",
      "LOSS train 0.0037360423506402145 valid 0.2304218399050241\n",
      "valid accuracy 98.27\n",
      "test accuracy 98.3\n",
      "EPOCH 89:\n",
      "  batch 100 loss: 0.0013953529660288022\n",
      "  batch 200 loss: 0.001530890937533472\n",
      "  batch 300 loss: 0.003683448566464218\n",
      "LOSS train 0.003683448566464218 valid 0.24941958354125585\n",
      "valid accuracy 98.03\n",
      "test accuracy 98.01\n",
      "EPOCH 90:\n",
      "  batch 100 loss: 0.008063716940779439\n",
      "  batch 200 loss: 0.00907448931955498\n",
      "  batch 300 loss: 0.00435437738059788\n",
      "LOSS train 0.00435437738059788 valid 0.22847420281553332\n",
      "valid accuracy 98.13\n",
      "test accuracy 98.25\n",
      "EPOCH 91:\n",
      "  batch 100 loss: 0.004992930432015439\n",
      "  batch 200 loss: 0.00564929350552779\n",
      "  batch 300 loss: 0.006134156159044223\n",
      "LOSS train 0.006134156159044223 valid 0.3158590569161293\n",
      "valid accuracy 97.62\n",
      "test accuracy 97.75\n",
      "EPOCH 92:\n",
      "  batch 100 loss: 0.003643510387574551\n",
      "  batch 200 loss: 0.00036073598043923126\n",
      "  batch 300 loss: 0.00010269612515994786\n",
      "LOSS train 0.00010269612515994786 valid 0.2296055345513205\n",
      "valid accuracy 98.01\n",
      "test accuracy 98.13\n",
      "EPOCH 93:\n",
      "  batch 100 loss: 0.005780100525165386\n",
      "  batch 200 loss: 0.0049137080880496346\n",
      "  batch 300 loss: 0.010668392081981897\n",
      "LOSS train 0.010668392081981897 valid 0.25640643199726054\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.22\n",
      "EPOCH 94:\n",
      "  batch 100 loss: 0.00218047967959754\n",
      "  batch 200 loss: 0.00380612187538508\n",
      "  batch 300 loss: 0.0038679125222940824\n",
      "LOSS train 0.0038679125222940824 valid 0.2287509357101277\n",
      "valid accuracy 98.16\n",
      "test accuracy 98.3\n",
      "EPOCH 95:\n",
      "  batch 100 loss: 0.0006335705492349186\n",
      "  batch 200 loss: 0.0005510222306609113\n",
      "  batch 300 loss: 0.00026740868706556954\n",
      "LOSS train 0.00026740868706556954 valid 0.21110871317680707\n",
      "valid accuracy 98.35\n",
      "test accuracy 98.34\n",
      "EPOCH 96:\n",
      "  batch 100 loss: 7.985204472498175e-06\n",
      "  batch 200 loss: 2.915061053460899e-06\n",
      "  batch 300 loss: 0.0005467045556974109\n",
      "LOSS train 0.0005467045556974109 valid 0.24962119390524456\n",
      "valid accuracy 98.21\n",
      "test accuracy 98.28\n",
      "EPOCH 97:\n",
      "  batch 100 loss: 4.350614176516776e-06\n",
      "  batch 200 loss: 0.0006062240152451842\n",
      "  batch 300 loss: 4.682187426410245e-06\n",
      "LOSS train 4.682187426410245e-06 valid 0.2330008947669279\n",
      "valid accuracy 98.31\n",
      "test accuracy 98.37\n",
      "EPOCH 98:\n",
      "  batch 100 loss: 7.195451552469145e-07\n",
      "  batch 200 loss: 3.265852230179278e-06\n",
      "  batch 300 loss: 9.994181403052693e-06\n",
      "LOSS train 9.994181403052693e-06 valid 0.23034923974444588\n",
      "valid accuracy 98.31\n",
      "test accuracy 98.36\n",
      "EPOCH 99:\n",
      "  batch 100 loss: 5.968051538238206e-07\n",
      "  batch 200 loss: 4.071287538797985e-06\n",
      "  batch 300 loss: 3.800122748970347e-07\n",
      "LOSS train 3.800122748970347e-07 valid 0.23297798677187725\n",
      "valid accuracy 98.3\n",
      "test accuracy 98.4\n",
      "EPOCH 100:\n",
      "  batch 100 loss: 2.840716762003703e-07\n",
      "  batch 200 loss: 3.727381734305868e-07\n",
      "  batch 300 loss: 5.119858641183806e-07\n",
      "LOSS train 5.119858641183806e-07 valid 0.2330304332624881\n",
      "valid accuracy 98.31\n",
      "test accuracy 98.4\n",
      "EPOCH 101:\n",
      "  batch 100 loss: 4.2406332945998935e-07\n",
      "  batch 200 loss: 2.792314813493402e-07\n",
      "  batch 300 loss: 2.471392810668438e-07\n",
      "LOSS train 2.471392810668438e-07 valid 0.23311682435985978\n",
      "valid accuracy 98.31\n",
      "test accuracy 98.41\n",
      "EPOCH 102:\n",
      "  batch 100 loss: 3.1078770193482264e-07\n",
      "  batch 200 loss: 2.9913170223216933e-07\n",
      "  batch 300 loss: 1.8363163166973394e-07\n",
      "LOSS train 1.8363163166973394e-07 valid 0.23264876674778995\n",
      "valid accuracy 98.32\n",
      "test accuracy 98.41\n",
      "EPOCH 103:\n",
      "  batch 100 loss: 3.5613074002571744e-07\n",
      "  batch 200 loss: 2.59048454262778e-07\n",
      "  batch 300 loss: 1.9761767517723071e-07\n",
      "LOSS train 1.9761767517723071e-07 valid 0.23291258450035746\n",
      "valid accuracy 98.3\n",
      "test accuracy 98.41\n",
      "EPOCH 104:\n",
      "  batch 100 loss: 1.9646333612399758e-07\n",
      "  batch 200 loss: 2.1209908483726948e-07\n",
      "  batch 300 loss: 2.7230592416910284e-07\n",
      "LOSS train 2.7230592416910284e-07 valid 0.2327124916357431\n",
      "valid accuracy 98.34\n",
      "test accuracy 98.41\n",
      "EPOCH 105:\n",
      "  batch 100 loss: 1.482898942378741e-07\n",
      "  batch 200 loss: 1.9802298865806422e-07\n",
      "  batch 300 loss: 1.275772745445991e-07\n",
      "LOSS train 1.275772745445991e-07 valid 0.2327341668679114\n",
      "valid accuracy 98.29\n",
      "test accuracy 98.43\n",
      "EPOCH 106:\n",
      "  batch 100 loss: 1.4690128534139425e-07\n",
      "  batch 200 loss: 1.4748007971382427e-07\n",
      "  batch 300 loss: 1.773525508402818e-07\n",
      "LOSS train 1.773525508402818e-07 valid 0.23287466817385083\n",
      "valid accuracy 98.3\n",
      "test accuracy 98.43\n",
      "EPOCH 107:\n",
      "  batch 100 loss: 1.2490909206491762e-07\n",
      "  batch 200 loss: 1.1062685614376954e-07\n",
      "  batch 300 loss: 1.9215941708561335e-07\n",
      "LOSS train 1.9215941708561335e-07 valid 0.23250468967987212\n",
      "valid accuracy 98.3\n",
      "test accuracy 98.47\n",
      "EPOCH 108:\n",
      "  batch 100 loss: 8.718394440748733e-08\n",
      "  batch 200 loss: 1.1329842099921938e-07\n",
      "  batch 300 loss: 7.32702179592759e-08\n",
      "LOSS train 7.32702179592759e-08 valid 0.23442216733716392\n",
      "valid accuracy 98.28\n",
      "test accuracy 98.46\n",
      "EPOCH 109:\n",
      "  batch 100 loss: 5.010931393656381e-08\n",
      "  batch 200 loss: 3.794172067639323e-08\n",
      "  batch 300 loss: 3.2875342966809386e-08\n",
      "LOSS train 3.2875342966809386e-08 valid 0.23958554873266605\n",
      "valid accuracy 98.29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 98.46\n",
      "EPOCH 110:\n",
      "  batch 100 loss: 2.568574024336412e-08\n",
      "  batch 200 loss: 2.038186748348769e-08\n",
      "  batch 300 loss: 1.2288766959300812e-08\n",
      "LOSS train 1.2288766959300812e-08 valid 0.2447078640351046\n",
      "valid accuracy 98.3\n",
      "test accuracy 98.49\n",
      "EPOCH 111:\n",
      "  batch 100 loss: 1.0840566169179055e-08\n",
      "  batch 200 loss: 9.397014755219146e-09\n",
      "  batch 300 loss: 1.0444761926464708e-08\n",
      "LOSS train 1.0444761926464708e-08 valid 0.24839736625249517\n",
      "valid accuracy 98.28\n",
      "test accuracy 98.47\n",
      "EPOCH 112:\n",
      "  batch 100 loss: 8.619373999541047e-09\n",
      "  batch 200 loss: 5.429602727424765e-09\n",
      "  batch 300 loss: 6.807959466170743e-09\n",
      "LOSS train 6.807959466170743e-09 valid 0.2519665381062715\n",
      "valid accuracy 98.27\n",
      "test accuracy 98.48\n",
      "EPOCH 113:\n",
      "  batch 100 loss: 6.03962035605754e-09\n",
      "  batch 200 loss: 4.3539288596683436e-09\n",
      "  batch 300 loss: 4.209574522850979e-09\n",
      "LOSS train 4.209574522850979e-09 valid 0.25477551149707395\n",
      "valid accuracy 98.26\n",
      "test accuracy 98.48\n",
      "EPOCH 114:\n",
      "  batch 100 loss: 3.995371679887949e-09\n",
      "  batch 200 loss: 2.672893695265799e-09\n",
      "  batch 300 loss: 3.324819627115172e-09\n",
      "LOSS train 3.324819627115172e-09 valid 0.25715900575445794\n",
      "valid accuracy 98.28\n",
      "test accuracy 98.47\n",
      "EPOCH 115:\n",
      "  batch 100 loss: 3.2363443305816197e-09\n",
      "  batch 200 loss: 3.1664945782106102e-09\n",
      "  batch 300 loss: 2.8219060663192687e-09\n",
      "LOSS train 2.8219060663192687e-09 valid 0.25930045225363124\n",
      "valid accuracy 98.3\n",
      "test accuracy 98.48\n",
      "EPOCH 116:\n",
      "  batch 100 loss: 2.617015292527558e-09\n",
      "  batch 200 loss: 2.1280711867688317e-09\n",
      "  batch 300 loss: 2.5005999854110782e-09\n",
      "LOSS train 2.5005999854110782e-09 valid 0.2616700288290663\n",
      "valid accuracy 98.29\n",
      "test accuracy 98.5\n",
      "EPOCH 117:\n",
      "  batch 100 loss: 2.132727874026852e-09\n",
      "  batch 200 loss: 1.3271342882981152e-09\n",
      "  batch 300 loss: 2.011655990119543e-09\n",
      "LOSS train 2.011655990119543e-09 valid 0.2632543245269511\n",
      "valid accuracy 98.29\n",
      "test accuracy 98.49\n",
      "EPOCH 118:\n",
      "  batch 100 loss: 1.3457606429234658e-09\n",
      "  batch 200 loss: 1.201405792627952e-09\n",
      "  batch 300 loss: 1.792795417954185e-09\n",
      "LOSS train 1.792795417954185e-09 valid 0.265427555883358\n",
      "valid accuracy 98.29\n",
      "test accuracy 98.5\n",
      "EPOCH 119:\n",
      "  batch 100 loss: 1.2340021141032941e-09\n",
      "  batch 200 loss: 1.4761459529721853e-09\n",
      "  batch 300 loss: 1.0756773446973789e-09\n",
      "LOSS train 1.0756773446973789e-09 valid 0.2669863038217566\n",
      "valid accuracy 98.3\n",
      "test accuracy 98.48\n",
      "EPOCH 120:\n",
      "  batch 100 loss: 1.1082736300904727e-09\n",
      "  batch 200 loss: 1.2386587319723752e-09\n",
      "  batch 300 loss: 9.639186707155468e-10\n",
      "LOSS train 9.639186707155468e-10 valid 0.2687011988677534\n",
      "valid accuracy 98.3\n",
      "test accuracy 98.5\n",
      "EPOCH 121:\n",
      "  batch 100 loss: 6.472690586289965e-10\n",
      "  batch 200 loss: 6.519256684489604e-10\n",
      "  batch 300 loss: 1.2340021754431163e-09\n",
      "LOSS train 1.2340021754431163e-09 valid 0.26997664980016267\n",
      "valid accuracy 98.31\n",
      "test accuracy 98.5\n",
      "EPOCH 122:\n",
      "  batch 100 loss: 8.754430813606717e-10\n",
      "  batch 200 loss: 7.124616585585386e-10\n",
      "  batch 300 loss: 9.406356185626131e-10\n",
      "LOSS train 9.406356185626131e-10 valid 0.2714984328709427\n",
      "valid accuracy 98.31\n",
      "test accuracy 98.49\n",
      "EPOCH 123:\n",
      "  batch 100 loss: 5.960463658749582e-10\n",
      "  batch 200 loss: 6.239860333989711e-10\n",
      "  batch 300 loss: 7.543711738611236e-10\n",
      "LOSS train 7.543711738611236e-10 valid 0.2725381480040496\n",
      "valid accuracy 98.3\n",
      "test accuracy 98.51\n",
      "EPOCH 124:\n",
      "  batch 100 loss: 6.472691102543671e-10\n",
      "  batch 200 loss: 5.820765203168321e-10\n",
      "  batch 300 loss: 4.610046097641707e-10\n",
      "LOSS train 4.610046097641707e-10 valid 0.27375371749862315\n",
      "valid accuracy 98.3\n",
      "test accuracy 98.51\n",
      "EPOCH 125:\n",
      "  batch 100 loss: 3.725289848821589e-10\n",
      "  batch 200 loss: 5.029141264545345e-10\n",
      "  batch 300 loss: 6.845220096307613e-10\n",
      "LOSS train 6.845220096307613e-10 valid 0.27491113373137904\n",
      "valid accuracy 98.31\n",
      "test accuracy 98.5\n",
      "EPOCH 126:\n",
      "  batch 100 loss: 4.749744669796385e-10\n",
      "  batch 200 loss: 5.122273560864699e-10\n",
      "  batch 300 loss: 4.5169138956913104e-10\n",
      "LOSS train 4.5169138956913104e-10 valid 0.2756598213997441\n",
      "valid accuracy 98.3\n",
      "test accuracy 98.52\n",
      "EPOCH 127:\n",
      "  batch 100 loss: 4.423781790885428e-10\n",
      "  batch 200 loss: 3.2130625743365115e-10\n",
      "  batch 300 loss: 4.237517159388915e-10\n",
      "LOSS train 4.237517159388915e-10 valid 0.2769223327837346\n",
      "valid accuracy 98.32\n",
      "test accuracy 98.51\n",
      "EPOCH 128:\n",
      "  batch 100 loss: 4.423781779783198e-10\n",
      "  batch 200 loss: 3.8184221423653855e-10\n",
      "  batch 300 loss: 3.678723878297596e-10\n",
      "LOSS train 3.678723878297596e-10 valid 0.27721341263533306\n",
      "valid accuracy 98.32\n",
      "test accuracy 98.5\n",
      "EPOCH 129:\n",
      "  batch 100 loss: 2.840533602777029e-10\n",
      "  batch 200 loss: 3.3527609660799484e-10\n",
      "  batch 300 loss: 2.421438510813445e-10\n",
      "LOSS train 2.421438510813445e-10 valid 0.2777980690672497\n",
      "valid accuracy 98.31\n",
      "test accuracy 98.52\n",
      "EPOCH 130:\n",
      "  batch 100 loss: 3.4924593717011733e-10\n",
      "  batch 200 loss: 2.654269121160624e-10\n",
      "  batch 300 loss: 2.6077029785520625e-10\n",
      "LOSS train 2.6077029785520625e-10 valid 0.2788791327299619\n",
      "valid accuracy 98.32\n",
      "test accuracy 98.51\n",
      "EPOCH 131:\n",
      "  batch 100 loss: 2.2817401218455657e-10\n",
      "  batch 200 loss: 2.747401378622172e-10\n",
      "  batch 300 loss: 2.747401359193269e-10\n",
      "LOSS train 2.747401359193269e-10 valid 0.2796077118894363\n",
      "valid accuracy 98.32\n",
      "test accuracy 98.53\n",
      "EPOCH 132:\n",
      "  batch 100 loss: 2.6542691322628544e-10\n",
      "  batch 200 loss: 3.1199303807127876e-10\n",
      "  batch 300 loss: 1.1175870090474049e-10\n",
      "LOSS train 1.1175870090474049e-10 valid 0.28024505640630765\n",
      "valid accuracy 98.32\n",
      "test accuracy 98.52\n",
      "EPOCH 133:\n",
      "  batch 100 loss: 2.421438535793463e-10\n",
      "  batch 200 loss: 2.561136852596846e-10\n",
      "  batch 300 loss: 1.8160789094778806e-10\n",
      "LOSS train 1.8160789094778806e-10 valid 0.2808969175877189\n",
      "valid accuracy 98.33\n",
      "test accuracy 98.52\n",
      "EPOCH 134:\n",
      "  batch 100 loss: 1.862645016004194e-10\n",
      "  batch 200 loss: 1.4435499046117074e-10\n",
      "  batch 300 loss: 1.9092111558371983e-10\n",
      "LOSS train 1.9092111558371983e-10 valid 0.28133169975385397\n",
      "valid accuracy 98.31\n",
      "test accuracy 98.53\n",
      "EPOCH 135:\n",
      "  batch 100 loss: 2.4680046367686616e-10\n",
      "  batch 200 loss: 1.1175870201496351e-10\n",
      "  batch 300 loss: 1.9092111613883133e-10\n",
      "LOSS train 1.9092111613883133e-10 valid 0.2824189032910713\n",
      "valid accuracy 98.32\n",
      "test accuracy 98.52\n",
      "EPOCH 136:\n",
      "  batch 100 loss: 1.4435498990605921e-10\n",
      "  batch 200 loss: 1.0710208969699764e-10\n",
      "  batch 300 loss: 1.1641531488804091e-10\n",
      "LOSS train 1.1641531488804091e-10 valid 0.2827239872151444\n",
      "valid accuracy 98.31\n",
      "test accuracy 98.53\n",
      "EPOCH 137:\n",
      "  batch 100 loss: 1.1641531405537365e-10\n",
      "  batch 200 loss: 1.1175870201496351e-10\n",
      "  batch 300 loss: 8.381902671938946e-11\n",
      "LOSS train 8.381902671938946e-11 valid 0.2839915150751434\n",
      "valid accuracy 98.33\n",
      "test accuracy 98.52\n",
      "EPOCH 138:\n",
      "  batch 100 loss: 1.0710208969699764e-10\n",
      "  batch 200 loss: 9.313225191043273e-12\n",
      "  batch 300 loss: 1.396983778656491e-10\n",
      "LOSS train 1.396983778656491e-10 valid 0.28466480452569726\n",
      "valid accuracy 98.3\n",
      "test accuracy 98.52\n",
      "EPOCH 139:\n",
      "  batch 100 loss: 9.778886450595436e-11\n",
      "  batch 200 loss: 5.587935114625964e-11\n",
      "  batch 300 loss: 6.053596374178128e-11\n",
      "LOSS train 6.053596374178128e-11 valid 0.2851014888305385\n",
      "valid accuracy 98.32\n",
      "test accuracy 98.53\n",
      "EPOCH 140:\n",
      "  batch 100 loss: 8.84756393149111e-11\n",
      "  batch 200 loss: 4.656612567766061e-11\n",
      "  batch 300 loss: 5.587935114625964e-11\n",
      "LOSS train 5.587935114625964e-11 valid 0.28615587268251375\n",
      "valid accuracy 98.32\n",
      "test accuracy 98.53\n",
      "EPOCH 141:\n",
      "  batch 100 loss: 6.519257633730291e-11\n",
      "  batch 200 loss: 4.190951335969473e-11\n",
      "  batch 300 loss: 5.1222738550738e-11\n",
      "LOSS train 5.1222738550738e-11 valid 0.2865459484891127\n",
      "valid accuracy 98.34\n",
      "test accuracy 98.5\n",
      "EPOCH 142:\n",
      "  batch 100 loss: 3.725290076417309e-11\n",
      "  batch 200 loss: 3.2596288168651456e-11\n",
      "  batch 300 loss: 3.725290076417309e-11\n",
      "LOSS train 3.725290076417309e-11 valid 0.28676862846467555\n",
      "valid accuracy 98.32\n",
      "test accuracy 98.52\n",
      "EPOCH 143:\n",
      "  batch 100 loss: 2.793967557312982e-11\n",
      "  batch 200 loss: 1.8626450382086545e-11\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.28814285597468803\n",
      "valid accuracy 98.33\n",
      "test accuracy 98.52\n",
      "EPOCH 144:\n",
      "  batch 100 loss: 1.396983778656491e-11\n",
      "  batch 200 loss: 9.313225191043273e-12\n",
      "  batch 300 loss: 1.8626450382086545e-11\n",
      "LOSS train 1.8626450382086545e-11 valid 0.28850082924150716\n",
      "valid accuracy 98.34\n",
      "test accuracy 98.5\n",
      "EPOCH 145:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 2.3283062977608182e-11\n",
      "LOSS train 2.3283062977608182e-11 valid 0.2889876324055858\n",
      "valid accuracy 98.34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 98.52\n",
      "EPOCH 146:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 9.313225191043273e-12\n",
      "  batch 300 loss: 9.313225191043273e-12\n",
      "LOSS train 9.313225191043273e-12 valid 0.28968146726505944\n",
      "valid accuracy 98.32\n",
      "test accuracy 98.51\n",
      "EPOCH 147:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 4.656612595521636e-12\n",
      "LOSS train 4.656612595521636e-12 valid 0.2905102309929369\n",
      "valid accuracy 98.31\n",
      "test accuracy 98.52\n",
      "EPOCH 148:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.29106914628776415\n",
      "valid accuracy 98.33\n",
      "test accuracy 98.5\n",
      "EPOCH 149:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 4.656612595521636e-12\n",
      "LOSS train 4.656612595521636e-12 valid 0.29205569732455616\n",
      "valid accuracy 98.34\n",
      "test accuracy 98.52\n",
      "EPOCH 150:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.29203372675018563\n",
      "valid accuracy 98.33\n",
      "test accuracy 98.52\n",
      "EPOCH 151:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.2927236276298848\n",
      "valid accuracy 98.33\n",
      "test accuracy 98.53\n",
      "EPOCH 152:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.2932294050586029\n",
      "valid accuracy 98.33\n",
      "test accuracy 98.51\n",
      "EPOCH 153:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.29384548494117824\n",
      "valid accuracy 98.34\n",
      "test accuracy 98.53\n",
      "EPOCH 154:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.29446651784909517\n",
      "valid accuracy 98.34\n",
      "test accuracy 98.54\n",
      "EPOCH 155:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.2950435812504604\n",
      "valid accuracy 98.34\n",
      "test accuracy 98.54\n",
      "EPOCH 156:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.2954494033675431\n",
      "valid accuracy 98.34\n",
      "test accuracy 98.54\n",
      "EPOCH 157:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.29608574583450376\n",
      "valid accuracy 98.33\n",
      "test accuracy 98.54\n",
      "EPOCH 158:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.2961668795776649\n",
      "valid accuracy 98.32\n",
      "test accuracy 98.54\n",
      "EPOCH 159:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.2969752676816561\n",
      "valid accuracy 98.34\n",
      "test accuracy 98.53\n",
      "EPOCH 160:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.29725074780343075\n",
      "valid accuracy 98.34\n",
      "test accuracy 98.54\n",
      "EPOCH 161:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.2975514236194407\n",
      "valid accuracy 98.33\n",
      "test accuracy 98.54\n",
      "EPOCH 162:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.2978168154026683\n",
      "valid accuracy 98.34\n",
      "test accuracy 98.54\n",
      "EPOCH 163:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.29824096906291486\n",
      "valid accuracy 98.34\n",
      "test accuracy 98.54\n",
      "EPOCH 164:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.2987439267833002\n",
      "valid accuracy 98.34\n",
      "test accuracy 98.53\n",
      "EPOCH 165:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.29879736261659656\n",
      "valid accuracy 98.34\n",
      "test accuracy 98.53\n",
      "EPOCH 166:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.29903888890228075\n",
      "valid accuracy 98.34\n",
      "test accuracy 98.53\n",
      "EPOCH 167:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.2989863109984864\n",
      "valid accuracy 98.34\n",
      "test accuracy 98.53\n",
      "EPOCH 168:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.29935737726119843\n",
      "valid accuracy 98.34\n",
      "test accuracy 98.54\n",
      "EPOCH 169:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.2999224812036088\n",
      "valid accuracy 98.34\n",
      "test accuracy 98.55\n",
      "EPOCH 170:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.30004421319376756\n",
      "valid accuracy 98.34\n",
      "test accuracy 98.56\n",
      "EPOCH 171:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.3002063763040211\n",
      "valid accuracy 98.33\n",
      "test accuracy 98.55\n",
      "EPOCH 172:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.30043803837079136\n",
      "valid accuracy 98.33\n",
      "test accuracy 98.54\n",
      "EPOCH 173:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.30053106403465196\n",
      "valid accuracy 98.34\n",
      "test accuracy 98.55\n",
      "EPOCH 174:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.300834193843454\n",
      "valid accuracy 98.34\n",
      "test accuracy 98.56\n",
      "EPOCH 175:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.3013004128942766\n",
      "valid accuracy 98.34\n",
      "test accuracy 98.55\n",
      "EPOCH 176:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.30072578736078115\n",
      "valid accuracy 98.34\n",
      "test accuracy 98.58\n",
      "EPOCH 177:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.3008691463088289\n",
      "valid accuracy 98.35\n",
      "test accuracy 98.56\n",
      "EPOCH 178:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.30162315488542635\n",
      "valid accuracy 98.35\n",
      "test accuracy 98.56\n",
      "EPOCH 179:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.30156032902001245\n",
      "valid accuracy 98.33\n",
      "test accuracy 98.55\n",
      "EPOCH 180:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.30171006241537834\n",
      "valid accuracy 98.34\n",
      "test accuracy 98.55\n",
      "EPOCH 181:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.3018272535224643\n",
      "valid accuracy 98.34\n",
      "test accuracy 98.55\n",
      "EPOCH 182:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.30236774718239506\n",
      "valid accuracy 98.33\n",
      "test accuracy 98.58\n",
      "EPOCH 183:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.3024624123373134\n",
      "valid accuracy 98.34\n",
      "test accuracy 98.57\n",
      "EPOCH 184:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.3026660118650309\n",
      "valid accuracy 98.35\n",
      "test accuracy 98.57\n",
      "EPOCH 185:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.3029556485220843\n",
      "valid accuracy 98.35\n",
      "test accuracy 98.57\n",
      "EPOCH 186:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.3029796772349048\n",
      "valid accuracy 98.35\n",
      "test accuracy 98.57\n",
      "EPOCH 187:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.3033088796565907\n",
      "valid accuracy 98.32\n",
      "test accuracy 98.57\n",
      "EPOCH 188:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.3036352205105619\n",
      "valid accuracy 98.34\n",
      "test accuracy 98.59\n",
      "EPOCH 189:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.30361869949960446\n",
      "valid accuracy 98.35\n",
      "test accuracy 98.58\n",
      "EPOCH 190:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.30435071138272046\n",
      "valid accuracy 98.34\n",
      "test accuracy 98.58\n",
      "EPOCH 191:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.3044454321292707\n",
      "valid accuracy 98.34\n",
      "test accuracy 98.58\n",
      "EPOCH 192:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.30481677159866544\n",
      "valid accuracy 98.32\n",
      "test accuracy 98.58\n",
      "EPOCH 193:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.3043472788984613\n",
      "valid accuracy 98.32\n",
      "test accuracy 98.57\n",
      "EPOCH 194:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.30510736063183225\n",
      "valid accuracy 98.35\n",
      "test accuracy 98.57\n",
      "EPOCH 195:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.30537588436957186\n",
      "valid accuracy 98.35\n",
      "test accuracy 98.58\n",
      "EPOCH 196:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.3056795620315286\n",
      "valid accuracy 98.36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 98.6\n",
      "EPOCH 197:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.3055745167710219\n",
      "valid accuracy 98.33\n",
      "test accuracy 98.58\n",
      "EPOCH 198:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.30608668176525017\n",
      "valid accuracy 98.34\n",
      "test accuracy 98.58\n",
      "EPOCH 199:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.30647197269545895\n",
      "valid accuracy 98.35\n",
      "test accuracy 98.57\n",
      "EPOCH 200:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.3067849354147078\n",
      "valid accuracy 98.33\n",
      "test accuracy 98.58\n",
      "EPOCH 201:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.3067270743498468\n",
      "valid accuracy 98.35\n",
      "test accuracy 98.58\n",
      "EPOCH 202:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.30761623359685414\n",
      "valid accuracy 98.31\n",
      "test accuracy 98.58\n",
      "EPOCH 203:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.30772855795344406\n",
      "valid accuracy 98.33\n",
      "test accuracy 98.58\n",
      "EPOCH 204:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.30770096844534434\n",
      "valid accuracy 98.35\n",
      "test accuracy 98.6\n",
      "EPOCH 205:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.3078209764350413\n",
      "valid accuracy 98.33\n",
      "test accuracy 98.58\n",
      "EPOCH 206:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.3086674705777701\n",
      "valid accuracy 98.34\n",
      "test accuracy 98.59\n",
      "EPOCH 207:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.308587302959109\n",
      "valid accuracy 98.34\n",
      "test accuracy 98.57\n",
      "EPOCH 208:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.30943563501957777\n",
      "valid accuracy 98.34\n",
      "test accuracy 98.57\n",
      "EPOCH 209:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.30933238122480106\n",
      "valid accuracy 98.34\n",
      "test accuracy 98.57\n",
      "EPOCH 210:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.30983692814710356\n",
      "valid accuracy 98.34\n",
      "test accuracy 98.59\n",
      "EPOCH 211:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.3095778599362355\n",
      "valid accuracy 98.33\n",
      "test accuracy 98.58\n",
      "EPOCH 212:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.3105796321595243\n",
      "valid accuracy 98.34\n",
      "test accuracy 98.59\n",
      "EPOCH 213:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.31069544327040344\n",
      "valid accuracy 98.35\n",
      "test accuracy 98.56\n",
      "EPOCH 214:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.31128850416653087\n",
      "valid accuracy 98.34\n",
      "test accuracy 98.59\n",
      "EPOCH 215:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.31141859011523776\n",
      "valid accuracy 98.33\n",
      "test accuracy 98.58\n",
      "EPOCH 216:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.31145766639348127\n",
      "valid accuracy 98.36\n",
      "test accuracy 98.59\n",
      "EPOCH 217:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.3115676370965636\n",
      "valid accuracy 98.34\n",
      "test accuracy 98.57\n",
      "EPOCH 218:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.31222397558400405\n",
      "valid accuracy 98.34\n",
      "test accuracy 98.59\n",
      "EPOCH 219:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.3126885534499769\n",
      "valid accuracy 98.34\n",
      "test accuracy 98.58\n",
      "EPOCH 220:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.3130390252406014\n",
      "valid accuracy 98.35\n",
      "test accuracy 98.59\n",
      "EPOCH 221:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.31326882645622145\n",
      "valid accuracy 98.34\n",
      "test accuracy 98.57\n",
      "EPOCH 222:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.3138209264253713\n",
      "valid accuracy 98.34\n",
      "test accuracy 98.58\n",
      "EPOCH 223:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.313611837484683\n",
      "valid accuracy 98.37\n",
      "test accuracy 98.58\n",
      "EPOCH 224:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.31425776425124924\n",
      "valid accuracy 98.36\n",
      "test accuracy 98.58\n",
      "EPOCH 225:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.31484024338726396\n",
      "valid accuracy 98.35\n",
      "test accuracy 98.58\n",
      "EPOCH 226:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.315075015407888\n",
      "valid accuracy 98.36\n",
      "test accuracy 98.59\n",
      "EPOCH 227:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.3155178046238544\n",
      "valid accuracy 98.36\n",
      "test accuracy 98.59\n",
      "EPOCH 228:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.3159201114990321\n",
      "valid accuracy 98.37\n",
      "test accuracy 98.59\n",
      "EPOCH 229:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.31574471067099763\n",
      "valid accuracy 98.35\n",
      "test accuracy 98.58\n",
      "EPOCH 230:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.31645265442869924\n",
      "valid accuracy 98.35\n",
      "test accuracy 98.56\n",
      "EPOCH 231:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.31730177122937\n",
      "valid accuracy 98.37\n",
      "test accuracy 98.59\n",
      "EPOCH 232:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.3172216277157443\n",
      "valid accuracy 98.37\n",
      "test accuracy 98.59\n",
      "EPOCH 233:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.31746007843863294\n",
      "valid accuracy 98.38\n",
      "test accuracy 98.57\n",
      "EPOCH 234:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.31832980973985747\n",
      "valid accuracy 98.36\n",
      "test accuracy 98.58\n",
      "EPOCH 235:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.3187288908963157\n",
      "valid accuracy 98.36\n",
      "test accuracy 98.57\n",
      "EPOCH 236:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.3192634898359653\n",
      "valid accuracy 98.35\n",
      "test accuracy 98.58\n",
      "EPOCH 237:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.31981722238070404\n",
      "valid accuracy 98.34\n",
      "test accuracy 98.58\n",
      "EPOCH 238:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.3198056847179216\n",
      "valid accuracy 98.37\n",
      "test accuracy 98.57\n",
      "EPOCH 239:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.3207105364471762\n",
      "valid accuracy 98.35\n",
      "test accuracy 98.56\n",
      "EPOCH 240:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.32148710610976683\n",
      "valid accuracy 98.33\n",
      "test accuracy 98.56\n",
      "EPOCH 241:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.32260854254591526\n",
      "valid accuracy 98.34\n",
      "test accuracy 98.56\n",
      "EPOCH 242:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.32256435624393415\n",
      "valid accuracy 98.34\n",
      "test accuracy 98.55\n",
      "EPOCH 243:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.32335931631154163\n",
      "valid accuracy 98.34\n",
      "test accuracy 98.57\n",
      "EPOCH 244:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.32458314010729306\n",
      "valid accuracy 98.33\n",
      "test accuracy 98.56\n",
      "EPOCH 245:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.32490872476645793\n",
      "valid accuracy 98.33\n",
      "test accuracy 98.56\n",
      "EPOCH 246:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.32632264952423307\n",
      "valid accuracy 98.33\n",
      "test accuracy 98.56\n",
      "EPOCH 247:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "LOSS train 0.0 valid 0.3268014445220287\n",
      "valid accuracy 98.34\n",
      "test accuracy 98.56\n",
      "EPOCH 248:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 9.313225191043273e-12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 9.313225191043273e-12 valid 0.32828320167727487\n",
      "valid accuracy 98.34\n",
      "test accuracy 98.56\n",
      "EPOCH 249:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 9.313225191043273e-12\n",
      "LOSS train 9.313225191043273e-12 valid 0.32976854404333805\n",
      "valid accuracy 98.34\n",
      "test accuracy 98.55\n",
      "EPOCH 250:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 9.313225191043273e-12\n",
      "  batch 300 loss: 1.8626449271863523e-11\n",
      "LOSS train 1.8626449271863523e-11 valid 0.32892949265523325\n",
      "valid accuracy 98.36\n",
      "test accuracy 98.56\n",
      "EPOCH 251:\n",
      "  batch 100 loss: 0.032489485891857475\n",
      "  batch 200 loss: 0.08384104379859196\n",
      "  batch 300 loss: 0.03286796346444021\n",
      "LOSS train 0.03286796346444021 valid 0.30199997363547754\n",
      "valid accuracy 97.94\n",
      "test accuracy 98.19\n",
      "EPOCH 252:\n",
      "  batch 100 loss: 0.013445432582072208\n",
      "  batch 200 loss: 0.005881734852050133\n",
      "  batch 300 loss: 0.002950784892280271\n",
      "LOSS train 0.002950784892280271 valid 0.2925812455684155\n",
      "valid accuracy 98.11\n",
      "test accuracy 98.2\n",
      "EPOCH 253:\n",
      "  batch 100 loss: 0.0034985468697249296\n",
      "  batch 200 loss: 0.002626034521651836\n",
      "  batch 300 loss: 0.003522949792201304\n",
      "LOSS train 0.003522949792201304 valid 0.2900300642678229\n",
      "valid accuracy 98.04\n",
      "test accuracy 98.4\n",
      "EPOCH 254:\n",
      "  batch 100 loss: 0.0008762391518601731\n",
      "  batch 200 loss: 0.004166500159698939\n",
      "  batch 300 loss: 0.0026384692806112205\n",
      "LOSS train 0.0026384692806112205 valid 0.3059177337421373\n",
      "valid accuracy 98.1\n",
      "test accuracy 98.41\n",
      "EPOCH 255:\n",
      "  batch 100 loss: 0.0018784408986581677\n",
      "  batch 200 loss: 0.0003340469551076336\n",
      "  batch 300 loss: 0.0018680132523484782\n",
      "LOSS train 0.0018680132523484782 valid 0.31211578227897663\n",
      "valid accuracy 98.12\n",
      "test accuracy 98.36\n",
      "EPOCH 256:\n",
      "  batch 100 loss: 2.69834855226847e-05\n",
      "  batch 200 loss: 1.897842804807037e-05\n",
      "  batch 300 loss: 2.34012250775259e-05\n",
      "LOSS train 2.34012250775259e-05 valid 0.30804251081713924\n",
      "valid accuracy 98.17\n",
      "test accuracy 98.48\n",
      "EPOCH 257:\n",
      "  batch 100 loss: 1.415870076043324e-06\n",
      "  batch 200 loss: 6.284283223501052e-07\n",
      "  batch 300 loss: 3.585120801177788e-07\n",
      "LOSS train 3.585120801177788e-07 valid 0.30857908413468693\n",
      "valid accuracy 98.18\n",
      "test accuracy 98.49\n",
      "EPOCH 258:\n",
      "  batch 100 loss: 2.9328371605075175e-07\n",
      "  batch 200 loss: 1.3317586551708782e-07\n",
      "  batch 300 loss: 2.1028487953433926e-07\n",
      "LOSS train 2.1028487953433926e-07 valid 0.3090350783966849\n",
      "valid accuracy 98.17\n",
      "test accuracy 98.51\n",
      "EPOCH 259:\n",
      "  batch 100 loss: 1.74021248569578e-07\n",
      "  batch 200 loss: 1.622654274635593e-07\n",
      "  batch 300 loss: 1.3309287680868254e-07\n",
      "LOSS train 1.3309287680868254e-07 valid 0.3090810093061907\n",
      "valid accuracy 98.16\n",
      "test accuracy 98.51\n",
      "EPOCH 260:\n",
      "  batch 100 loss: 1.1807059404012054e-07\n",
      "  batch 200 loss: 1.2387957211451584e-07\n",
      "  batch 300 loss: 9.42139639442674e-08\n",
      "LOSS train 9.42139639442674e-08 valid 0.3091702828335345\n",
      "valid accuracy 98.19\n",
      "test accuracy 98.5\n",
      "EPOCH 261:\n",
      "  batch 100 loss: 6.661087443599056e-08\n",
      "  batch 200 loss: 9.515989379760281e-08\n",
      "  batch 300 loss: 8.708904180088073e-08\n",
      "LOSS train 8.708904180088073e-08 valid 0.3098291978088047\n",
      "valid accuracy 98.17\n",
      "test accuracy 98.49\n",
      "EPOCH 262:\n",
      "  batch 100 loss: 7.001506777626342e-08\n",
      "  batch 200 loss: 6.600069453244206e-08\n",
      "  batch 300 loss: 4.452565055768076e-08\n",
      "LOSS train 4.452565055768076e-08 valid 0.3101331530613123\n",
      "valid accuracy 98.18\n",
      "test accuracy 98.5\n",
      "EPOCH 263:\n",
      "  batch 100 loss: 3.979018266014878e-08\n",
      "  batch 200 loss: 3.60229829968195e-08\n",
      "  batch 300 loss: 5.378715011467028e-08\n",
      "LOSS train 5.378715011467028e-08 valid 0.31154048919521715\n",
      "valid accuracy 98.19\n",
      "test accuracy 98.5\n",
      "EPOCH 264:\n",
      "  batch 100 loss: 3.8249035810566937e-08\n",
      "  batch 200 loss: 4.856735548702806e-08\n",
      "  batch 300 loss: 2.7683187424631937e-08\n",
      "LOSS train 2.7683187424631937e-08 valid 0.3128693691101495\n",
      "valid accuracy 98.2\n",
      "test accuracy 98.51\n",
      "EPOCH 265:\n",
      "  batch 100 loss: 2.8144233211735425e-08\n",
      "  batch 200 loss: 2.388352909582503e-08\n",
      "  batch 300 loss: 2.2155884275565364e-08\n",
      "LOSS train 2.2155884275565364e-08 valid 0.3141833775623351\n",
      "valid accuracy 98.2\n",
      "test accuracy 98.51\n",
      "EPOCH 266:\n",
      "  batch 100 loss: 1.9138512120930874e-08\n",
      "  batch 200 loss: 1.8975551923927637e-08\n",
      "  batch 300 loss: 2.4190809523039202e-08\n",
      "LOSS train 2.4190809523039202e-08 valid 0.3163117170984554\n",
      "valid accuracy 98.18\n",
      "test accuracy 98.52\n",
      "EPOCH 267:\n",
      "  batch 100 loss: 1.3722997976028939e-08\n",
      "  batch 200 loss: 1.6586698227682993e-08\n",
      "  batch 300 loss: 1.438888110716352e-08\n",
      "LOSS train 1.438888110716352e-08 valid 0.31867981048429855\n",
      "valid accuracy 98.19\n",
      "test accuracy 98.52\n",
      "EPOCH 268:\n",
      "  batch 100 loss: 1.1040789757998937e-08\n",
      "  batch 200 loss: 1.0365581235571852e-08\n",
      "  batch 300 loss: 1.352736464621529e-08\n",
      "LOSS train 1.352736464621529e-08 valid 0.32047839169255976\n",
      "valid accuracy 98.19\n",
      "test accuracy 98.52\n",
      "EPOCH 269:\n",
      "  batch 100 loss: 7.16185439675332e-09\n",
      "  batch 200 loss: 8.61471489477239e-09\n",
      "  batch 300 loss: 6.486650125647753e-09\n",
      "LOSS train 6.486650125647753e-09 valid 0.32235820953483313\n",
      "valid accuracy 98.19\n",
      "test accuracy 98.53\n",
      "EPOCH 270:\n",
      "  batch 100 loss: 9.70899894148225e-09\n",
      "  batch 200 loss: 5.038444004512588e-09\n",
      "  batch 300 loss: 6.7800164835207784e-09\n",
      "LOSS train 6.7800164835207784e-09 valid 0.3242632446709186\n",
      "valid accuracy 98.2\n",
      "test accuracy 98.53\n",
      "EPOCH 271:\n",
      "  batch 100 loss: 5.955786819555886e-09\n",
      "  batch 200 loss: 6.314359154435678e-09\n",
      "  batch 300 loss: 3.846358721526499e-09\n",
      "LOSS train 3.846358721526499e-09 valid 0.3257870030221226\n",
      "valid accuracy 98.2\n",
      "test accuracy 98.53\n",
      "EPOCH 272:\n",
      "  batch 100 loss: 3.8975814520281205e-09\n",
      "  batch 200 loss: 3.860327711380229e-09\n",
      "  batch 300 loss: 4.316668414938718e-09\n",
      "LOSS train 4.316668414938718e-09 valid 0.32717609039639084\n",
      "valid accuracy 98.2\n",
      "test accuracy 98.51\n",
      "EPOCH 273:\n",
      "  batch 100 loss: 4.1071278145121685e-09\n",
      "  batch 200 loss: 2.3702145901527416e-09\n",
      "  batch 300 loss: 3.7858189341766215e-09\n",
      "LOSS train 3.7858189341766215e-09 valid 0.32882200498175673\n",
      "valid accuracy 98.19\n",
      "test accuracy 98.51\n",
      "EPOCH 274:\n",
      "  batch 100 loss: 3.459857251808529e-09\n",
      "  batch 200 loss: 2.5192258065742658e-09\n",
      "  batch 300 loss: 3.0407663939580053e-09\n",
      "LOSS train 3.0407663939580053e-09 valid 0.3299652522511601\n",
      "valid accuracy 98.2\n",
      "test accuracy 98.51\n",
      "EPOCH 275:\n",
      "  batch 100 loss: 3.4039797300322583e-09\n",
      "  batch 200 loss: 1.8347042979316442e-09\n",
      "  batch 300 loss: 2.002342714135796e-09\n",
      "LOSS train 2.002342714135796e-09 valid 0.3315315184822074\n",
      "valid accuracy 98.2\n",
      "test accuracy 98.51\n",
      "EPOCH 276:\n",
      "  batch 100 loss: 1.6018740417456102e-09\n",
      "  batch 200 loss: 2.114101280981107e-09\n",
      "  batch 300 loss: 2.3469304305523407e-09\n",
      "LOSS train 2.3469304305523407e-09 valid 0.33326168777506815\n",
      "valid accuracy 98.2\n",
      "test accuracy 98.5\n",
      "EPOCH 277:\n",
      "  batch 100 loss: 1.6298140406711958e-09\n",
      "  batch 200 loss: 1.5646212561670758e-09\n",
      "  batch 300 loss: 1.4295794886765023e-09\n",
      "LOSS train 1.4295794886765023e-09 valid 0.3342384944058043\n",
      "valid accuracy 98.22\n",
      "test accuracy 98.5\n",
      "EPOCH 278:\n",
      "  batch 100 loss: 1.550651661796909e-09\n",
      "  batch 200 loss: 1.4016400967653553e-09\n",
      "  batch 300 loss: 1.3643871327184699e-09\n",
      "LOSS train 1.3643871327184699e-09 valid 0.33605663590162893\n",
      "valid accuracy 98.23\n",
      "test accuracy 98.5\n",
      "EPOCH 279:\n",
      "  batch 100 loss: 1.2572850210945674e-09\n",
      "  batch 200 loss: 1.3364474940602556e-09\n",
      "  batch 300 loss: 9.639182133036605e-10\n",
      "LOSS train 9.639182133036605e-10 valid 0.33664366604111284\n",
      "valid accuracy 98.24\n",
      "test accuracy 98.5\n",
      "EPOCH 280:\n",
      "  batch 100 loss: 9.173524909145137e-10\n",
      "  batch 200 loss: 1.0430810329364882e-09\n",
      "  batch 300 loss: 1.0104845302172372e-09\n",
      "LOSS train 1.0104845302172372e-09 valid 0.33864581820700873\n",
      "valid accuracy 98.23\n",
      "test accuracy 98.49\n",
      "EPOCH 281:\n",
      "  batch 100 loss: 9.965147260149187e-10\n",
      "  batch 200 loss: 1.1082735845713288e-09\n",
      "  batch 300 loss: 9.778884799138687e-10\n",
      "LOSS train 9.778884799138687e-10 valid 0.33941821169608594\n",
      "valid accuracy 98.22\n",
      "test accuracy 98.49\n",
      "EPOCH 282:\n",
      "  batch 100 loss: 9.313222112949937e-10\n",
      "  batch 200 loss: 8.149071120677753e-10\n",
      "  batch 300 loss: 7.729976031489727e-10\n",
      "LOSS train 7.729976031489727e-10 valid 0.34066415730943533\n",
      "valid accuracy 98.23\n",
      "test accuracy 98.49\n",
      "EPOCH 283:\n",
      "  batch 100 loss: 6.146727968281418e-10\n",
      "  batch 200 loss: 9.918583077084264e-10\n",
      "  batch 300 loss: 6.891784967710813e-10\n",
      "LOSS train 6.891784967710813e-10 valid 0.3417719954572168\n",
      "valid accuracy 98.23\n",
      "test accuracy 98.47\n",
      "EPOCH 284:\n",
      "  batch 100 loss: 7.823107789350914e-10\n",
      "  batch 200 loss: 5.820765286435048e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 300 loss: 4.982574858258815e-10\n",
      "LOSS train 4.982574858258815e-10 valid 0.3426502482731055\n",
      "valid accuracy 98.24\n",
      "test accuracy 98.47\n",
      "EPOCH 285:\n",
      "  batch 100 loss: 5.029141267320902e-10\n",
      "  batch 200 loss: 5.168839392610814e-10\n",
      "  batch 300 loss: 6.752087414185759e-10\n",
      "LOSS train 6.752087414185759e-10 valid 0.3445041964960629\n",
      "valid accuracy 98.22\n",
      "test accuracy 98.48\n",
      "EPOCH 286:\n",
      "  batch 100 loss: 5.401670044591356e-10\n",
      "  batch 200 loss: 4.936008896061495e-10\n",
      "  batch 300 loss: 4.1909511583337887e-10\n",
      "LOSS train 4.1909511583337887e-10 valid 0.3454397387623449\n",
      "valid accuracy 98.22\n",
      "test accuracy 98.47\n",
      "EPOCH 287:\n",
      "  batch 100 loss: 3.911554399826933e-10\n",
      "  batch 200 loss: 5.448236117810978e-10\n",
      "  batch 300 loss: 5.075707382173889e-10\n",
      "LOSS train 5.075707382173889e-10 valid 0.3459135535168302\n",
      "valid accuracy 98.24\n",
      "test accuracy 98.46\n",
      "EPOCH 288:\n",
      "  batch 100 loss: 3.352760977182179e-10\n",
      "  batch 200 loss: 3.6787237867041966e-10\n",
      "  batch 300 loss: 3.81842202301641e-10\n",
      "LOSS train 3.81842202301641e-10 valid 0.34727685543323705\n",
      "valid accuracy 98.23\n",
      "test accuracy 98.47\n",
      "EPOCH 289:\n",
      "  batch 100 loss: 2.887099728732245e-10\n",
      "  batch 200 loss: 3.539025361654069e-10\n",
      "  batch 300 loss: 4.051252674996952e-10\n",
      "LOSS train 4.051252674996952e-10 valid 0.34869266115900266\n",
      "valid accuracy 98.23\n",
      "test accuracy 98.46\n",
      "EPOCH 290:\n",
      "  batch 100 loss: 2.3283062505763397e-10\n",
      "  batch 200 loss: 4.3772155983168305e-10\n",
      "  batch 300 loss: 2.607702917489796e-10\n",
      "LOSS train 2.607702917489796e-10 valid 0.34930639342264097\n",
      "valid accuracy 98.24\n",
      "test accuracy 98.47\n",
      "EPOCH 291:\n",
      "  batch 100 loss: 2.933665826931886e-10\n",
      "  batch 200 loss: 3.6321577079334586e-10\n",
      "  batch 300 loss: 2.887099698201112e-10\n",
      "LOSS train 2.887099698201112e-10 valid 0.34979922868524443\n",
      "valid accuracy 98.23\n",
      "test accuracy 98.47\n",
      "EPOCH 292:\n",
      "  batch 100 loss: 2.095475581942452e-10\n",
      "  batch 200 loss: 2.7008352415647254e-10\n",
      "  batch 300 loss: 2.4214385135890026e-10\n",
      "LOSS train 2.4214385135890026e-10 valid 0.35092180208530366\n",
      "valid accuracy 98.23\n",
      "test accuracy 98.46\n",
      "EPOCH 293:\n",
      "  batch 100 loss: 2.328306236698552e-10\n",
      "  batch 200 loss: 2.1886078865884783e-10\n",
      "  batch 300 loss: 1.955777209627918e-10\n",
      "LOSS train 1.955777209627918e-10 valid 0.3521381917957686\n",
      "valid accuracy 98.21\n",
      "test accuracy 98.46\n",
      "EPOCH 294:\n",
      "  batch 100 loss: 2.0489095226006172e-10\n",
      "  batch 200 loss: 1.722946579851836e-10\n",
      "  batch 300 loss: 2.4214385024867726e-10\n",
      "LOSS train 2.4214385024867726e-10 valid 0.3525879415380142\n",
      "valid accuracy 98.23\n",
      "test accuracy 98.47\n",
      "EPOCH 295:\n",
      "  batch 100 loss: 1.7229466464652177e-10\n",
      "  batch 200 loss: 1.1641531488804091e-10\n",
      "  batch 300 loss: 2.1886078782618058e-10\n",
      "LOSS train 2.1886078782618058e-10 valid 0.35334907539343496\n",
      "valid accuracy 98.23\n",
      "test accuracy 98.47\n",
      "EPOCH 296:\n",
      "  batch 100 loss: 1.9557772679146267e-10\n",
      "  batch 200 loss: 1.443549882407247e-10\n",
      "  batch 300 loss: 1.2107192637333953e-10\n",
      "LOSS train 1.2107192637333953e-10 valid 0.3541897193397606\n",
      "valid accuracy 98.23\n",
      "test accuracy 98.48\n",
      "EPOCH 297:\n",
      "  batch 100 loss: 1.6298143779014395e-10\n",
      "  batch 200 loss: 9.778886339573134e-11\n",
      "  batch 300 loss: 1.2572853896886115e-10\n",
      "LOSS train 1.2572853896886115e-10 valid 0.3554631199832423\n",
      "valid accuracy 98.24\n",
      "test accuracy 98.48\n",
      "EPOCH 298:\n",
      "  batch 100 loss: 9.778886450595436e-11\n",
      "  batch 200 loss: 1.257285386913054e-10\n",
      "  batch 300 loss: 9.313225135532122e-11\n",
      "LOSS train 9.313225135532122e-11 valid 0.35585429043853\n",
      "valid accuracy 98.24\n",
      "test accuracy 98.47\n",
      "EPOCH 299:\n",
      "  batch 100 loss: 9.778886339573134e-11\n",
      "  batch 200 loss: 9.313225191043273e-11\n",
      "  batch 300 loss: 5.587935086870388e-11\n",
      "LOSS train 5.587935086870388e-11 valid 0.3568526768323109\n",
      "valid accuracy 98.24\n",
      "test accuracy 98.46\n",
      "EPOCH 300:\n",
      "  batch 100 loss: 8.381902671938946e-11\n",
      "  batch 200 loss: 2.3283062977608182e-11\n",
      "  batch 300 loss: 4.6566125955216364e-11\n",
      "LOSS train 4.6566125955216364e-11 valid 0.35776293031430245\n",
      "valid accuracy 98.23\n",
      "test accuracy 98.47\n"
     ]
    }
   ],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/MNIST_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "val_loss = []\n",
    "val_accuracy = []\n",
    "\n",
    "test_loss = []\n",
    "test_accuracy = []\n",
    "\n",
    "EPOCHS = 300\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "num_vbatches = int(len(validation_set) / batch_size) + 1\n",
    "vdenom = 2 ** num_vbatches - 1\n",
    "\n",
    "num_tbatches = int(len(test_data) / batch_size) + 1\n",
    "tdenom = 2 ** num_tbatches - 1\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number, writer)\n",
    "\n",
    "    # We don't need gradients on to do reporting\n",
    "    model.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    correct = 0\n",
    "    for i, vdata in enumerate(validation_loader):\n",
    "        vinputs, vlabels = vdata\n",
    "        vinputs = vinputs.view(-1, 784)\n",
    "        voutputs = model(vinputs)\n",
    "        _, predicted = torch.max(voutputs, 1)\n",
    "        correct += torch.sum(vlabels == predicted)\n",
    "        vce = ce_loss(voutputs, vlabels)\n",
    "        vkl = kl_loss(model)\n",
    "        kl_weight = 2**(num_vbatches-i) / vdenom\n",
    "        vloss = vce + kl_weight * vkl\n",
    "        running_vloss += vloss.item()\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    val_loss.append(avg_vloss)\n",
    "    val_accuracy.append(100 * float(correct)/ len(validation_set))\n",
    "    \n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "    print('valid accuracy {}'.format(val_accuracy[-1]))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = './models/gaussian-mixture-prior/model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    running_tloss = 0.0\n",
    "    correct = 0\n",
    "\n",
    "    for i, tdata in enumerate(test_loader):\n",
    "        tinputs, tlabels = tdata\n",
    "        tinputs = tinputs.view(-1, 784)\n",
    "        toutputs = model(tinputs)\n",
    "        _, predicted = torch.max(toutputs, 1)\n",
    "        correct += torch.sum(tlabels == predicted)\n",
    "        tce = ce_loss(toutputs, tlabels)\n",
    "        tkl = kl_loss(model)\n",
    "        kl_weight = 2*(num_tbatches - i) / tdenom\n",
    "        tloss = tce + kl_weight * tkl\n",
    "        running_tloss += tloss.item()\n",
    "\n",
    "    avg_tloss = running_tloss / (i + 1)\n",
    "    test_loss.append(avg_tloss)\n",
    "    test_accuracy.append(100 * float(correct)/ len(test_data))\n",
    "    print('test accuracy {}'.format(test_accuracy[-1]))\n",
    "#     print('LOSS test {}'.format(avg_tloss))\n",
    "#     print(\"Number of correct predictions {}\".format(correct))\n",
    "\n",
    "#     print(\"Accuracy: {:.2f}\".format(100 * float(correct)/ len(test_data)))\n",
    "    \n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "data['test-loss'] = test_loss\n",
    "data['test-accuracy'] = test_accuracy\n",
    "data['validation-loss'] = val_loss\n",
    "data['validation-accuracy'] = val_accuracy\n",
    "\n",
    "with open('./models/gaussian-mixture-prior/test-loss', 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "miTxSzr8jk1R"
   },
   "source": [
    "# Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uaDECpUpjmuw",
    "outputId": "07594c7d-a551-462b-c599-f28734a79179"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS test 0.2516464568644603\n",
      "Number of correct predictions 9847\n",
      "Accuracy: 98.47\n"
     ]
    }
   ],
   "source": [
    "# model = nn.Sequential(\n",
    "#     BayesLinearMixture(prior_mu1=0, prior_sigma1=sigma1, prior_mu2=0, prior_sigma2=sigma2, pi=pi, in_features=input_dim, out_features=hidden_dim),\n",
    "#     nn.ReLU(),\n",
    "#     BayesLinearMixture(prior_mu1=0, prior_sigma1=sigma1, prior_mu2=0, prior_sigma2=sigma2, pi=pi, in_features=hidden_dim, out_features=hidden_dim),\n",
    "#     nn.ReLU(),\n",
    "#     BayesLinearMixture(prior_mu1=0, prior_sigma1=sigma1, prior_mu2=0, prior_sigma2=sigma2, pi=pi, in_features=hidden_dim, out_features=num_classes),\n",
    "# )\n",
    "\n",
    "# model.load_state_dict(torch.load('./model_20230406_204017_23'))\n",
    "\n",
    "model.train(False)\n",
    "\n",
    "running_tloss = 0.0\n",
    "correct = 0\n",
    "\n",
    "num_tbatches = int(len(test_data) / batch_size) + 1\n",
    "tdenom = 2 ** num_tbatches - 1\n",
    "\n",
    "for i, tdata in enumerate(test_loader):\n",
    "    tinputs, tlabels = tdata\n",
    "    tinputs = tinputs.view(-1, 784)\n",
    "    toutputs = model(tinputs)\n",
    "    _, predicted = torch.max(toutputs, 1)\n",
    "    correct += torch.sum(tlabels == predicted)\n",
    "    tce = ce_loss(toutputs, tlabels)\n",
    "    tkl = kl_loss(model)\n",
    "    kl_weight = 2*(num_tbatches - i) / tdenom\n",
    "    tloss = tce + kl_weight * tkl\n",
    "    running_tloss += tloss.item()\n",
    "\n",
    "avg_tloss = running_tloss / (i + 1)\n",
    "print('LOSS test {}'.format(avg_tloss))\n",
    "print(\"Number of correct predictions {}\".format(correct))\n",
    "\n",
    "print(\"Accuracy: {:.2f}\".format(100 * float(correct)/ len(test_data)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './models/gaussian-mixture-prior/model_{}_{}'.format(timestamp, epoch_number)\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
