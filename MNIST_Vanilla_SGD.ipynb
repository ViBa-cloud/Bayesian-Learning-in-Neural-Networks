{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4x1hZB-iJ47m"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hoNzE_BiKPlS"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "JMuxrQaRLhJR"
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "input_dim = 784\n",
    "\n",
    "# Hyperparameters\n",
    "lr = 0.05\n",
    "hidden_dim = 1200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "3LmpHydwKSPW"
   },
   "outputs": [],
   "source": [
    "# iris = datasets.load_iris()\n",
    "training_data = datasets.MNIST(root='data', train=True, download=True, transform=ToTensor())\n",
    "training_set, validation_set = torch.utils.data.random_split(training_data, [50000, 10000])\n",
    "test_data = datasets.MNIST(root='data', train=False, download=True, transform=ToTensor())\n",
    "\n",
    "training_loader = DataLoader(training_set, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_set, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "1zMGuntBNrZ4"
   },
   "outputs": [],
   "source": [
    "num_batches = int(len(training_set) / batch_size) + 1\n",
    "denom = 2 ** num_batches  - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 675
    },
    "id": "W1t4gjGjZv5m",
    "outputId": "54ea280f-1f79-4a4b-93c4-5343e8c6cfe8"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x800 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the data\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img, label = training_data[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(label)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "# No need to normalize. The data values are already between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RMu3DnfuK02O",
    "outputId": "c64ffeb7-c175-45c7-bbca-87e98506e4cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 10000\n"
     ]
    }
   ],
   "source": [
    "# print(img.shape)\n",
    "print(len(training_set),len(validation_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "2jBQ085QK3Z9"
   },
   "outputs": [],
   "source": [
    "# x, y = torch.from_numpy(X).float(), torch.from_numpy(Y).long()\n",
    "# x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "h1zQZNEiK5Hn"
   },
   "outputs": [],
   "source": [
    "sigma1 = 10 ** log_sigma1\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(in_features=input_dim, out_features=hidden_dim, bias=False),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=hidden_dim, out_features=hidden_dim, bias=False),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=hidden_dim, out_features=num_classes, bias=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "TKuZ_u5aK7O7"
   },
   "outputs": [],
   "source": [
    "ce_loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "hTdPC-GWedSM"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(training_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "\n",
    "        inputs = inputs.view(-1, 784)\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        ################## WE CAN AVERAGE ACROSS MULTIPLE W SAMPLES HERE BEFORE DOING loss.backward() #########################\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = ce_loss(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            last_loss = running_loss / 100 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(training_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vKLhziHlfCyw",
    "outputId": "933638f9-fb78-4aca-ccd4-d188f0623a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 100 loss: 1.9667944765090943\n",
      "  batch 200 loss: 0.8174952438473702\n",
      "  batch 300 loss: 0.5054942998290062\n",
      "LOSS train 0.5054942998290062 valid 0.4072777289378492\n",
      "valid accuracy 88.12\n",
      "test accuracy 89.14\n",
      "EPOCH 2:\n",
      "  batch 100 loss: 0.36982057183980943\n",
      "  batch 200 loss: 0.33762901797890665\n",
      "  batch 300 loss: 0.32938731238245966\n",
      "LOSS train 0.32938731238245966 valid 0.32427893503557276\n",
      "valid accuracy 90.6\n",
      "test accuracy 91.56\n",
      "EPOCH 3:\n",
      "  batch 100 loss: 0.28742374569177626\n",
      "  batch 200 loss: 0.28352433115243914\n",
      "  batch 300 loss: 0.2704421563446522\n",
      "LOSS train 0.2704421563446522 valid 0.26748400360723085\n",
      "valid accuracy 92.14\n",
      "test accuracy 92.93\n",
      "EPOCH 4:\n",
      "  batch 100 loss: 0.25631081461906435\n",
      "  batch 200 loss: 0.23514131888747214\n",
      "  batch 300 loss: 0.2216703938692808\n",
      "LOSS train 0.2216703938692808 valid 0.22927808752165565\n",
      "valid accuracy 93.54\n",
      "test accuracy 94.12\n",
      "EPOCH 5:\n",
      "  batch 100 loss: 0.2146958953887224\n",
      "  batch 200 loss: 0.20657061785459518\n",
      "  batch 300 loss: 0.19668228641152383\n",
      "LOSS train 0.19668228641152383 valid 0.20424755813577508\n",
      "valid accuracy 94.23\n",
      "test accuracy 94.38\n",
      "EPOCH 6:\n",
      "  batch 100 loss: 0.18895196199417114\n",
      "  batch 200 loss: 0.17238139405846595\n",
      "  batch 300 loss: 0.17100063048303127\n",
      "LOSS train 0.17100063048303127 valid 0.1793992787222319\n",
      "valid accuracy 94.74\n",
      "test accuracy 95.15\n",
      "EPOCH 7:\n",
      "  batch 100 loss: 0.16494061298668383\n",
      "  batch 200 loss: 0.16327219851315022\n",
      "  batch 300 loss: 0.1544300637766719\n",
      "LOSS train 0.1544300637766719 valid 0.16142764810142637\n",
      "valid accuracy 95.29\n",
      "test accuracy 95.48\n",
      "EPOCH 8:\n",
      "  batch 100 loss: 0.14362497881054878\n",
      "  batch 200 loss: 0.13590659283101558\n",
      "  batch 300 loss: 0.13597899679094552\n",
      "LOSS train 0.13597899679094552 valid 0.14881588933588583\n",
      "valid accuracy 95.57\n",
      "test accuracy 96.07\n",
      "EPOCH 9:\n",
      "  batch 100 loss: 0.12734814003109932\n",
      "  batch 200 loss: 0.12128179106861353\n",
      "  batch 300 loss: 0.12448290839791298\n",
      "LOSS train 0.12448290839791298 valid 0.14722158104369912\n",
      "valid accuracy 95.6\n",
      "test accuracy 95.84\n",
      "EPOCH 10:\n",
      "  batch 100 loss: 0.11414927337318659\n",
      "  batch 200 loss: 0.1109240696951747\n",
      "  batch 300 loss: 0.12019365353509784\n",
      "LOSS train 0.12019365353509784 valid 0.12866493809638144\n",
      "valid accuracy 96.06\n",
      "test accuracy 96.44\n",
      "EPOCH 11:\n",
      "  batch 100 loss: 0.10655961688607932\n",
      "  batch 200 loss: 0.11057760953903198\n",
      "  batch 300 loss: 0.10001631878316403\n",
      "LOSS train 0.10001631878316403 valid 0.12089439019372192\n",
      "valid accuracy 96.29\n",
      "test accuracy 96.65\n",
      "EPOCH 12:\n",
      "  batch 100 loss: 0.09412452939897775\n",
      "  batch 200 loss: 0.09406739875674247\n",
      "  batch 300 loss: 0.09584035722538829\n",
      "LOSS train 0.09584035722538829 valid 0.11947674233513543\n",
      "valid accuracy 96.47\n",
      "test accuracy 96.76\n",
      "EPOCH 13:\n",
      "  batch 100 loss: 0.08727224092930555\n",
      "  batch 200 loss: 0.08775882735848427\n",
      "  batch 300 loss: 0.0835914171114564\n",
      "LOSS train 0.0835914171114564 valid 0.1082789944366941\n",
      "valid accuracy 96.73\n",
      "test accuracy 97.09\n",
      "EPOCH 14:\n",
      "  batch 100 loss: 0.07460907028988004\n",
      "  batch 200 loss: 0.07780309036374092\n",
      "  batch 300 loss: 0.08392983039841057\n",
      "LOSS train 0.08392983039841057 valid 0.10235229412777515\n",
      "valid accuracy 96.93\n",
      "test accuracy 97.15\n",
      "EPOCH 15:\n",
      "  batch 100 loss: 0.06920927470549941\n",
      "  batch 200 loss: 0.07295223658904433\n",
      "  batch 300 loss: 0.07592320250347256\n",
      "LOSS train 0.07592320250347256 valid 0.09720849191557758\n",
      "valid accuracy 97.11\n",
      "test accuracy 97.27\n",
      "EPOCH 16:\n",
      "  batch 100 loss: 0.06419722508639097\n",
      "  batch 200 loss: 0.0646834722906351\n",
      "  batch 300 loss: 0.0682016240619123\n",
      "LOSS train 0.0682016240619123 valid 0.100147167079245\n",
      "valid accuracy 97.0\n",
      "test accuracy 97.17\n",
      "EPOCH 17:\n",
      "  batch 100 loss: 0.061423722002655265\n",
      "  batch 200 loss: 0.06239882121793926\n",
      "  batch 300 loss: 0.06522682763636112\n",
      "LOSS train 0.06522682763636112 valid 0.09247570392923264\n",
      "valid accuracy 97.22\n",
      "test accuracy 97.42\n",
      "EPOCH 18:\n",
      "  batch 100 loss: 0.05587241936475038\n",
      "  batch 200 loss: 0.056756106801331044\n",
      "  batch 300 loss: 0.05921833854168654\n",
      "LOSS train 0.05921833854168654 valid 0.08883395984391623\n",
      "valid accuracy 97.23\n",
      "test accuracy 97.52\n",
      "EPOCH 19:\n",
      "  batch 100 loss: 0.050754978209733966\n",
      "  batch 200 loss: 0.057380663016811016\n",
      "  batch 300 loss: 0.051114129666239023\n",
      "LOSS train 0.051114129666239023 valid 0.09083727226132833\n",
      "valid accuracy 97.27\n",
      "test accuracy 97.39\n",
      "EPOCH 20:\n",
      "  batch 100 loss: 0.049493666430935265\n",
      "  batch 200 loss: 0.04965920813381672\n",
      "  batch 300 loss: 0.04921072150580585\n",
      "LOSS train 0.04921072150580585 valid 0.08431557297140738\n",
      "valid accuracy 97.47\n",
      "test accuracy 97.52\n",
      "EPOCH 21:\n",
      "  batch 100 loss: 0.046979454047977924\n",
      "  batch 200 loss: 0.04487395392730832\n",
      "  batch 300 loss: 0.045417520757764576\n",
      "LOSS train 0.045417520757764576 valid 0.08170091846509825\n",
      "valid accuracy 97.42\n",
      "test accuracy 97.55\n",
      "EPOCH 22:\n",
      "  batch 100 loss: 0.04180788865312934\n",
      "  batch 200 loss: 0.04249709314666689\n",
      "  batch 300 loss: 0.044840869726613164\n",
      "LOSS train 0.044840869726613164 valid 0.08345373455859438\n",
      "valid accuracy 97.36\n",
      "test accuracy 97.61\n",
      "EPOCH 23:\n",
      "  batch 100 loss: 0.03969317932613194\n",
      "  batch 200 loss: 0.037214351715520025\n",
      "  batch 300 loss: 0.041097428081557155\n",
      "LOSS train 0.041097428081557155 valid 0.0785048190457157\n",
      "valid accuracy 97.57\n",
      "test accuracy 97.75\n",
      "EPOCH 24:\n",
      "  batch 100 loss: 0.03317491394467652\n",
      "  batch 200 loss: 0.03886444956064224\n",
      "  batch 300 loss: 0.03558338633738458\n",
      "LOSS train 0.03558338633738458 valid 0.07917909701413746\n",
      "valid accuracy 97.59\n",
      "test accuracy 97.72\n",
      "EPOCH 25:\n",
      "  batch 100 loss: 0.035889700306579474\n",
      "  batch 200 loss: 0.03383378205820918\n",
      "  batch 300 loss: 0.0329382889950648\n",
      "LOSS train 0.0329382889950648 valid 0.07835277544829665\n",
      "valid accuracy 97.58\n",
      "test accuracy 97.78\n",
      "EPOCH 26:\n",
      "  batch 100 loss: 0.030484823109582067\n",
      "  batch 200 loss: 0.03165055030956864\n",
      "  batch 300 loss: 0.031833221204578876\n",
      "LOSS train 0.031833221204578876 valid 0.07646936720496492\n",
      "valid accuracy 97.64\n",
      "test accuracy 97.88\n",
      "EPOCH 27:\n",
      "  batch 100 loss: 0.028899512188509106\n",
      "  batch 200 loss: 0.02968577413354069\n",
      "  batch 300 loss: 0.027826676350086926\n",
      "LOSS train 0.027826676350086926 valid 0.07440234872783665\n",
      "valid accuracy 97.65\n",
      "test accuracy 97.88\n",
      "EPOCH 28:\n",
      "  batch 100 loss: 0.027758973077870907\n",
      "  batch 200 loss: 0.02449764424934983\n",
      "  batch 300 loss: 0.03013044408522546\n",
      "LOSS train 0.03013044408522546 valid 0.07397919070400015\n",
      "valid accuracy 97.73\n",
      "test accuracy 97.85\n",
      "EPOCH 29:\n",
      "  batch 100 loss: 0.022673939019441605\n",
      "  batch 200 loss: 0.02953439306933433\n",
      "  batch 300 loss: 0.023746775286272167\n",
      "LOSS train 0.023746775286272167 valid 0.07329328315733355\n",
      "valid accuracy 97.74\n",
      "test accuracy 97.85\n",
      "EPOCH 30:\n",
      "  batch 100 loss: 0.020992006845772265\n",
      "  batch 200 loss: 0.023852691096253695\n",
      "  batch 300 loss: 0.022858509379439056\n",
      "LOSS train 0.022858509379439056 valid 0.0739868337500699\n",
      "valid accuracy 97.77\n",
      "test accuracy 97.91\n",
      "EPOCH 31:\n",
      "  batch 100 loss: 0.02366004840005189\n",
      "  batch 200 loss: 0.022227146765217184\n",
      "  batch 300 loss: 0.020914395973086357\n",
      "LOSS train 0.020914395973086357 valid 0.07226381802200517\n",
      "valid accuracy 97.68\n",
      "test accuracy 97.92\n",
      "EPOCH 32:\n",
      "  batch 100 loss: 0.018342938898131253\n",
      "  batch 200 loss: 0.02005150672979653\n",
      "  batch 300 loss: 0.02304219492012635\n",
      "LOSS train 0.02304219492012635 valid 0.07128202629900432\n",
      "valid accuracy 97.72\n",
      "test accuracy 97.96\n",
      "EPOCH 33:\n",
      "  batch 100 loss: 0.019228747240267696\n",
      "  batch 200 loss: 0.020331085796933622\n",
      "  batch 300 loss: 0.01828431648667902\n",
      "LOSS train 0.01828431648667902 valid 0.07087737687309331\n",
      "valid accuracy 97.8\n",
      "test accuracy 97.92\n",
      "EPOCH 34:\n",
      "  batch 100 loss: 0.01761829585535452\n",
      "  batch 200 loss: 0.01802240633871406\n",
      "  batch 300 loss: 0.017049756837077438\n",
      "LOSS train 0.017049756837077438 valid 0.07087517301021498\n",
      "valid accuracy 97.86\n",
      "test accuracy 97.96\n",
      "EPOCH 35:\n",
      "  batch 100 loss: 0.013642160396557301\n",
      "  batch 200 loss: 0.01781800669617951\n",
      "  batch 300 loss: 0.016859847600571812\n",
      "LOSS train 0.016859847600571812 valid 0.06978833490298895\n",
      "valid accuracy 97.82\n",
      "test accuracy 97.97\n",
      "EPOCH 36:\n",
      "  batch 100 loss: 0.015607520008925348\n",
      "  batch 200 loss: 0.015083610711153597\n",
      "  batch 300 loss: 0.015217994849663227\n",
      "LOSS train 0.015217994849663227 valid 0.07148216913536756\n",
      "valid accuracy 97.8\n",
      "test accuracy 97.99\n",
      "EPOCH 37:\n",
      "  batch 100 loss: 0.015042044089641422\n",
      "  batch 200 loss: 0.014478752685245127\n",
      "  batch 300 loss: 0.014252793721389025\n",
      "LOSS train 0.014252793721389025 valid 0.07035456969155164\n",
      "valid accuracy 97.82\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 97.93\n",
      "EPOCH 38:\n",
      "  batch 100 loss: 0.01333402045769617\n",
      "  batch 200 loss: 0.012850103268865496\n",
      "  batch 300 loss: 0.014435644382610918\n",
      "LOSS train 0.014435644382610918 valid 0.0697507777168781\n",
      "valid accuracy 97.86\n",
      "test accuracy 97.98\n",
      "EPOCH 39:\n",
      "  batch 100 loss: 0.011887112522963435\n",
      "  batch 200 loss: 0.012153042950667442\n",
      "  batch 300 loss: 0.013184712016955018\n",
      "LOSS train 0.013184712016955018 valid 0.07067783937282578\n",
      "valid accuracy 97.81\n",
      "test accuracy 97.98\n",
      "EPOCH 40:\n",
      "  batch 100 loss: 0.010117317198310048\n",
      "  batch 200 loss: 0.013924552302341908\n",
      "  batch 300 loss: 0.011877249388489872\n",
      "LOSS train 0.011877249388489872 valid 0.06961728311792205\n",
      "valid accuracy 97.85\n",
      "test accuracy 97.97\n",
      "EPOCH 41:\n",
      "  batch 100 loss: 0.010926876233424991\n",
      "  batch 200 loss: 0.010476822792552411\n",
      "  batch 300 loss: 0.01172569960122928\n",
      "LOSS train 0.01172569960122928 valid 0.07107330185561618\n",
      "valid accuracy 97.88\n",
      "test accuracy 98.02\n",
      "EPOCH 42:\n",
      "  batch 100 loss: 0.010729069134686142\n",
      "  batch 200 loss: 0.010017766596283763\n",
      "  batch 300 loss: 0.011116225253790617\n",
      "LOSS train 0.011116225253790617 valid 0.06997787814493044\n",
      "valid accuracy 97.89\n",
      "test accuracy 98.01\n",
      "EPOCH 43:\n",
      "  batch 100 loss: 0.010643400624394417\n",
      "  batch 200 loss: 0.010253072855994106\n",
      "  batch 300 loss: 0.00993174156639725\n",
      "LOSS train 0.00993174156639725 valid 0.07016859624440534\n",
      "valid accuracy 97.89\n",
      "test accuracy 98.06\n",
      "EPOCH 44:\n",
      "  batch 100 loss: 0.008818716967944056\n",
      "  batch 200 loss: 0.010295117949135601\n",
      "  batch 300 loss: 0.009890406623017043\n",
      "LOSS train 0.009890406623017043 valid 0.07128850947122407\n",
      "valid accuracy 97.88\n",
      "test accuracy 97.99\n",
      "EPOCH 45:\n",
      "  batch 100 loss: 0.008208187788259238\n",
      "  batch 200 loss: 0.009721576604060828\n",
      "  batch 300 loss: 0.008251958494074643\n",
      "LOSS train 0.008251958494074643 valid 0.06965785353338416\n",
      "valid accuracy 97.94\n",
      "test accuracy 98.03\n",
      "EPOCH 46:\n",
      "  batch 100 loss: 0.007597995457472279\n",
      "  batch 200 loss: 0.009552941186120734\n",
      "  batch 300 loss: 0.008256128339562565\n",
      "LOSS train 0.008256128339562565 valid 0.07100207786535538\n",
      "valid accuracy 97.87\n",
      "test accuracy 98.03\n",
      "EPOCH 47:\n",
      "  batch 100 loss: 0.007962188336532562\n",
      "  batch 200 loss: 0.009002934918971733\n",
      "  batch 300 loss: 0.007738602131139487\n",
      "LOSS train 0.007738602131139487 valid 0.07183570783773932\n",
      "valid accuracy 97.82\n",
      "test accuracy 98.0\n",
      "EPOCH 48:\n",
      "  batch 100 loss: 0.00812088834354654\n",
      "  batch 200 loss: 0.008301320923492313\n",
      "  batch 300 loss: 0.006918941149488092\n",
      "LOSS train 0.006918941149488092 valid 0.07133695729595574\n",
      "valid accuracy 97.84\n",
      "test accuracy 98.04\n",
      "EPOCH 49:\n",
      "  batch 100 loss: 0.007156147384084761\n",
      "  batch 200 loss: 0.006713086204836145\n",
      "  batch 300 loss: 0.006471749268239364\n",
      "LOSS train 0.006471749268239364 valid 0.0707166849906686\n",
      "valid accuracy 97.93\n",
      "test accuracy 98.05\n",
      "EPOCH 50:\n",
      "  batch 100 loss: 0.007582744645187631\n",
      "  batch 200 loss: 0.006246790466830135\n",
      "  batch 300 loss: 0.006483587393304333\n",
      "LOSS train 0.006483587393304333 valid 0.07051634314599671\n",
      "valid accuracy 97.91\n",
      "test accuracy 98.1\n",
      "EPOCH 51:\n",
      "  batch 100 loss: 0.0060917605645954605\n",
      "  batch 200 loss: 0.00656084582908079\n",
      "  batch 300 loss: 0.006762147935805842\n",
      "LOSS train 0.006762147935805842 valid 0.07078963415579329\n",
      "valid accuracy 97.89\n",
      "test accuracy 98.06\n",
      "EPOCH 52:\n",
      "  batch 100 loss: 0.005897539362777025\n",
      "  batch 200 loss: 0.005891076594125479\n",
      "  batch 300 loss: 0.0057846231514122335\n",
      "LOSS train 0.0057846231514122335 valid 0.07056852331078506\n",
      "valid accuracy 97.95\n",
      "test accuracy 98.06\n",
      "EPOCH 53:\n",
      "  batch 100 loss: 0.005629946313565597\n",
      "  batch 200 loss: 0.005679603527532891\n",
      "  batch 300 loss: 0.006359684971394017\n",
      "LOSS train 0.006359684971394017 valid 0.07100698254012232\n",
      "valid accuracy 97.95\n",
      "test accuracy 98.07\n",
      "EPOCH 54:\n",
      "  batch 100 loss: 0.004929369858000427\n",
      "  batch 200 loss: 0.005366208900231868\n",
      "  batch 300 loss: 0.006260870838304982\n",
      "LOSS train 0.006260870838304982 valid 0.07122881088075758\n",
      "valid accuracy 97.94\n",
      "test accuracy 98.1\n",
      "EPOCH 55:\n",
      "  batch 100 loss: 0.006013065396109596\n",
      "  batch 200 loss: 0.004909738200949505\n",
      "  batch 300 loss: 0.00504430626402609\n",
      "LOSS train 0.00504430626402609 valid 0.07090794980007259\n",
      "valid accuracy 97.99\n",
      "test accuracy 98.13\n",
      "EPOCH 56:\n",
      "  batch 100 loss: 0.004494703155942261\n",
      "  batch 200 loss: 0.005137539440765977\n",
      "  batch 300 loss: 0.005090919805224985\n",
      "LOSS train 0.005090919805224985 valid 0.0712517790637816\n",
      "valid accuracy 97.91\n",
      "test accuracy 98.13\n",
      "EPOCH 57:\n",
      "  batch 100 loss: 0.004560099780792371\n",
      "  batch 200 loss: 0.005208995963912457\n",
      "  batch 300 loss: 0.004509464007569477\n",
      "LOSS train 0.004509464007569477 valid 0.07400004979505946\n",
      "valid accuracy 97.86\n",
      "test accuracy 98.13\n",
      "EPOCH 58:\n",
      "  batch 100 loss: 0.004024804852670059\n",
      "  batch 200 loss: 0.004205748945241794\n",
      "  batch 300 loss: 0.005613989335251972\n",
      "LOSS train 0.005613989335251972 valid 0.07155219099449017\n",
      "valid accuracy 98.0\n",
      "test accuracy 98.09\n",
      "EPOCH 59:\n",
      "  batch 100 loss: 0.0038742215593811124\n",
      "  batch 200 loss: 0.004826901636552066\n",
      "  batch 300 loss: 0.005147629559505731\n",
      "LOSS train 0.005147629559505731 valid 0.07261081228527842\n",
      "valid accuracy 97.94\n",
      "test accuracy 98.1\n",
      "EPOCH 60:\n",
      "  batch 100 loss: 0.004026986144017428\n",
      "  batch 200 loss: 0.00406540155177936\n",
      "  batch 300 loss: 0.0048259811406023805\n",
      "LOSS train 0.0048259811406023805 valid 0.07288461120775606\n",
      "valid accuracy 97.92\n",
      "test accuracy 98.13\n",
      "EPOCH 61:\n",
      "  batch 100 loss: 0.003985415998613462\n",
      "  batch 200 loss: 0.004398412398295477\n",
      "  batch 300 loss: 0.003980706515721977\n",
      "LOSS train 0.003980706515721977 valid 0.0728469727313311\n",
      "valid accuracy 97.91\n",
      "test accuracy 98.07\n",
      "EPOCH 62:\n",
      "  batch 100 loss: 0.003956865288200788\n",
      "  batch 200 loss: 0.003751196978846565\n",
      "  batch 300 loss: 0.00441410987637937\n",
      "LOSS train 0.00441410987637937 valid 0.07220496788408749\n",
      "valid accuracy 97.94\n",
      "test accuracy 98.06\n",
      "EPOCH 63:\n",
      "  batch 100 loss: 0.003644400418270379\n",
      "  batch 200 loss: 0.0039311952888965605\n",
      "  batch 300 loss: 0.004436696593184024\n",
      "LOSS train 0.004436696593184024 valid 0.07234358172156388\n",
      "valid accuracy 97.93\n",
      "test accuracy 98.04\n",
      "EPOCH 64:\n",
      "  batch 100 loss: 0.004122086194693111\n",
      "  batch 200 loss: 0.003336778829107061\n",
      "  batch 300 loss: 0.003520625369856134\n",
      "LOSS train 0.003520625369856134 valid 0.07263443489335006\n",
      "valid accuracy 97.98\n",
      "test accuracy 98.08\n",
      "EPOCH 65:\n",
      "  batch 100 loss: 0.003537359342444688\n",
      "  batch 200 loss: 0.0034178479760885237\n",
      "  batch 300 loss: 0.00393060555565171\n",
      "LOSS train 0.00393060555565171 valid 0.07288484834134579\n",
      "valid accuracy 97.9\n",
      "test accuracy 98.07\n",
      "EPOCH 66:\n",
      "  batch 100 loss: 0.003311179293668829\n",
      "  batch 200 loss: 0.0035507875418988987\n",
      "  batch 300 loss: 0.0035151600709650665\n",
      "LOSS train 0.0035151600709650665 valid 0.07293131027394269\n",
      "valid accuracy 97.89\n",
      "test accuracy 98.15\n",
      "EPOCH 67:\n",
      "  batch 100 loss: 0.0032239852077327667\n",
      "  batch 200 loss: 0.0034259951539570466\n",
      "  batch 300 loss: 0.0035209678194951266\n",
      "LOSS train 0.0035209678194951266 valid 0.07299171834948319\n",
      "valid accuracy 98.0\n",
      "test accuracy 98.1\n",
      "EPOCH 68:\n",
      "  batch 100 loss: 0.0031311725033447146\n",
      "  batch 200 loss: 0.0032521179434843363\n",
      "  batch 300 loss: 0.0032484384399140255\n",
      "LOSS train 0.0032484384399140255 valid 0.07293327664367005\n",
      "valid accuracy 97.94\n",
      "test accuracy 98.11\n",
      "EPOCH 69:\n",
      "  batch 100 loss: 0.0032863740879110993\n",
      "  batch 200 loss: 0.0027461396233411507\n",
      "  batch 300 loss: 0.003376549686072394\n",
      "LOSS train 0.003376549686072394 valid 0.07366895791779779\n",
      "valid accuracy 97.96\n",
      "test accuracy 98.1\n",
      "EPOCH 70:\n",
      "  batch 100 loss: 0.0030454881640616803\n",
      "  batch 200 loss: 0.0029191597213502974\n",
      "  batch 300 loss: 0.0027441065333550798\n",
      "LOSS train 0.0027441065333550798 valid 0.07363793942369992\n",
      "valid accuracy 97.95\n",
      "test accuracy 98.12\n",
      "EPOCH 71:\n",
      "  batch 100 loss: 0.003069045122247189\n",
      "  batch 200 loss: 0.0028387116140220314\n",
      "  batch 300 loss: 0.0028444075718289242\n",
      "LOSS train 0.0028444075718289242 valid 0.0734765307706651\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.12\n",
      "EPOCH 72:\n",
      "  batch 100 loss: 0.0027548773773014546\n",
      "  batch 200 loss: 0.002753398054628633\n",
      "  batch 300 loss: 0.0029193655558628962\n",
      "LOSS train 0.0029193655558628962 valid 0.0742948525703123\n",
      "valid accuracy 97.95\n",
      "test accuracy 98.08\n",
      "EPOCH 73:\n",
      "  batch 100 loss: 0.0028311928757466377\n",
      "  batch 200 loss: 0.0025801478076027708\n",
      "  batch 300 loss: 0.002797051893430762\n",
      "LOSS train 0.002797051893430762 valid 0.07417013106537582\n",
      "valid accuracy 97.97\n",
      "test accuracy 98.17\n",
      "EPOCH 74:\n",
      "  batch 100 loss: 0.002578565349103883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 200 loss: 0.002639779785531573\n",
      "  batch 300 loss: 0.0028306024632183836\n",
      "LOSS train 0.0028306024632183836 valid 0.07393742551956373\n",
      "valid accuracy 97.97\n",
      "test accuracy 98.1\n",
      "EPOCH 75:\n",
      "  batch 100 loss: 0.00238666185119655\n",
      "  batch 200 loss: 0.0024417987745255234\n",
      "  batch 300 loss: 0.0025388921110425147\n",
      "LOSS train 0.0025388921110425147 valid 0.07410326005914543\n",
      "valid accuracy 97.94\n",
      "test accuracy 98.12\n",
      "EPOCH 76:\n",
      "  batch 100 loss: 0.0025429992348654194\n",
      "  batch 200 loss: 0.00246996510628378\n",
      "  batch 300 loss: 0.0024794576814747417\n",
      "LOSS train 0.0024794576814747417 valid 0.07432125678545312\n",
      "valid accuracy 98.0\n",
      "test accuracy 98.12\n",
      "EPOCH 77:\n",
      "  batch 100 loss: 0.0025598466023802756\n",
      "  batch 200 loss: 0.002355328630656004\n",
      "  batch 300 loss: 0.0024490471149329097\n",
      "LOSS train 0.0024490471149329097 valid 0.07499243041406138\n",
      "valid accuracy 97.93\n",
      "test accuracy 98.05\n",
      "EPOCH 78:\n",
      "  batch 100 loss: 0.0024069851654348897\n",
      "  batch 200 loss: 0.0022068356408271937\n",
      "  batch 300 loss: 0.0024118504661601037\n",
      "LOSS train 0.0024118504661601037 valid 0.07491030516338688\n",
      "valid accuracy 98.01\n",
      "test accuracy 98.09\n",
      "EPOCH 79:\n",
      "  batch 100 loss: 0.002361104709561914\n",
      "  batch 200 loss: 0.0021229405753547324\n",
      "  batch 300 loss: 0.002285028240876272\n",
      "LOSS train 0.002285028240876272 valid 0.07513521155460348\n",
      "valid accuracy 97.95\n",
      "test accuracy 98.1\n",
      "EPOCH 80:\n",
      "  batch 100 loss: 0.0023107084230287\n",
      "  batch 200 loss: 0.0023050572228385134\n",
      "  batch 300 loss: 0.002101139124133624\n",
      "LOSS train 0.002101139124133624 valid 0.07486430636335965\n",
      "valid accuracy 97.94\n",
      "test accuracy 98.08\n",
      "EPOCH 81:\n",
      "  batch 100 loss: 0.0021078403701540083\n",
      "  batch 200 loss: 0.0021370848384685815\n",
      "  batch 300 loss: 0.002258606527466327\n",
      "LOSS train 0.002258606527466327 valid 0.07498350824359097\n",
      "valid accuracy 97.99\n",
      "test accuracy 98.09\n",
      "EPOCH 82:\n",
      "  batch 100 loss: 0.001973916129791178\n",
      "  batch 200 loss: 0.0021972167256171815\n",
      "  batch 300 loss: 0.0019977216178085656\n",
      "LOSS train 0.0019977216178085656 valid 0.07518115978617268\n",
      "valid accuracy 97.96\n",
      "test accuracy 98.09\n",
      "EPOCH 83:\n",
      "  batch 100 loss: 0.002053383533784654\n",
      "  batch 200 loss: 0.0018827023374615238\n",
      "  batch 300 loss: 0.0022797121171606706\n",
      "LOSS train 0.0022797121171606706 valid 0.07575886151930199\n",
      "valid accuracy 97.95\n",
      "test accuracy 98.11\n",
      "EPOCH 84:\n",
      "  batch 100 loss: 0.002022470623778645\n",
      "  batch 200 loss: 0.0019098015202325769\n",
      "  batch 300 loss: 0.0019285413273610174\n",
      "LOSS train 0.0019285413273610174 valid 0.07554227051242621\n",
      "valid accuracy 98.04\n",
      "test accuracy 98.12\n",
      "EPOCH 85:\n",
      "  batch 100 loss: 0.002031613879080396\n",
      "  batch 200 loss: 0.0018123506481060759\n",
      "  batch 300 loss: 0.001863271795446053\n",
      "LOSS train 0.001863271795446053 valid 0.0755061762631411\n",
      "valid accuracy 97.99\n",
      "test accuracy 98.1\n",
      "EPOCH 86:\n",
      "  batch 100 loss: 0.0019513786633615381\n",
      "  batch 200 loss: 0.0017875727033242583\n",
      "  batch 300 loss: 0.001945186613011174\n",
      "LOSS train 0.001945186613011174 valid 0.07547887784275639\n",
      "valid accuracy 97.99\n",
      "test accuracy 98.09\n",
      "EPOCH 87:\n",
      "  batch 100 loss: 0.0019305929422262125\n",
      "  batch 200 loss: 0.0017622248388943261\n",
      "  batch 300 loss: 0.0019506985554471612\n",
      "LOSS train 0.0019506985554471612 valid 0.07623088754760692\n",
      "valid accuracy 97.92\n",
      "test accuracy 98.14\n",
      "EPOCH 88:\n",
      "  batch 100 loss: 0.0017770750832278282\n",
      "  batch 200 loss: 0.0018401358550181612\n",
      "  batch 300 loss: 0.0018194223602768034\n",
      "LOSS train 0.0018194223602768034 valid 0.0761948079342329\n",
      "valid accuracy 97.94\n",
      "test accuracy 98.12\n",
      "EPOCH 89:\n",
      "  batch 100 loss: 0.0017180202127201484\n",
      "  batch 200 loss: 0.0017795310670044274\n",
      "  batch 300 loss: 0.001760387879330665\n",
      "LOSS train 0.001760387879330665 valid 0.07616405810901447\n",
      "valid accuracy 97.94\n",
      "test accuracy 98.08\n",
      "EPOCH 90:\n",
      "  batch 100 loss: 0.0015281983927707189\n",
      "  batch 200 loss: 0.0018416030213120394\n",
      "  batch 300 loss: 0.0017163764545693994\n",
      "LOSS train 0.0017163764545693994 valid 0.07650330303166109\n",
      "valid accuracy 97.98\n",
      "test accuracy 98.14\n",
      "EPOCH 91:\n",
      "  batch 100 loss: 0.001580760832002852\n",
      "  batch 200 loss: 0.001732095154293347\n",
      "  batch 300 loss: 0.001712855194346048\n",
      "LOSS train 0.001712855194346048 valid 0.07637234807863266\n",
      "valid accuracy 97.98\n",
      "test accuracy 98.12\n",
      "EPOCH 92:\n",
      "  batch 100 loss: 0.001508255803200882\n",
      "  batch 200 loss: 0.0016840786204556934\n",
      "  batch 300 loss: 0.0016712858667597174\n",
      "LOSS train 0.0016712858667597174 valid 0.0764099005138195\n",
      "valid accuracy 97.97\n",
      "test accuracy 98.14\n",
      "EPOCH 93:\n",
      "  batch 100 loss: 0.0015366425516549498\n",
      "  batch 200 loss: 0.0017009115440305323\n",
      "  batch 300 loss: 0.0016103316389489918\n",
      "LOSS train 0.0016103316389489918 valid 0.07674555631378029\n",
      "valid accuracy 97.98\n",
      "test accuracy 98.09\n",
      "EPOCH 94:\n",
      "  batch 100 loss: 0.0015285153151489794\n",
      "  batch 200 loss: 0.0017012480233097448\n",
      "  batch 300 loss: 0.0015135248529259116\n",
      "LOSS train 0.0015135248529259116 valid 0.07710949711124354\n",
      "valid accuracy 97.97\n",
      "test accuracy 98.13\n",
      "EPOCH 95:\n",
      "  batch 100 loss: 0.001435047171980841\n",
      "  batch 200 loss: 0.001685725214483682\n",
      "  batch 300 loss: 0.0014592642395291476\n",
      "LOSS train 0.0014592642395291476 valid 0.07677312945074673\n",
      "valid accuracy 97.98\n",
      "test accuracy 98.07\n",
      "EPOCH 96:\n",
      "  batch 100 loss: 0.00153532222204376\n",
      "  batch 200 loss: 0.0016226810662192293\n",
      "  batch 300 loss: 0.0014827152830548585\n",
      "LOSS train 0.0014827152830548585 valid 0.07708636133141722\n",
      "valid accuracy 98.01\n",
      "test accuracy 98.1\n",
      "EPOCH 97:\n",
      "  batch 100 loss: 0.0014856563863577322\n",
      "  batch 200 loss: 0.0014109637044020927\n",
      "  batch 300 loss: 0.0016104986576829107\n",
      "LOSS train 0.0016104986576829107 valid 0.07722012676840907\n",
      "valid accuracy 98.01\n",
      "test accuracy 98.09\n",
      "EPOCH 98:\n",
      "  batch 100 loss: 0.0014815147433546371\n",
      "  batch 200 loss: 0.0014620549706160092\n",
      "  batch 300 loss: 0.001511441488401033\n",
      "LOSS train 0.001511441488401033 valid 0.07708021826877058\n",
      "valid accuracy 98.01\n",
      "test accuracy 98.13\n",
      "EPOCH 99:\n",
      "  batch 100 loss: 0.0014295533229596912\n",
      "  batch 200 loss: 0.001493608385208063\n",
      "  batch 300 loss: 0.0014500573743134737\n",
      "LOSS train 0.0014500573743134737 valid 0.07736810139392185\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.12\n",
      "EPOCH 100:\n",
      "  batch 100 loss: 0.001455798561219126\n",
      "  batch 200 loss: 0.0014172764483373612\n",
      "  batch 300 loss: 0.001398272221267689\n",
      "LOSS train 0.001398272221267689 valid 0.07754613138452361\n",
      "valid accuracy 97.99\n",
      "test accuracy 98.1\n",
      "EPOCH 101:\n",
      "  batch 100 loss: 0.0013912016799440608\n",
      "  batch 200 loss: 0.001301657598814927\n",
      "  batch 300 loss: 0.001403844770102296\n",
      "LOSS train 0.001403844770102296 valid 0.07733906362842344\n",
      "valid accuracy 97.99\n",
      "test accuracy 98.09\n",
      "EPOCH 102:\n",
      "  batch 100 loss: 0.0013055347625049762\n",
      "  batch 200 loss: 0.0013487903983332218\n",
      "  batch 300 loss: 0.0013978114342899062\n",
      "LOSS train 0.0013978114342899062 valid 0.07744498619856903\n",
      "valid accuracy 97.96\n",
      "test accuracy 98.09\n",
      "EPOCH 103:\n",
      "  batch 100 loss: 0.0014144454742199741\n",
      "  batch 200 loss: 0.0013146381379920058\n",
      "  batch 300 loss: 0.0013511827154434287\n",
      "LOSS train 0.0013511827154434287 valid 0.07747739600607112\n",
      "valid accuracy 98.0\n",
      "test accuracy 98.08\n",
      "EPOCH 104:\n",
      "  batch 100 loss: 0.0012710642046295107\n",
      "  batch 200 loss: 0.0012022458572755568\n",
      "  batch 300 loss: 0.0013960172701627015\n",
      "LOSS train 0.0013960172701627015 valid 0.07754050396217764\n",
      "valid accuracy 97.99\n",
      "test accuracy 98.1\n",
      "EPOCH 105:\n",
      "  batch 100 loss: 0.0012965525579056703\n",
      "  batch 200 loss: 0.0014123142222524621\n",
      "  batch 300 loss: 0.0012786226609023288\n",
      "LOSS train 0.0012786226609023288 valid 0.07751138004934109\n",
      "valid accuracy 97.98\n",
      "test accuracy 98.11\n",
      "EPOCH 106:\n",
      "  batch 100 loss: 0.0013911651860689744\n",
      "  batch 200 loss: 0.0012785635582986288\n",
      "  batch 300 loss: 0.001370886341028381\n",
      "LOSS train 0.001370886341028381 valid 0.07760508377340776\n",
      "valid accuracy 97.99\n",
      "test accuracy 98.09\n",
      "EPOCH 107:\n",
      "  batch 100 loss: 0.0012909022069652566\n",
      "  batch 200 loss: 0.0013140745140844956\n",
      "  batch 300 loss: 0.0013126420942717232\n",
      "LOSS train 0.0013126420942717232 valid 0.07751276690064918\n",
      "valid accuracy 97.99\n",
      "test accuracy 98.09\n",
      "EPOCH 108:\n",
      "  batch 100 loss: 0.0012364047241862864\n",
      "  batch 200 loss: 0.001257464586524293\n",
      "  batch 300 loss: 0.001354337978118565\n",
      "LOSS train 0.001354337978118565 valid 0.07756839760968202\n",
      "valid accuracy 97.99\n",
      "test accuracy 98.08\n",
      "EPOCH 109:\n",
      "  batch 100 loss: 0.0013994280295446515\n",
      "  batch 200 loss: 0.0012673191644717008\n",
      "  batch 300 loss: 0.0012827647944504861\n",
      "LOSS train 0.0012827647944504861 valid 0.07761588627236742\n",
      "valid accuracy 98.01\n",
      "test accuracy 98.08\n",
      "EPOCH 110:\n",
      "  batch 100 loss: 0.0013303392582747619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 200 loss: 0.0012492867957917043\n",
      "  batch 300 loss: 0.001275829160003923\n",
      "LOSS train 0.001275829160003923 valid 0.07789811819697483\n",
      "valid accuracy 97.97\n",
      "test accuracy 98.09\n",
      "EPOCH 111:\n",
      "  batch 100 loss: 0.0013123911689035595\n",
      "  batch 200 loss: 0.0013178450282430277\n",
      "  batch 300 loss: 0.001233185329765547\n",
      "LOSS train 0.001233185329765547 valid 0.0775548505325672\n",
      "valid accuracy 97.98\n",
      "test accuracy 98.09\n",
      "EPOCH 112:\n",
      "  batch 100 loss: 0.001333578463527374\n",
      "  batch 200 loss: 0.0012391880541690625\n",
      "  batch 300 loss: 0.0012923386433976703\n",
      "LOSS train 0.0012923386433976703 valid 0.07781943852152629\n",
      "valid accuracy 97.99\n",
      "test accuracy 98.08\n",
      "EPOCH 113:\n",
      "  batch 100 loss: 0.0012182389822555707\n",
      "  batch 200 loss: 0.0012555789051111787\n",
      "  batch 300 loss: 0.0012798178591765462\n",
      "LOSS train 0.0012798178591765462 valid 0.07786051313027362\n",
      "valid accuracy 97.96\n",
      "test accuracy 98.08\n",
      "EPOCH 114:\n",
      "  batch 100 loss: 0.001256419358542189\n",
      "  batch 200 loss: 0.0012650226222467608\n",
      "  batch 300 loss: 0.0011793831069371664\n",
      "LOSS train 0.0011793831069371664 valid 0.07788916199196931\n",
      "valid accuracy 97.98\n",
      "test accuracy 98.12\n",
      "EPOCH 115:\n",
      "  batch 100 loss: 0.0011765165530960076\n",
      "  batch 200 loss: 0.001281407666974701\n",
      "  batch 300 loss: 0.0012667792395222933\n",
      "LOSS train 0.0012667792395222933 valid 0.07780832766048328\n",
      "valid accuracy 98.01\n",
      "test accuracy 98.09\n",
      "EPOCH 116:\n",
      "  batch 100 loss: 0.0011801395393558779\n",
      "  batch 200 loss: 0.0012228429142851383\n",
      "  batch 300 loss: 0.0012733411998488008\n",
      "LOSS train 0.0012733411998488008 valid 0.0778671580402157\n",
      "valid accuracy 98.0\n",
      "test accuracy 98.09\n",
      "EPOCH 117:\n",
      "  batch 100 loss: 0.0012740027470863424\n",
      "  batch 200 loss: 0.0012183523940620944\n",
      "  batch 300 loss: 0.0012062481296015904\n",
      "LOSS train 0.0012062481296015904 valid 0.07800471177914098\n",
      "valid accuracy 98.01\n",
      "test accuracy 98.08\n",
      "EPOCH 118:\n",
      "  batch 100 loss: 0.0011793593052425422\n",
      "  batch 200 loss: 0.0012773286475567147\n",
      "  batch 300 loss: 0.001226376969134435\n",
      "LOSS train 0.001226376969134435 valid 0.07811375397340029\n",
      "valid accuracy 97.96\n",
      "test accuracy 98.09\n",
      "EPOCH 119:\n",
      "  batch 100 loss: 0.0011763297757715918\n",
      "  batch 200 loss: 0.001241966159432195\n",
      "  batch 300 loss: 0.0012094020008225925\n",
      "LOSS train 0.0012094020008225925 valid 0.07814767314828461\n",
      "valid accuracy 97.98\n",
      "test accuracy 98.08\n",
      "EPOCH 120:\n",
      "  batch 100 loss: 0.0012056198916980065\n",
      "  batch 200 loss: 0.0011993251103558577\n",
      "  batch 300 loss: 0.0011529089935356752\n",
      "LOSS train 0.0011529089935356752 valid 0.07803127747854288\n",
      "valid accuracy 97.98\n",
      "test accuracy 98.08\n",
      "EPOCH 121:\n",
      "  batch 100 loss: 0.0011869907134678215\n",
      "  batch 200 loss: 0.0011865481903078035\n",
      "  batch 300 loss: 0.0012515864762826822\n",
      "LOSS train 0.0012515864762826822 valid 0.07799810188221215\n",
      "valid accuracy 98.0\n",
      "test accuracy 98.1\n",
      "EPOCH 122:\n",
      "  batch 100 loss: 0.001158818620024249\n",
      "  batch 200 loss: 0.0012446988499141298\n",
      "  batch 300 loss: 0.0012236645317170768\n",
      "LOSS train 0.0012236645317170768 valid 0.07824188651210523\n",
      "valid accuracy 98.0\n",
      "test accuracy 98.08\n",
      "EPOCH 123:\n",
      "  batch 100 loss: 0.0011228682802175171\n",
      "  batch 200 loss: 0.0011417358061589767\n",
      "  batch 300 loss: 0.001230513050686568\n",
      "LOSS train 0.001230513050686568 valid 0.07809222945825586\n",
      "valid accuracy 98.01\n",
      "test accuracy 98.08\n",
      "EPOCH 124:\n",
      "  batch 100 loss: 0.0012609032564796507\n",
      "  batch 200 loss: 0.0011143870621162933\n",
      "  batch 300 loss: 0.001233985687722452\n",
      "LOSS train 0.001233985687722452 valid 0.07820746065647918\n",
      "valid accuracy 97.99\n",
      "test accuracy 98.09\n",
      "EPOCH 125:\n",
      "  batch 100 loss: 0.0012142754426167812\n",
      "  batch 200 loss: 0.0011552409455180168\n",
      "  batch 300 loss: 0.0012624022102681919\n",
      "LOSS train 0.0012624022102681919 valid 0.07814009808286836\n",
      "valid accuracy 98.0\n",
      "test accuracy 98.09\n",
      "EPOCH 126:\n",
      "  batch 100 loss: 0.0012336463676183484\n",
      "  batch 200 loss: 0.0011048505030339583\n",
      "  batch 300 loss: 0.0012260597373824566\n",
      "LOSS train 0.0012260597373824566 valid 0.07820458943024278\n",
      "valid accuracy 98.01\n",
      "test accuracy 98.1\n",
      "EPOCH 127:\n",
      "  batch 100 loss: 0.0011921881246962584\n",
      "  batch 200 loss: 0.0011557244785944931\n",
      "  batch 300 loss: 0.0012064779932552483\n",
      "LOSS train 0.0012064779932552483 valid 0.07826535596476891\n",
      "valid accuracy 98.0\n",
      "test accuracy 98.09\n",
      "EPOCH 128:\n",
      "  batch 100 loss: 0.0011798553042171988\n",
      "  batch 200 loss: 0.001125597218342591\n",
      "  batch 300 loss: 0.0012122935318620875\n",
      "LOSS train 0.0012122935318620875 valid 0.078392547755679\n",
      "valid accuracy 97.98\n",
      "test accuracy 98.09\n",
      "EPOCH 129:\n",
      "  batch 100 loss: 0.0011579243894084357\n",
      "  batch 200 loss: 0.0011844929368817246\n",
      "  batch 300 loss: 0.0011561093694763258\n",
      "LOSS train 0.0011561093694763258 valid 0.0783291203087738\n",
      "valid accuracy 98.0\n",
      "test accuracy 98.09\n",
      "EPOCH 130:\n",
      "  batch 100 loss: 0.0011347665276844054\n",
      "  batch 200 loss: 0.0012226480949902906\n",
      "  batch 300 loss: 0.0011547348901513033\n",
      "LOSS train 0.0011547348901513033 valid 0.07836800424641446\n",
      "valid accuracy 97.98\n",
      "test accuracy 98.08\n",
      "EPOCH 131:\n",
      "  batch 100 loss: 0.0010739362676395103\n",
      "  batch 200 loss: 0.001193352522677742\n",
      "  batch 300 loss: 0.0011719311022898183\n",
      "LOSS train 0.0011719311022898183 valid 0.07827663607305929\n",
      "valid accuracy 98.01\n",
      "test accuracy 98.1\n",
      "EPOCH 132:\n",
      "  batch 100 loss: 0.0011690407070273068\n",
      "  batch 200 loss: 0.0011488409002777188\n",
      "  batch 300 loss: 0.001132164220616687\n",
      "LOSS train 0.001132164220616687 valid 0.07852347691460879\n",
      "valid accuracy 97.98\n",
      "test accuracy 98.09\n",
      "EPOCH 133:\n",
      "  batch 100 loss: 0.0011652513887383976\n",
      "  batch 200 loss: 0.0011729761780588889\n",
      "  batch 300 loss: 0.0012045061500975862\n",
      "LOSS train 0.0012045061500975862 valid 0.07846187654576159\n",
      "valid accuracy 97.99\n",
      "test accuracy 98.09\n",
      "EPOCH 134:\n",
      "  batch 100 loss: 0.0010637427962501534\n",
      "  batch 200 loss: 0.0011230143925058656\n",
      "  batch 300 loss: 0.0012067007520818152\n",
      "LOSS train 0.0012067007520818152 valid 0.07843150508507521\n",
      "valid accuracy 97.98\n",
      "test accuracy 98.1\n",
      "EPOCH 135:\n",
      "  batch 100 loss: 0.0010478436297853477\n",
      "  batch 200 loss: 0.0011584941376349888\n",
      "  batch 300 loss: 0.0012300913452054374\n",
      "LOSS train 0.0012300913452054374 valid 0.0785290306799491\n",
      "valid accuracy 97.97\n",
      "test accuracy 98.08\n",
      "EPOCH 136:\n",
      "  batch 100 loss: 0.0011428940569749102\n",
      "  batch 200 loss: 0.00118206840736093\n",
      "  batch 300 loss: 0.0011152139876503498\n",
      "LOSS train 0.0011152139876503498 valid 0.07855660316263195\n",
      "valid accuracy 97.98\n",
      "test accuracy 98.09\n",
      "EPOCH 137:\n",
      "  batch 100 loss: 0.0011287800740683452\n",
      "  batch 200 loss: 0.0011273261881433428\n",
      "  batch 300 loss: 0.0011311218308401295\n",
      "LOSS train 0.0011311218308401295 valid 0.0786494224780251\n",
      "valid accuracy 98.0\n",
      "test accuracy 98.09\n",
      "EPOCH 138:\n",
      "  batch 100 loss: 0.0011409451067447662\n",
      "  batch 200 loss: 0.0010123415221460165\n",
      "  batch 300 loss: 0.0011340840751654468\n",
      "LOSS train 0.0011340840751654468 valid 0.07867525217323741\n",
      "valid accuracy 98.0\n",
      "test accuracy 98.08\n",
      "EPOCH 139:\n",
      "  batch 100 loss: 0.0011200564440514427\n",
      "  batch 200 loss: 0.0010904963797656818\n",
      "  batch 300 loss: 0.0011341135541442782\n",
      "LOSS train 0.0011341135541442782 valid 0.0785510137417837\n",
      "valid accuracy 97.98\n",
      "test accuracy 98.08\n",
      "EPOCH 140:\n",
      "  batch 100 loss: 0.0010423870096565224\n",
      "  batch 200 loss: 0.0011405013914918528\n",
      "  batch 300 loss: 0.001077339037146885\n",
      "LOSS train 0.001077339037146885 valid 0.0787098091095686\n",
      "valid accuracy 98.01\n",
      "test accuracy 98.1\n",
      "EPOCH 141:\n",
      "  batch 100 loss: 0.0011016732599819078\n",
      "  batch 200 loss: 0.0010667336263577454\n",
      "  batch 300 loss: 0.0010546183402766474\n",
      "LOSS train 0.0010546183402766474 valid 0.07869117966369737\n",
      "valid accuracy 98.0\n",
      "test accuracy 98.1\n",
      "EPOCH 142:\n",
      "  batch 100 loss: 0.001051856477279216\n",
      "  batch 200 loss: 0.0011217252005008048\n",
      "  batch 300 loss: 0.001160901102120988\n",
      "LOSS train 0.001160901102120988 valid 0.07867078399993008\n",
      "valid accuracy 97.99\n",
      "test accuracy 98.08\n",
      "EPOCH 143:\n",
      "  batch 100 loss: 0.0011857329502527136\n",
      "  batch 200 loss: 0.001066952002729522\n",
      "  batch 300 loss: 0.0011009100324008614\n",
      "LOSS train 0.0011009100324008614 valid 0.07866743419767369\n",
      "valid accuracy 97.96\n",
      "test accuracy 98.09\n",
      "EPOCH 144:\n",
      "  batch 100 loss: 0.0010483892384218051\n",
      "  batch 200 loss: 0.0010760818773997017\n",
      "  batch 300 loss: 0.001130025213060435\n",
      "LOSS train 0.001130025213060435 valid 0.07875971668223038\n",
      "valid accuracy 97.99\n",
      "test accuracy 98.1\n",
      "EPOCH 145:\n",
      "  batch 100 loss: 0.0011218098187237046\n",
      "  batch 200 loss: 0.0010493448510533198\n",
      "  batch 300 loss: 0.0010473437936161644\n",
      "LOSS train 0.0010473437936161644 valid 0.0788526706562578\n",
      "valid accuracy 98.0\n",
      "test accuracy 98.08\n",
      "EPOCH 146:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 100 loss: 0.001123302373162005\n",
      "  batch 200 loss: 0.0010172707040328533\n",
      "  batch 300 loss: 0.0011200198251754044\n",
      "LOSS train 0.0011200198251754044 valid 0.07887947054792054\n",
      "valid accuracy 97.99\n",
      "test accuracy 98.09\n",
      "EPOCH 147:\n",
      "  batch 100 loss: 0.0011005423119058832\n",
      "  batch 200 loss: 0.0010971373226493597\n",
      "  batch 300 loss: 0.0009945430344669149\n",
      "LOSS train 0.0009945430344669149 valid 0.07899108103511832\n",
      "valid accuracy 98.0\n",
      "test accuracy 98.08\n",
      "EPOCH 148:\n",
      "  batch 100 loss: 0.0010608165834855754\n",
      "  batch 200 loss: 0.0010639869706938043\n",
      "  batch 300 loss: 0.0011041210530675016\n",
      "LOSS train 0.0011041210530675016 valid 0.07900566547000899\n",
      "valid accuracy 98.0\n",
      "test accuracy 98.08\n",
      "EPOCH 149:\n",
      "  batch 100 loss: 0.0011060516093857587\n",
      "  batch 200 loss: 0.001089798241155222\n",
      "  batch 300 loss: 0.0010513897132477723\n",
      "LOSS train 0.0010513897132477723 valid 0.07900176224404876\n",
      "valid accuracy 97.98\n",
      "test accuracy 98.11\n",
      "EPOCH 150:\n",
      "  batch 100 loss: 0.0010393684994778596\n",
      "  batch 200 loss: 0.0010779774785623886\n",
      "  batch 300 loss: 0.001093393990304321\n",
      "LOSS train 0.001093393990304321 valid 0.07889960030140945\n",
      "valid accuracy 97.98\n",
      "test accuracy 98.09\n",
      "EPOCH 151:\n",
      "  batch 100 loss: 0.001098082788521424\n",
      "  batch 200 loss: 0.001062247139052488\n",
      "  batch 300 loss: 0.0010376497929973994\n",
      "LOSS train 0.0010376497929973994 valid 0.07911175617924597\n",
      "valid accuracy 98.01\n",
      "test accuracy 98.09\n",
      "EPOCH 152:\n",
      "  batch 100 loss: 0.0009882610410568305\n",
      "  batch 200 loss: 0.0010582658706698566\n",
      "  batch 300 loss: 0.0010773656857782045\n",
      "LOSS train 0.0010773656857782045 valid 0.07893745861711758\n",
      "valid accuracy 97.98\n",
      "test accuracy 98.1\n",
      "EPOCH 153:\n",
      "  batch 100 loss: 0.0010243384324712679\n",
      "  batch 200 loss: 0.0010810579937242438\n",
      "  batch 300 loss: 0.0010588426733738742\n",
      "LOSS train 0.0010588426733738742 valid 0.07897770109503896\n",
      "valid accuracy 97.99\n",
      "test accuracy 98.09\n",
      "EPOCH 154:\n",
      "  batch 100 loss: 0.00104052434413461\n",
      "  batch 200 loss: 0.0010834100015927107\n",
      "  batch 300 loss: 0.0010108366773056332\n",
      "LOSS train 0.0010108366773056332 valid 0.0789975935086325\n",
      "valid accuracy 97.99\n",
      "test accuracy 98.09\n",
      "EPOCH 155:\n",
      "  batch 100 loss: 0.0009604950653738342\n",
      "  batch 200 loss: 0.001160124646848999\n",
      "  batch 300 loss: 0.0010940112278331072\n",
      "LOSS train 0.0010940112278331072 valid 0.07906180567143462\n",
      "valid accuracy 97.98\n",
      "test accuracy 98.09\n",
      "EPOCH 156:\n",
      "  batch 100 loss: 0.001059490848419955\n",
      "  batch 200 loss: 0.0009581495124439244\n",
      "  batch 300 loss: 0.0009831941622542217\n",
      "LOSS train 0.0009831941622542217 valid 0.07905708687214912\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.11\n",
      "EPOCH 157:\n",
      "  batch 100 loss: 0.0010075439707725308\n",
      "  batch 200 loss: 0.001042096968740225\n",
      "  batch 300 loss: 0.0010483856126666069\n",
      "LOSS train 0.0010483856126666069 valid 0.07916584016189357\n",
      "valid accuracy 97.99\n",
      "test accuracy 98.08\n",
      "EPOCH 158:\n",
      "  batch 100 loss: 0.0010078252368839458\n",
      "  batch 200 loss: 0.0010103379146312362\n",
      "  batch 300 loss: 0.0010810561291873454\n",
      "LOSS train 0.0010810561291873454 valid 0.0790823984808654\n",
      "valid accuracy 97.98\n",
      "test accuracy 98.09\n",
      "EPOCH 159:\n",
      "  batch 100 loss: 0.0009865607487154193\n",
      "  batch 200 loss: 0.0010090237238910048\n",
      "  batch 300 loss: 0.001023193357978016\n",
      "LOSS train 0.001023193357978016 valid 0.07924303496728026\n",
      "valid accuracy 98.0\n",
      "test accuracy 98.09\n",
      "EPOCH 160:\n",
      "  batch 100 loss: 0.0010092582055949606\n",
      "  batch 200 loss: 0.001000672132940963\n",
      "  batch 300 loss: 0.0010280399330076762\n",
      "LOSS train 0.0010280399330076762 valid 0.07933995220810175\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.11\n",
      "EPOCH 161:\n",
      "  batch 100 loss: 0.0009415466815698892\n",
      "  batch 200 loss: 0.001101479106437182\n",
      "  batch 300 loss: 0.001019880541425664\n",
      "LOSS train 0.001019880541425664 valid 0.07929963245987892\n",
      "valid accuracy 97.99\n",
      "test accuracy 98.09\n",
      "EPOCH 162:\n",
      "  batch 100 loss: 0.0010457550309365615\n",
      "  batch 200 loss: 0.0010369284704211169\n",
      "  batch 300 loss: 0.0009730848169419914\n",
      "LOSS train 0.0009730848169419914 valid 0.07933187322054483\n",
      "valid accuracy 98.01\n",
      "test accuracy 98.09\n",
      "EPOCH 163:\n",
      "  batch 100 loss: 0.0010020361212082207\n",
      "  batch 200 loss: 0.0010494026730884798\n",
      "  batch 300 loss: 0.0009870184134342709\n",
      "LOSS train 0.0009870184134342709 valid 0.07935758782668581\n",
      "valid accuracy 98.0\n",
      "test accuracy 98.1\n",
      "EPOCH 164:\n",
      "  batch 100 loss: 0.0009646132617490366\n",
      "  batch 200 loss: 0.000994852313597221\n",
      "  batch 300 loss: 0.0010732196876779199\n",
      "LOSS train 0.0010732196876779199 valid 0.07930490578512979\n",
      "valid accuracy 97.98\n",
      "test accuracy 98.09\n",
      "EPOCH 165:\n",
      "  batch 100 loss: 0.0009522129470133223\n",
      "  batch 200 loss: 0.0009763754718005658\n",
      "  batch 300 loss: 0.001052940345252864\n",
      "LOSS train 0.001052940345252864 valid 0.07939319265416905\n",
      "valid accuracy 97.98\n",
      "test accuracy 98.1\n",
      "EPOCH 166:\n",
      "  batch 100 loss: 0.000985396582982503\n",
      "  batch 200 loss: 0.0009848387111560442\n",
      "  batch 300 loss: 0.0009708732186118141\n",
      "LOSS train 0.0009708732186118141 valid 0.07936303992460988\n",
      "valid accuracy 98.0\n",
      "test accuracy 98.09\n",
      "EPOCH 167:\n",
      "  batch 100 loss: 0.0009888995891378727\n",
      "  batch 200 loss: 0.0010144966491498052\n",
      "  batch 300 loss: 0.0009561581062735059\n",
      "LOSS train 0.0009561581062735059 valid 0.07943185737144344\n",
      "valid accuracy 97.99\n",
      "test accuracy 98.1\n",
      "EPOCH 168:\n",
      "  batch 100 loss: 0.0009670018701581285\n",
      "  batch 200 loss: 0.0009507058330927976\n",
      "  batch 300 loss: 0.001049602951388806\n",
      "LOSS train 0.001049602951388806 valid 0.07940816309868912\n",
      "valid accuracy 98.0\n",
      "test accuracy 98.1\n",
      "EPOCH 169:\n",
      "  batch 100 loss: 0.0009862546692602336\n",
      "  batch 200 loss: 0.000942330670368392\n",
      "  batch 300 loss: 0.001054883804754354\n",
      "LOSS train 0.001054883804754354 valid 0.07955690669603174\n",
      "valid accuracy 98.0\n",
      "test accuracy 98.09\n",
      "EPOCH 170:\n",
      "  batch 100 loss: 0.001004777865309734\n",
      "  batch 200 loss: 0.0010097600053995848\n",
      "  batch 300 loss: 0.000971490498923231\n",
      "LOSS train 0.000971490498923231 valid 0.07952044589468572\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.11\n",
      "EPOCH 171:\n",
      "  batch 100 loss: 0.0009520964263356291\n",
      "  batch 200 loss: 0.0009308759437408298\n",
      "  batch 300 loss: 0.0009368587218341418\n",
      "LOSS train 0.0009368587218341418 valid 0.07956575692478049\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.1\n",
      "EPOCH 172:\n",
      "  batch 100 loss: 0.0009288597013801336\n",
      "  batch 200 loss: 0.0009816438794950954\n",
      "  batch 300 loss: 0.0009787338500609622\n",
      "LOSS train 0.0009787338500609622 valid 0.0795461795811506\n",
      "valid accuracy 97.99\n",
      "test accuracy 98.09\n",
      "EPOCH 173:\n",
      "  batch 100 loss: 0.0009842121304245665\n",
      "  batch 200 loss: 0.000990258969104616\n",
      "  batch 300 loss: 0.000933563748258166\n",
      "LOSS train 0.000933563748258166 valid 0.07966007173202838\n",
      "valid accuracy 98.01\n",
      "test accuracy 98.1\n",
      "EPOCH 174:\n",
      "  batch 100 loss: 0.0010018884492455982\n",
      "  batch 200 loss: 0.0009372977595194243\n",
      "  batch 300 loss: 0.0009457585042400751\n",
      "LOSS train 0.0009457585042400751 valid 0.07962594661696613\n",
      "valid accuracy 98.0\n",
      "test accuracy 98.1\n",
      "EPOCH 175:\n",
      "  batch 100 loss: 0.0009312446325202472\n",
      "  batch 200 loss: 0.0010264895905856975\n",
      "  batch 300 loss: 0.000920915350143332\n",
      "LOSS train 0.000920915350143332 valid 0.07978717471720485\n",
      "valid accuracy 97.99\n",
      "test accuracy 98.1\n",
      "EPOCH 176:\n",
      "  batch 100 loss: 0.0009432220592861995\n",
      "  batch 200 loss: 0.0010067448210611474\n",
      "  batch 300 loss: 0.000941342799924314\n",
      "LOSS train 0.000941342799924314 valid 0.07966964165503278\n",
      "valid accuracy 98.03\n",
      "test accuracy 98.11\n",
      "EPOCH 177:\n",
      "  batch 100 loss: 0.0009739256728062174\n",
      "  batch 200 loss: 0.0008781654824269935\n",
      "  batch 300 loss: 0.0009669721387035679\n",
      "LOSS train 0.0009669721387035679 valid 0.07978284149725415\n",
      "valid accuracy 98.0\n",
      "test accuracy 98.08\n",
      "EPOCH 178:\n",
      "  batch 100 loss: 0.0010154370444070081\n",
      "  batch 200 loss: 0.0008742773135600146\n",
      "  batch 300 loss: 0.000936735700306599\n",
      "LOSS train 0.000936735700306599 valid 0.079703945012268\n",
      "valid accuracy 98.0\n",
      "test accuracy 98.08\n",
      "EPOCH 179:\n",
      "  batch 100 loss: 0.0009517962942481972\n",
      "  batch 200 loss: 0.0009161343563755509\n",
      "  batch 300 loss: 0.0009568196244072169\n",
      "LOSS train 0.0009568196244072169 valid 0.07980631181640149\n",
      "valid accuracy 97.98\n",
      "test accuracy 98.1\n",
      "EPOCH 180:\n",
      "  batch 100 loss: 0.0009377680988109204\n",
      "  batch 200 loss: 0.0008920915832277387\n",
      "  batch 300 loss: 0.0009167882814654149\n",
      "LOSS train 0.0009167882814654149 valid 0.07983836816930318\n",
      "valid accuracy 98.01\n",
      "test accuracy 98.09\n",
      "EPOCH 181:\n",
      "  batch 100 loss: 0.0009624661135603674\n",
      "  batch 200 loss: 0.0009028294557356276\n",
      "  batch 300 loss: 0.0009141493675997481\n",
      "LOSS train 0.0009141493675997481 valid 0.07978669958354175\n",
      "valid accuracy 98.03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 98.12\n",
      "EPOCH 182:\n",
      "  batch 100 loss: 0.0008443748082208913\n",
      "  batch 200 loss: 0.0009570400955271907\n",
      "  batch 300 loss: 0.0010142942865786608\n",
      "LOSS train 0.0010142942865786608 valid 0.07979861096938766\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.12\n",
      "EPOCH 183:\n",
      "  batch 100 loss: 0.0009228690041345545\n",
      "  batch 200 loss: 0.0008998402915312908\n",
      "  batch 300 loss: 0.0009841652665636502\n",
      "LOSS train 0.0009841652665636502 valid 0.07994805456763959\n",
      "valid accuracy 98.01\n",
      "test accuracy 98.09\n",
      "EPOCH 184:\n",
      "  batch 100 loss: 0.0008776840932841879\n",
      "  batch 200 loss: 0.0009071146600763314\n",
      "  batch 300 loss: 0.0009570953866932541\n",
      "LOSS train 0.0009570953866932541 valid 0.07988091779849198\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.08\n",
      "EPOCH 185:\n",
      "  batch 100 loss: 0.0009350833456846885\n",
      "  batch 200 loss: 0.0009516192568116822\n",
      "  batch 300 loss: 0.0009011157034547068\n",
      "LOSS train 0.0009011157034547068 valid 0.07989767680086102\n",
      "valid accuracy 98.01\n",
      "test accuracy 98.09\n",
      "EPOCH 186:\n",
      "  batch 100 loss: 0.0008673863721196539\n",
      "  batch 200 loss: 0.0008955017014523037\n",
      "  batch 300 loss: 0.0009768065920798107\n",
      "LOSS train 0.0009768065920798107 valid 0.08007397620549685\n",
      "valid accuracy 98.03\n",
      "test accuracy 98.07\n",
      "EPOCH 187:\n",
      "  batch 100 loss: 0.000866884197894251\n",
      "  batch 200 loss: 0.0009280405606841668\n",
      "  batch 300 loss: 0.0009493619602289982\n",
      "LOSS train 0.0009493619602289982 valid 0.07999985514185097\n",
      "valid accuracy 98.0\n",
      "test accuracy 98.1\n",
      "EPOCH 188:\n",
      "  batch 100 loss: 0.0009255727211711928\n",
      "  batch 200 loss: 0.0009157479053828866\n",
      "  batch 300 loss: 0.0008773804569500498\n",
      "LOSS train 0.0008773804569500498 valid 0.08001499329516783\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.1\n",
      "EPOCH 189:\n",
      "  batch 100 loss: 0.0008560577379830648\n",
      "  batch 200 loss: 0.0009180724892939907\n",
      "  batch 300 loss: 0.0009530485505820252\n",
      "LOSS train 0.0009530485505820252 valid 0.08014910630999675\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.09\n",
      "EPOCH 190:\n",
      "  batch 100 loss: 0.0008755307368119247\n",
      "  batch 200 loss: 0.0009496178667177446\n",
      "  batch 300 loss: 0.000851399464008864\n",
      "LOSS train 0.000851399464008864 valid 0.08006022193268696\n",
      "valid accuracy 98.01\n",
      "test accuracy 98.09\n",
      "EPOCH 191:\n",
      "  batch 100 loss: 0.0008463350834790617\n",
      "  batch 200 loss: 0.0009058281479519792\n",
      "  batch 300 loss: 0.0009203527084900997\n",
      "LOSS train 0.0009203527084900997 valid 0.08006083490798556\n",
      "valid accuracy 98.0\n",
      "test accuracy 98.09\n",
      "EPOCH 192:\n",
      "  batch 100 loss: 0.0009030443386291154\n",
      "  batch 200 loss: 0.0008762875027605332\n",
      "  batch 300 loss: 0.0008387053316982929\n",
      "LOSS train 0.0008387053316982929 valid 0.08019028545086142\n",
      "valid accuracy 98.01\n",
      "test accuracy 98.09\n",
      "EPOCH 193:\n",
      "  batch 100 loss: 0.0008912843742291443\n",
      "  batch 200 loss: 0.0008951254925341345\n",
      "  batch 300 loss: 0.0008995808989857323\n",
      "LOSS train 0.0008995808989857323 valid 0.08016232430015373\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.09\n",
      "EPOCH 194:\n",
      "  batch 100 loss: 0.0009695108907180839\n",
      "  batch 200 loss: 0.0008142007602145896\n",
      "  batch 300 loss: 0.0008718860719818622\n",
      "LOSS train 0.0008718860719818622 valid 0.08017790835067819\n",
      "valid accuracy 98.03\n",
      "test accuracy 98.11\n",
      "EPOCH 195:\n",
      "  batch 100 loss: 0.0009222214558394626\n",
      "  batch 200 loss: 0.0008608059294056147\n",
      "  batch 300 loss: 0.0009051851237018127\n",
      "LOSS train 0.0009051851237018127 valid 0.08018152510653002\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.11\n",
      "EPOCH 196:\n",
      "  batch 100 loss: 0.0008387674420373514\n",
      "  batch 200 loss: 0.0009002026476082392\n",
      "  batch 300 loss: 0.0008705696075048763\n",
      "LOSS train 0.0008705696075048763 valid 0.08026035826158108\n",
      "valid accuracy 97.99\n",
      "test accuracy 98.08\n",
      "EPOCH 197:\n",
      "  batch 100 loss: 0.0008318723920092453\n",
      "  batch 200 loss: 0.000794361708976794\n",
      "  batch 300 loss: 0.0009320149800623767\n",
      "LOSS train 0.0009320149800623767 valid 0.08022809162429428\n",
      "valid accuracy 98.0\n",
      "test accuracy 98.09\n",
      "EPOCH 198:\n",
      "  batch 100 loss: 0.000851456627715379\n",
      "  batch 200 loss: 0.0008629084966378286\n",
      "  batch 300 loss: 0.0009166670292324853\n",
      "LOSS train 0.0009166670292324853 valid 0.08024088943471448\n",
      "valid accuracy 98.01\n",
      "test accuracy 98.1\n",
      "EPOCH 199:\n",
      "  batch 100 loss: 0.0008625993094756268\n",
      "  batch 200 loss: 0.0009087607597757597\n",
      "  batch 300 loss: 0.0008786686844541691\n",
      "LOSS train 0.0008786686844541691 valid 0.0802135126064001\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.11\n",
      "EPOCH 200:\n",
      "  batch 100 loss: 0.0008853532894863747\n",
      "  batch 200 loss: 0.0008331829965754877\n",
      "  batch 300 loss: 0.0008701677877979818\n",
      "LOSS train 0.0008701677877979818 valid 0.0802670471499898\n",
      "valid accuracy 98.01\n",
      "test accuracy 98.09\n",
      "EPOCH 201:\n",
      "  batch 100 loss: 0.0008837598888203501\n",
      "  batch 200 loss: 0.0008413191721774638\n",
      "  batch 300 loss: 0.0008410916588036343\n",
      "LOSS train 0.0008410916588036343 valid 0.08028125313782616\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.1\n",
      "EPOCH 202:\n",
      "  batch 100 loss: 0.000870566203375347\n",
      "  batch 200 loss: 0.0008198119395819958\n",
      "  batch 300 loss: 0.0008540892598102801\n",
      "LOSS train 0.0008540892598102801 valid 0.08033749333853964\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.1\n",
      "EPOCH 203:\n",
      "  batch 100 loss: 0.0009147053246852011\n",
      "  batch 200 loss: 0.0008619112367159687\n",
      "  batch 300 loss: 0.0008061846921918914\n",
      "LOSS train 0.0008061846921918914 valid 0.08036113065915017\n",
      "valid accuracy 98.01\n",
      "test accuracy 98.1\n",
      "EPOCH 204:\n",
      "  batch 100 loss: 0.0008135830641549547\n",
      "  batch 200 loss: 0.0008188301940390374\n",
      "  batch 300 loss: 0.0009027359267929569\n",
      "LOSS train 0.0009027359267929569 valid 0.08037871312750858\n",
      "valid accuracy 98.01\n",
      "test accuracy 98.1\n",
      "EPOCH 205:\n",
      "  batch 100 loss: 0.0008167260469053872\n",
      "  batch 200 loss: 0.0009019563859328628\n",
      "  batch 300 loss: 0.0008486535889096558\n",
      "LOSS train 0.0008486535889096558 valid 0.08037345502761346\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.09\n",
      "EPOCH 206:\n",
      "  batch 100 loss: 0.0007871285433066077\n",
      "  batch 200 loss: 0.0008401197849889286\n",
      "  batch 300 loss: 0.0008462568458344322\n",
      "LOSS train 0.0008462568458344322 valid 0.08036965303288042\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.1\n",
      "EPOCH 207:\n",
      "  batch 100 loss: 0.0008599468297325074\n",
      "  batch 200 loss: 0.0008631960485945456\n",
      "  batch 300 loss: 0.000840920801856555\n",
      "LOSS train 0.000840920801856555 valid 0.08036838099949911\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.1\n",
      "EPOCH 208:\n",
      "  batch 100 loss: 0.0008602085537859239\n",
      "  batch 200 loss: 0.0008362277629203163\n",
      "  batch 300 loss: 0.0008725769014563411\n",
      "LOSS train 0.0008725769014563411 valid 0.08040397402040567\n",
      "valid accuracy 98.01\n",
      "test accuracy 98.1\n",
      "EPOCH 209:\n",
      "  batch 100 loss: 0.0008199985485407524\n",
      "  batch 200 loss: 0.0009145868881023489\n",
      "  batch 300 loss: 0.0008324105833889916\n",
      "LOSS train 0.0008324105833889916 valid 0.08041590235278576\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.1\n",
      "EPOCH 210:\n",
      "  batch 100 loss: 0.0008618975960416719\n",
      "  batch 200 loss: 0.0008325413893908262\n",
      "  batch 300 loss: 0.0008287200189079158\n",
      "LOSS train 0.0008287200189079158 valid 0.08044047048904851\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.09\n",
      "EPOCH 211:\n",
      "  batch 100 loss: 0.0007869193580700085\n",
      "  batch 200 loss: 0.0008317818303476088\n",
      "  batch 300 loss: 0.0008736278506694361\n",
      "LOSS train 0.0008736278506694361 valid 0.08037475108675941\n",
      "valid accuracy 98.01\n",
      "test accuracy 98.11\n",
      "EPOCH 212:\n",
      "  batch 100 loss: 0.0008520713784673717\n",
      "  batch 200 loss: 0.0008356280245061499\n",
      "  batch 300 loss: 0.0008574795123422518\n",
      "LOSS train 0.0008574795123422518 valid 0.08045764798534256\n",
      "valid accuracy 98.01\n",
      "test accuracy 98.1\n",
      "EPOCH 213:\n",
      "  batch 100 loss: 0.0008591430417436641\n",
      "  batch 200 loss: 0.0007913258107146248\n",
      "  batch 300 loss: 0.0008553375353221781\n",
      "LOSS train 0.0008553375353221781 valid 0.08041742344422242\n",
      "valid accuracy 98.01\n",
      "test accuracy 98.1\n",
      "EPOCH 214:\n",
      "  batch 100 loss: 0.0008527193623012863\n",
      "  batch 200 loss: 0.0008047857420751825\n",
      "  batch 300 loss: 0.0008635103833512404\n",
      "LOSS train 0.0008635103833512404 valid 0.08045839163462949\n",
      "valid accuracy 98.01\n",
      "test accuracy 98.1\n",
      "EPOCH 215:\n",
      "  batch 100 loss: 0.0008398105332162231\n",
      "  batch 200 loss: 0.0008096708983066492\n",
      "  batch 300 loss: 0.0008712302427738905\n",
      "LOSS train 0.0008712302427738905 valid 0.08044827234919501\n",
      "valid accuracy 98.01\n",
      "test accuracy 98.1\n",
      "EPOCH 216:\n",
      "  batch 100 loss: 0.0008433878002688289\n",
      "  batch 200 loss: 0.0008958246599650011\n",
      "  batch 300 loss: 0.0007980646383657586\n",
      "LOSS train 0.0007980646383657586 valid 0.0804326157144542\n",
      "valid accuracy 98.01\n",
      "test accuracy 98.11\n",
      "EPOCH 217:\n",
      "  batch 100 loss: 0.0008363613935944158\n",
      "  batch 200 loss: 0.0008506400015903636\n",
      "  batch 300 loss: 0.000793573899572948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 0.000793573899572948 valid 0.0804585522465125\n",
      "valid accuracy 98.01\n",
      "test accuracy 98.09\n",
      "EPOCH 218:\n",
      "  batch 100 loss: 0.0008075175349949859\n",
      "  batch 200 loss: 0.0008132263748848346\n",
      "  batch 300 loss: 0.0008476278992020525\n",
      "LOSS train 0.0008476278992020525 valid 0.08046865779788623\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.09\n",
      "EPOCH 219:\n",
      "  batch 100 loss: 0.000825250961643178\n",
      "  batch 200 loss: 0.0008411495931795798\n",
      "  batch 300 loss: 0.0008213559695286676\n",
      "LOSS train 0.0008213559695286676 valid 0.08046905175912418\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.1\n",
      "EPOCH 220:\n",
      "  batch 100 loss: 0.0008416159541229717\n",
      "  batch 200 loss: 0.0008556336408946663\n",
      "  batch 300 loss: 0.0008647928750724532\n",
      "LOSS train 0.0008647928750724532 valid 0.08049187018783598\n",
      "valid accuracy 98.01\n",
      "test accuracy 98.1\n",
      "EPOCH 221:\n",
      "  batch 100 loss: 0.0008244935495895333\n",
      "  batch 200 loss: 0.0008598610022454523\n",
      "  batch 300 loss: 0.0007832725788466632\n",
      "LOSS train 0.0007832725788466632 valid 0.08047087963882692\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.1\n",
      "EPOCH 222:\n",
      "  batch 100 loss: 0.000801105911959894\n",
      "  batch 200 loss: 0.0008531467562715989\n",
      "  batch 300 loss: 0.000832648774376139\n",
      "LOSS train 0.000832648774376139 valid 0.08047763307732117\n",
      "valid accuracy 98.01\n",
      "test accuracy 98.1\n",
      "EPOCH 223:\n",
      "  batch 100 loss: 0.0008248801781155635\n",
      "  batch 200 loss: 0.0008004195957619231\n",
      "  batch 300 loss: 0.0008484371411032044\n",
      "LOSS train 0.0008484371411032044 valid 0.08050304197387982\n",
      "valid accuracy 98.01\n",
      "test accuracy 98.1\n",
      "EPOCH 224:\n",
      "  batch 100 loss: 0.0008148750194231979\n",
      "  batch 200 loss: 0.0008348272772855125\n",
      "  batch 300 loss: 0.0008518150166491978\n",
      "LOSS train 0.0008518150166491978 valid 0.08048942501220523\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.1\n",
      "EPOCH 225:\n",
      "  batch 100 loss: 0.0008176943674334325\n",
      "  batch 200 loss: 0.0008063904123264365\n",
      "  batch 300 loss: 0.0008642200971371494\n",
      "LOSS train 0.0008642200971371494 valid 0.08048485063865216\n",
      "valid accuracy 98.01\n",
      "test accuracy 98.1\n",
      "EPOCH 226:\n",
      "  batch 100 loss: 0.0008043968773563392\n",
      "  batch 200 loss: 0.0008229119253519456\n",
      "  batch 300 loss: 0.0007842634324333631\n",
      "LOSS train 0.0007842634324333631 valid 0.08048993831241055\n",
      "valid accuracy 98.01\n",
      "test accuracy 98.11\n",
      "EPOCH 227:\n",
      "  batch 100 loss: 0.0007944535004207865\n",
      "  batch 200 loss: 0.0008253637488815002\n",
      "  batch 300 loss: 0.0008559133829839994\n",
      "LOSS train 0.0008559133829839994 valid 0.08052754815361357\n",
      "valid accuracy 98.01\n",
      "test accuracy 98.1\n",
      "EPOCH 228:\n",
      "  batch 100 loss: 0.000849847042845795\n",
      "  batch 200 loss: 0.0008500360150355845\n",
      "  batch 300 loss: 0.0007831278003868646\n",
      "LOSS train 0.0007831278003868646 valid 0.08055020348888033\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.1\n",
      "EPOCH 229:\n",
      "  batch 100 loss: 0.0007743849475809839\n",
      "  batch 200 loss: 0.0008820957160787657\n",
      "  batch 300 loss: 0.0008281156892189757\n",
      "LOSS train 0.0008281156892189757 valid 0.08055375940225358\n",
      "valid accuracy 98.01\n",
      "test accuracy 98.11\n",
      "EPOCH 230:\n",
      "  batch 100 loss: 0.0008122249637381174\n",
      "  batch 200 loss: 0.0009050748316803948\n",
      "  batch 300 loss: 0.0008004964086285326\n",
      "LOSS train 0.0008004964086285326 valid 0.08054151555782632\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.1\n",
      "EPOCH 231:\n",
      "  batch 100 loss: 0.0008106012808275409\n",
      "  batch 200 loss: 0.0007958558130485472\n",
      "  batch 300 loss: 0.0008642277382023167\n",
      "LOSS train 0.0008642277382023167 valid 0.08054770889420863\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.11\n",
      "EPOCH 232:\n",
      "  batch 100 loss: 0.0008444848671206273\n",
      "  batch 200 loss: 0.0008510979256243445\n",
      "  batch 300 loss: 0.0008361850607616361\n",
      "LOSS train 0.0008361850607616361 valid 0.08061619404750535\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.1\n",
      "EPOCH 233:\n",
      "  batch 100 loss: 0.0008657429946470074\n",
      "  batch 200 loss: 0.000841265198396286\n",
      "  batch 300 loss: 0.000792271412210539\n",
      "LOSS train 0.000792271412210539 valid 0.0806047558324718\n",
      "valid accuracy 98.01\n",
      "test accuracy 98.1\n",
      "EPOCH 234:\n",
      "  batch 100 loss: 0.0008096886196290143\n",
      "  batch 200 loss: 0.0007855493396345992\n",
      "  batch 300 loss: 0.0008252564800204709\n",
      "LOSS train 0.0008252564800204709 valid 0.08060122087385647\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.1\n",
      "EPOCH 235:\n",
      "  batch 100 loss: 0.0008250583833432757\n",
      "  batch 200 loss: 0.0008625051309354603\n",
      "  batch 300 loss: 0.0007837700734671671\n",
      "LOSS train 0.0007837700734671671 valid 0.080602994170983\n",
      "valid accuracy 98.0\n",
      "test accuracy 98.11\n",
      "EPOCH 236:\n",
      "  batch 100 loss: 0.000763069304812234\n",
      "  batch 200 loss: 0.0008741202016244642\n",
      "  batch 300 loss: 0.0008756075138808228\n",
      "LOSS train 0.0008756075138808228 valid 0.08059806961424743\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.1\n",
      "EPOCH 237:\n",
      "  batch 100 loss: 0.0008296809253806714\n",
      "  batch 200 loss: 0.0007863417698536068\n",
      "  batch 300 loss: 0.0008373442351876292\n",
      "LOSS train 0.0008373442351876292 valid 0.08060024423477581\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.1\n",
      "EPOCH 238:\n",
      "  batch 100 loss: 0.0007719366125820671\n",
      "  batch 200 loss: 0.0008171527518425137\n",
      "  batch 300 loss: 0.0008458648991654627\n",
      "LOSS train 0.0008458648991654627 valid 0.08066413796777967\n",
      "valid accuracy 98.01\n",
      "test accuracy 98.1\n",
      "EPOCH 239:\n",
      "  batch 100 loss: 0.0008284861933498177\n",
      "  batch 200 loss: 0.0008287874533561989\n",
      "  batch 300 loss: 0.000819024468655698\n",
      "LOSS train 0.000819024468655698 valid 0.08064087770313402\n",
      "valid accuracy 98.01\n",
      "test accuracy 98.1\n",
      "EPOCH 240:\n",
      "  batch 100 loss: 0.0008072401178651489\n",
      "  batch 200 loss: 0.0008004149960470386\n",
      "  batch 300 loss: 0.000843965161475353\n",
      "LOSS train 0.000843965161475353 valid 0.08063697244358968\n",
      "valid accuracy 98.01\n",
      "test accuracy 98.1\n",
      "EPOCH 241:\n",
      "  batch 100 loss: 0.0007927558559458703\n",
      "  batch 200 loss: 0.0007773188521969132\n",
      "  batch 300 loss: 0.0008619473768339958\n",
      "LOSS train 0.0008619473768339958 valid 0.08062171125548738\n",
      "valid accuracy 98.0\n",
      "test accuracy 98.11\n",
      "EPOCH 242:\n",
      "  batch 100 loss: 0.0007847550473525189\n",
      "  batch 200 loss: 0.000795245229091961\n",
      "  batch 300 loss: 0.0008389758033445105\n",
      "LOSS train 0.0008389758033445105 valid 0.08065903520423777\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.1\n",
      "EPOCH 243:\n",
      "  batch 100 loss: 0.0008348447037860752\n",
      "  batch 200 loss: 0.000829238793667173\n",
      "  batch 300 loss: 0.0007832808948296587\n",
      "LOSS train 0.0007832808948296587 valid 0.08067433996408989\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.1\n",
      "EPOCH 244:\n",
      "  batch 100 loss: 0.0008291846548672766\n",
      "  batch 200 loss: 0.0008348855855001602\n",
      "  batch 300 loss: 0.0007987906971538905\n",
      "LOSS train 0.0007987906971538905 valid 0.08068320441024401\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.1\n",
      "EPOCH 245:\n",
      "  batch 100 loss: 0.0008228700778272469\n",
      "  batch 200 loss: 0.0007993858064583037\n",
      "  batch 300 loss: 0.0008905704725475516\n",
      "LOSS train 0.0008905704725475516 valid 0.08066469977338668\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.1\n",
      "EPOCH 246:\n",
      "  batch 100 loss: 0.0007702671537117567\n",
      "  batch 200 loss: 0.000810223149965168\n",
      "  batch 300 loss: 0.0008390325582877267\n",
      "LOSS train 0.0008390325582877267 valid 0.08067636410528913\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.1\n",
      "EPOCH 247:\n",
      "  batch 100 loss: 0.000763087955856463\n",
      "  batch 200 loss: 0.0008417378606100101\n",
      "  batch 300 loss: 0.0008083033755247015\n",
      "LOSS train 0.0008083033755247015 valid 0.08069750602368879\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.1\n",
      "EPOCH 248:\n",
      "  batch 100 loss: 0.0008319096456398256\n",
      "  batch 200 loss: 0.0008280479445238598\n",
      "  batch 300 loss: 0.0007958528664312325\n",
      "LOSS train 0.0007958528664312325 valid 0.08064684863025441\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.11\n",
      "EPOCH 249:\n",
      "  batch 100 loss: 0.0008174830186180771\n",
      "  batch 200 loss: 0.0007963995318277739\n",
      "  batch 300 loss: 0.0007805210805236129\n",
      "LOSS train 0.0007805210805236129 valid 0.0807021013048442\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.1\n",
      "EPOCH 250:\n",
      "  batch 100 loss: 0.0007987078989390284\n",
      "  batch 200 loss: 0.0007835872560099233\n",
      "  batch 300 loss: 0.0008283553321962244\n",
      "LOSS train 0.0008283553321962244 valid 0.08070965159000663\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.1\n",
      "EPOCH 251:\n",
      "  batch 100 loss: 0.0008203120197867974\n",
      "  batch 200 loss: 0.0008116129066183931\n",
      "  batch 300 loss: 0.0007768708042567595\n",
      "LOSS train 0.0007768708042567595 valid 0.0807172535576775\n",
      "valid accuracy 98.01\n",
      "test accuracy 98.1\n",
      "EPOCH 252:\n",
      "  batch 100 loss: 0.0008205024684139062\n",
      "  batch 200 loss: 0.0007354779788875021\n",
      "  batch 300 loss: 0.0008127359075297136\n",
      "LOSS train 0.0008127359075297136 valid 0.08070551797370368\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.1\n",
      "EPOCH 253:\n",
      "  batch 100 loss: 0.000803728538157884\n",
      "  batch 200 loss: 0.000743404715321958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 300 loss: 0.0008408340413006954\n",
      "LOSS train 0.0008408340413006954 valid 0.08076246041521619\n",
      "valid accuracy 98.01\n",
      "test accuracy 98.09\n",
      "EPOCH 254:\n",
      "  batch 100 loss: 0.0008472162396355998\n",
      "  batch 200 loss: 0.0007571556243055966\n",
      "  batch 300 loss: 0.0008272530097747222\n",
      "LOSS train 0.0008272530097747222 valid 0.08073574731338627\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.09\n",
      "EPOCH 255:\n",
      "  batch 100 loss: 0.0007657409549574368\n",
      "  batch 200 loss: 0.000837470545375254\n",
      "  batch 300 loss: 0.000818100746255368\n",
      "LOSS train 0.000818100746255368 valid 0.08071709922927467\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.1\n",
      "EPOCH 256:\n",
      "  batch 100 loss: 0.0008341149004991166\n",
      "  batch 200 loss: 0.0007800855854293332\n",
      "  batch 300 loss: 0.0007965466871246462\n",
      "LOSS train 0.0007965466871246462 valid 0.08074504790143876\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.09\n",
      "EPOCH 257:\n",
      "  batch 100 loss: 0.0007594451284967363\n",
      "  batch 200 loss: 0.0008097768801962956\n",
      "  batch 300 loss: 0.0008399932325119152\n",
      "LOSS train 0.0008399932325119152 valid 0.0807689253205457\n",
      "valid accuracy 98.01\n",
      "test accuracy 98.09\n",
      "EPOCH 258:\n",
      "  batch 100 loss: 0.0007881447512772865\n",
      "  batch 200 loss: 0.0008152728682034649\n",
      "  batch 300 loss: 0.000837596665660385\n",
      "LOSS train 0.000837596665660385 valid 0.0807892063903752\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.1\n",
      "EPOCH 259:\n",
      "  batch 100 loss: 0.0007715013973938767\n",
      "  batch 200 loss: 0.0008334688885952346\n",
      "  batch 300 loss: 0.0007925304313539527\n",
      "LOSS train 0.0007925304313539527 valid 0.0807827955253328\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.1\n",
      "EPOCH 260:\n",
      "  batch 100 loss: 0.0007467349884973374\n",
      "  batch 200 loss: 0.0008140245606773533\n",
      "  batch 300 loss: 0.0008231998127303087\n",
      "LOSS train 0.0008231998127303087 valid 0.08075344371550446\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.1\n",
      "EPOCH 261:\n",
      "  batch 100 loss: 0.0007776481717883144\n",
      "  batch 200 loss: 0.000827162369096186\n",
      "  batch 300 loss: 0.0008067032412509434\n",
      "LOSS train 0.0008067032412509434 valid 0.08076613438846189\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.11\n",
      "EPOCH 262:\n",
      "  batch 100 loss: 0.0007886720304668415\n",
      "  batch 200 loss: 0.0008275041723391041\n",
      "  batch 300 loss: 0.0007906159602862318\n",
      "LOSS train 0.0007906159602862318 valid 0.08079702502611695\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.11\n",
      "EPOCH 263:\n",
      "  batch 100 loss: 0.0008714377493015491\n",
      "  batch 200 loss: 0.0007872648615739309\n",
      "  batch 300 loss: 0.0008127261878689751\n",
      "LOSS train 0.0008127261878689751 valid 0.08081071566248053\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.1\n",
      "EPOCH 264:\n",
      "  batch 100 loss: 0.000842423373251222\n",
      "  batch 200 loss: 0.0007376390666468069\n",
      "  batch 300 loss: 0.0007893954787869007\n",
      "LOSS train 0.0007893954787869007 valid 0.08079168351980139\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.1\n",
      "EPOCH 265:\n",
      "  batch 100 loss: 0.0007776767804170959\n",
      "  batch 200 loss: 0.0007772713185113389\n",
      "  batch 300 loss: 0.0007836438922095112\n",
      "LOSS train 0.0007836438922095112 valid 0.08080160260035456\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.1\n",
      "EPOCH 266:\n",
      "  batch 100 loss: 0.0007993492379318923\n",
      "  batch 200 loss: 0.0008252384608204011\n",
      "  batch 300 loss: 0.0008075166193884797\n",
      "LOSS train 0.0008075166193884797 valid 0.08080641104238509\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.1\n",
      "EPOCH 267:\n",
      "  batch 100 loss: 0.0007770139703643508\n",
      "  batch 200 loss: 0.0008460076637857128\n",
      "  batch 300 loss: 0.0007661391465808265\n",
      "LOSS train 0.0007661391465808265 valid 0.08082720344367472\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.1\n",
      "EPOCH 268:\n",
      "  batch 100 loss: 0.0007674758177017793\n",
      "  batch 200 loss: 0.0008041799310012721\n",
      "  batch 300 loss: 0.0008390183301526121\n",
      "LOSS train 0.0008390183301526121 valid 0.08086070227967221\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.1\n",
      "EPOCH 269:\n",
      "  batch 100 loss: 0.000777425619889982\n",
      "  batch 200 loss: 0.0008272996474988759\n",
      "  batch 300 loss: 0.0007750274558202363\n",
      "LOSS train 0.0007750274558202363 valid 0.0808353605208612\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.1\n",
      "EPOCH 270:\n",
      "  batch 100 loss: 0.0007592200124054215\n",
      "  batch 200 loss: 0.0008033940329914913\n",
      "  batch 300 loss: 0.0007809306963463314\n",
      "LOSS train 0.0007809306963463314 valid 0.0808760946545797\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.1\n",
      "EPOCH 271:\n",
      "  batch 100 loss: 0.0007773414149414748\n",
      "  batch 200 loss: 0.000807444862293778\n",
      "  batch 300 loss: 0.000766308255988406\n",
      "LOSS train 0.000766308255988406 valid 0.08088511821542736\n",
      "valid accuracy 98.0\n",
      "test accuracy 98.09\n",
      "EPOCH 272:\n",
      "  batch 100 loss: 0.0007856002722110134\n",
      "  batch 200 loss: 0.0007618574408115819\n",
      "  batch 300 loss: 0.0007927959627704695\n",
      "LOSS train 0.0007927959627704695 valid 0.08090502693306041\n",
      "valid accuracy 98.03\n",
      "test accuracy 98.1\n",
      "EPOCH 273:\n",
      "  batch 100 loss: 0.0007550206950691063\n",
      "  batch 200 loss: 0.0007613811064220499\n",
      "  batch 300 loss: 0.0007583705142315011\n",
      "LOSS train 0.0007583705142315011 valid 0.080912187666031\n",
      "valid accuracy 98.0\n",
      "test accuracy 98.09\n",
      "EPOCH 274:\n",
      "  batch 100 loss: 0.0008333832725475076\n",
      "  batch 200 loss: 0.0007809659726626706\n",
      "  batch 300 loss: 0.0007602686024620198\n",
      "LOSS train 0.0007602686024620198 valid 0.08090325074765502\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.1\n",
      "EPOCH 275:\n",
      "  batch 100 loss: 0.0007969676281936699\n",
      "  batch 200 loss: 0.0007661149300110992\n",
      "  batch 300 loss: 0.0008320825794362463\n",
      "LOSS train 0.0008320825794362463 valid 0.08089988860242729\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.1\n",
      "EPOCH 276:\n",
      "  batch 100 loss: 0.00078870630270103\n",
      "  batch 200 loss: 0.000814392706961371\n",
      "  batch 300 loss: 0.000788655185024254\n",
      "LOSS train 0.000788655185024254 valid 0.0809062938736398\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.1\n",
      "EPOCH 277:\n",
      "  batch 100 loss: 0.0008406499425473158\n",
      "  batch 200 loss: 0.000708629238215508\n",
      "  batch 300 loss: 0.0007628656842280179\n",
      "LOSS train 0.0007628656842280179 valid 0.08088825545804221\n",
      "valid accuracy 98.03\n",
      "test accuracy 98.1\n",
      "EPOCH 278:\n",
      "  batch 100 loss: 0.0007543787571194116\n",
      "  batch 200 loss: 0.0007751801636186428\n",
      "  batch 300 loss: 0.000800938505853992\n",
      "LOSS train 0.000800938505853992 valid 0.08091794042199661\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.1\n",
      "EPOCH 279:\n",
      "  batch 100 loss: 0.0008157429171842523\n",
      "  batch 200 loss: 0.0007817836766480468\n",
      "  batch 300 loss: 0.0007521698405616917\n",
      "LOSS train 0.0007521698405616917 valid 0.08093172437923996\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.1\n",
      "EPOCH 280:\n",
      "  batch 100 loss: 0.0008362374769058079\n",
      "  batch 200 loss: 0.0007927016157191247\n",
      "  batch 300 loss: 0.0007535192467912566\n",
      "LOSS train 0.0007535192467912566 valid 0.0809665560616251\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.09\n",
      "EPOCH 281:\n",
      "  batch 100 loss: 0.0008424340112833306\n",
      "  batch 200 loss: 0.0007593161158729344\n",
      "  batch 300 loss: 0.0007539193677075673\n",
      "LOSS train 0.0007539193677075673 valid 0.08093697883047257\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.1\n",
      "EPOCH 282:\n",
      "  batch 100 loss: 0.0007396705215796829\n",
      "  batch 200 loss: 0.0007652177200361621\n",
      "  batch 300 loss: 0.0008455211378168314\n",
      "LOSS train 0.0008455211378168314 valid 0.08092445933224657\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.11\n",
      "EPOCH 283:\n",
      "  batch 100 loss: 0.0007643543899757787\n",
      "  batch 200 loss: 0.0008004513719060924\n",
      "  batch 300 loss: 0.0007328456855611876\n",
      "LOSS train 0.0007328456855611876 valid 0.08094627351228949\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.1\n",
      "EPOCH 284:\n",
      "  batch 100 loss: 0.0007803037179110106\n",
      "  batch 200 loss: 0.0007875080560916103\n",
      "  batch 300 loss: 0.0007603968010516837\n",
      "LOSS train 0.0007603968010516837 valid 0.08098878855875964\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.1\n",
      "EPOCH 285:\n",
      "  batch 100 loss: 0.000774391325248871\n",
      "  batch 200 loss: 0.0007790285462397151\n",
      "  batch 300 loss: 0.0007898874234524556\n",
      "LOSS train 0.0007898874234524556 valid 0.0809779977308044\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.1\n",
      "EPOCH 286:\n",
      "  batch 100 loss: 0.0007722082806867547\n",
      "  batch 200 loss: 0.0007984999261680059\n",
      "  batch 300 loss: 0.0007622784703562502\n",
      "LOSS train 0.0007622784703562502 valid 0.08098801636856191\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.1\n",
      "EPOCH 287:\n",
      "  batch 100 loss: 0.0007501872310240287\n",
      "  batch 200 loss: 0.0007933758816216141\n",
      "  batch 300 loss: 0.0007490566422347911\n",
      "LOSS train 0.0007490566422347911 valid 0.08097608964536575\n",
      "valid accuracy 98.03\n",
      "test accuracy 98.1\n",
      "EPOCH 288:\n",
      "  batch 100 loss: 0.0007518769543094094\n",
      "  batch 200 loss: 0.000798054221086204\n",
      "  batch 300 loss: 0.0007831891825480853\n",
      "LOSS train 0.0007831891825480853 valid 0.0809810413755005\n",
      "valid accuracy 98.03\n",
      "test accuracy 98.1\n",
      "EPOCH 289:\n",
      "  batch 100 loss: 0.0008205428217479494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 200 loss: 0.0007876014954672428\n",
      "  batch 300 loss: 0.0007552296383073554\n",
      "LOSS train 0.0007552296383073554 valid 0.08094758712509766\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.1\n",
      "EPOCH 290:\n",
      "  batch 100 loss: 0.0007784091839857865\n",
      "  batch 200 loss: 0.0008158695563906804\n",
      "  batch 300 loss: 0.0007869020689395257\n",
      "LOSS train 0.0007869020689395257 valid 0.0809805798462203\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.1\n",
      "EPOCH 291:\n",
      "  batch 100 loss: 0.0007774741295725107\n",
      "  batch 200 loss: 0.0007708102982724085\n",
      "  batch 300 loss: 0.0007559924294764642\n",
      "LOSS train 0.0007559924294764642 valid 0.0810097394145648\n",
      "valid accuracy 98.03\n",
      "test accuracy 98.1\n",
      "EPOCH 292:\n",
      "  batch 100 loss: 0.0007922089302155655\n",
      "  batch 200 loss: 0.0007478658205945976\n",
      "  batch 300 loss: 0.0008009267543093301\n",
      "LOSS train 0.0008009267543093301 valid 0.08104468252556989\n",
      "valid accuracy 98.03\n",
      "test accuracy 98.1\n",
      "EPOCH 293:\n",
      "  batch 100 loss: 0.0007694958095089533\n",
      "  batch 200 loss: 0.0007793240461614914\n",
      "  batch 300 loss: 0.0007465921960829291\n",
      "LOSS train 0.0007465921960829291 valid 0.08102478421163521\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.1\n",
      "EPOCH 294:\n",
      "  batch 100 loss: 0.0007714712018787396\n",
      "  batch 200 loss: 0.0007973390437837224\n",
      "  batch 300 loss: 0.0007346856797812506\n",
      "LOSS train 0.0007346856797812506 valid 0.08105379395493412\n",
      "valid accuracy 98.01\n",
      "test accuracy 98.1\n",
      "EPOCH 295:\n",
      "  batch 100 loss: 0.0007760402487474494\n",
      "  batch 200 loss: 0.0007323518657358363\n",
      "  batch 300 loss: 0.000746441756200511\n",
      "LOSS train 0.000746441756200511 valid 0.08104117003658527\n",
      "valid accuracy 98.01\n",
      "test accuracy 98.1\n",
      "EPOCH 296:\n",
      "  batch 100 loss: 0.000799552010721527\n",
      "  batch 200 loss: 0.0007535210547212046\n",
      "  batch 300 loss: 0.0007646898405801039\n",
      "LOSS train 0.0007646898405801039 valid 0.08107068707364835\n",
      "valid accuracy 98.01\n",
      "test accuracy 98.1\n",
      "EPOCH 297:\n",
      "  batch 100 loss: 0.0007658551324857399\n",
      "  batch 200 loss: 0.0007730045693460852\n",
      "  batch 300 loss: 0.0007404104281158652\n",
      "LOSS train 0.0007404104281158652 valid 0.08108268176570912\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.09\n",
      "EPOCH 298:\n",
      "  batch 100 loss: 0.0007612052440526896\n",
      "  batch 200 loss: 0.0007546996528981254\n",
      "  batch 300 loss: 0.0007536204808275216\n",
      "LOSS train 0.0007536204808275216 valid 0.08106138086229374\n",
      "valid accuracy 98.03\n",
      "test accuracy 98.1\n",
      "EPOCH 299:\n",
      "  batch 100 loss: 0.0007242675602901727\n",
      "  batch 200 loss: 0.0008326679866877385\n",
      "  batch 300 loss: 0.0007682038047641982\n",
      "LOSS train 0.0007682038047641982 valid 0.0810531259785536\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.1\n",
      "EPOCH 300:\n",
      "  batch 100 loss: 0.0007414565479848534\n",
      "  batch 200 loss: 0.0007501820596371544\n",
      "  batch 300 loss: 0.0007828272017650306\n",
      "LOSS train 0.0007828272017650306 valid 0.08108331111578058\n",
      "valid accuracy 98.02\n",
      "test accuracy 98.1\n"
     ]
    }
   ],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/MNIST_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "val_loss = []\n",
    "val_accuracy = []\n",
    "\n",
    "test_loss = []\n",
    "test_accuracy = []\n",
    "\n",
    "EPOCHS = 300\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "num_vbatches = int(len(validation_set) / batch_size) + 1\n",
    "vdenom = 2 ** num_vbatches - 1\n",
    "\n",
    "num_tbatches = int(len(test_data) / batch_size) + 1\n",
    "tdenom = 2 ** num_tbatches - 1\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.3)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number, writer)\n",
    "\n",
    "    # We don't need gradients on to do reporting\n",
    "    model.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    correct = 0\n",
    "    for i, vdata in enumerate(validation_loader):\n",
    "        vinputs, vlabels = vdata\n",
    "        vinputs = vinputs.view(-1, 784)\n",
    "        voutputs = model(vinputs)\n",
    "        _, predicted = torch.max(voutputs, 1)\n",
    "        correct += torch.sum(vlabels == predicted)\n",
    "        vloss = ce_loss(voutputs, vlabels)\n",
    "        running_vloss += vloss.item()\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    val_loss.append(avg_vloss)\n",
    "    val_accuracy.append(100 * float(correct)/ len(validation_set))\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "    print('valid accuracy {}'.format(val_accuracy[-1]))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = './models/vanilla-SGD/model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    # Update the learning rate\n",
    "    scheduler.step()\n",
    "    \n",
    "    running_tloss = 0.0\n",
    "    correct = 0\n",
    "\n",
    "    for i, tdata in enumerate(test_loader):\n",
    "        tinputs, tlabels = tdata\n",
    "        tinputs = tinputs.view(-1, 784)\n",
    "        toutputs = model(tinputs)\n",
    "        _, predicted = torch.max(toutputs, 1)\n",
    "        correct += torch.sum(tlabels == predicted)\n",
    "        tloss = ce_loss(voutputs, vlabels)\n",
    "        running_tloss += tloss.item()\n",
    "\n",
    "    avg_tloss = running_tloss / (i + 1)\n",
    "\n",
    "    test_loss.append(avg_tloss)\n",
    "    test_accuracy.append(100 * float(correct)/ len(test_data))\n",
    "    print('test accuracy {}'.format(test_accuracy[-1]))\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "data['test-loss'] = test_loss\n",
    "data['test-accuracy'] = test_accuracy\n",
    "data['validation-loss'] = val_loss\n",
    "data['validation-accuracy'] = val_accuracy\n",
    "\n",
    "with open('./models/vanilla-SGD/test-loss', 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "miTxSzr8jk1R"
   },
   "source": [
    "# Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uaDECpUpjmuw",
    "outputId": "805c8ddf-4252-4a31-83af-acdc31735237"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS test 0.12945786118507385\n",
      "Number of correct predictions 9810\n",
      "Accuracy: 98.10\n"
     ]
    }
   ],
   "source": [
    "model.train(False)\n",
    "\n",
    "running_tloss = 0.0\n",
    "correct = 0\n",
    "\n",
    "for i, tdata in enumerate(test_loader):\n",
    "    tinputs, tlabels = tdata\n",
    "    tinputs = tinputs.view(-1, 784)\n",
    "    toutputs = model(tinputs)\n",
    "    _, predicted = torch.max(toutputs, 1)\n",
    "    correct += torch.sum(tlabels == predicted)\n",
    "    tloss = ce_loss(voutputs, vlabels)\n",
    "    running_tloss += tloss.item()\n",
    "\n",
    "avg_tloss = running_tloss / (i + 1)\n",
    "print('LOSS test {}'.format(avg_tloss))\n",
    "print(\"Number of correct predictions {}\".format(correct))\n",
    "\n",
    "print(\"Accuracy: {:.2f}\".format(100 * float(correct)/ len(test_data)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './models/vanilla-SGD/model_{}_{}'.format(timestamp, epoch_number)\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "OHh7JGU9K_ZE"
   },
   "outputs": [],
   "source": [
    "# for step in range(3000):\n",
    "#     pre = model(x)\n",
    "#     ce = ce_loss(pre, y)\n",
    "#     kl = kl_loss(model)\n",
    "#     cost = ce + kl_weight*kl\n",
    "    \n",
    "#     optimizer.zero_grad()\n",
    "#     cost.backward()\n",
    "#     optimizer.step()\n",
    "    \n",
    "# _, predicted = torch.max(pre.data, 1)\n",
    "# total = y.size(0)\n",
    "# correct = (predicted == y).sum()\n",
    "# print('- Accuracy: %f %%' % (100 * float(correct) / total))\n",
    "# print('- CE : %2.2f, KL : %2.2f' % (ce.item(), kl.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HUKaRHNaLWPq"
   },
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "cA6ecKiZLBwG"
   },
   "outputs": [],
   "source": [
    "# def draw_plot(predicted) :\n",
    "#     fig = plt.figure(figsize = (16, 5))\n",
    "\n",
    "#     ax1 = fig.add_subplot(1, 2, 1)\n",
    "#     ax2 = fig.add_subplot(1, 2, 2)\n",
    "\n",
    "#     z1_plot = ax1.scatter(X[:, 0], X[:, 1], c = Y)\n",
    "#     z2_plot = ax2.scatter(X[:, 0], X[:, 1], c = predicted)\n",
    "\n",
    "#     plt.colorbar(z1_plot,ax=ax1)\n",
    "#     plt.colorbar(z2_plot,ax=ax2)\n",
    "\n",
    "#     ax1.set_title(\"REAL\")\n",
    "#     ax2.set_title(\"PREDICT\")\n",
    "\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "JDz-mKj0LGZz"
   },
   "outputs": [],
   "source": [
    "# pre = model(x)\n",
    "# _, predicted = torch.max(pre.data, 1)\n",
    "# draw_plot(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "gUnK7u3ZLJBy"
   },
   "outputs": [],
   "source": [
    "# Bayesian Neural Network will return different outputs even if inputs are same.\n",
    "# In other words, different plots will be shown every time forward method is called.\n",
    "# pre = model(x)\n",
    "# _, predicted = torch.max(pre.data, 1)\n",
    "# draw_plot(predicted)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
