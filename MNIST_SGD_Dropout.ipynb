{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "id": "4x1hZB-iJ47m"
   },
   "source": [
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hoNzE_BiKPlS"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "JMuxrQaRLhJR"
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "input_dim = 784\n",
    "\n",
    "# Hyperparameters\n",
    "lr = 1.0\n",
    "hidden_dim = 1200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "3LmpHydwKSPW"
   },
   "outputs": [],
   "source": [
    "# iris = datasets.load_iris()\n",
    "training_data = datasets.MNIST(root='data', train=True, download=True, transform=ToTensor())\n",
    "training_set, validation_set = torch.utils.data.random_split(training_data, [50000, 10000])\n",
    "test_data = datasets.MNIST(root='data', train=False, download=True, transform=ToTensor())\n",
    "\n",
    "training_loader = DataLoader(training_set, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_set, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "1zMGuntBNrZ4"
   },
   "outputs": [],
   "source": [
    "num_batches = int(len(training_set) / batch_size) + 1\n",
    "denom = 2 ** num_batches  - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 675
    },
    "id": "W1t4gjGjZv5m",
    "outputId": "54ea280f-1f79-4a4b-93c4-5343e8c6cfe8"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x800 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the data\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img, label = training_data[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(label)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "# No need to normalize. The data values are already between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RMu3DnfuK02O",
    "outputId": "c64ffeb7-c175-45c7-bbca-87e98506e4cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 10000\n"
     ]
    }
   ],
   "source": [
    "# print(img.shape)\n",
    "print(len(training_set),len(validation_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "2jBQ085QK3Z9"
   },
   "outputs": [],
   "source": [
    "# x, y = torch.from_numpy(X).float(), torch.from_numpy(Y).long()\n",
    "# x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "h1zQZNEiK5Hn"
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(in_features=input_dim, out_features=hidden_dim, bias=False),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(in_features=hidden_dim, out_features=hidden_dim, bias=False),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(in_features=hidden_dim, out_features=num_classes, bias=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "TKuZ_u5aK7O7"
   },
   "outputs": [],
   "source": [
    "ce_loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "hTdPC-GWedSM"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(training_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "\n",
    "        inputs = inputs.view(-1, 784)\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        ################## WE CAN AVERAGE ACROSS MULTIPLE W SAMPLES HERE BEFORE DOING loss.backward() #########################\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = ce_loss(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            last_loss = running_loss / 100 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(training_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vKLhziHlfCyw",
    "outputId": "933638f9-fb78-4aca-ccd4-d188f0623a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 100 loss: 1.083117382824421\n",
      "  batch 200 loss: 0.4206805904209614\n",
      "  batch 300 loss: 0.29018919050693515\n",
      "LOSS train 0.29018919050693515 valid 0.19412441740307626\n",
      "valid accuracy 94.11\n",
      "test accuracy 94.27\n",
      "EPOCH 2:\n",
      "  batch 100 loss: 0.21487657099962235\n",
      "  batch 200 loss: 0.20827333185821773\n",
      "  batch 300 loss: 0.18133973848074675\n",
      "LOSS train 0.18133973848074675 valid 0.12834056693164608\n",
      "valid accuracy 96.05\n",
      "test accuracy 96.37\n",
      "EPOCH 3:\n",
      "  batch 100 loss: 0.16929349567741156\n",
      "  batch 200 loss: 0.15723982758820057\n",
      "  batch 300 loss: 0.1414078575000167\n",
      "LOSS train 0.1414078575000167 valid 0.10571945604832866\n",
      "valid accuracy 96.57\n",
      "test accuracy 96.75\n",
      "EPOCH 4:\n",
      "  batch 100 loss: 0.148173599652946\n",
      "  batch 200 loss: 0.1329672410339117\n",
      "  batch 300 loss: 0.12956810221076012\n",
      "LOSS train 0.12956810221076012 valid 0.08808978432435778\n",
      "valid accuracy 97.23\n",
      "test accuracy 97.34\n",
      "EPOCH 5:\n",
      "  batch 100 loss: 0.12220355555415154\n",
      "  batch 200 loss: 0.11819904915988445\n",
      "  batch 300 loss: 0.1192546957731247\n",
      "LOSS train 0.1192546957731247 valid 0.08968975182764138\n",
      "valid accuracy 97.23\n",
      "test accuracy 97.13\n",
      "EPOCH 6:\n",
      "  batch 100 loss: 0.09999111656099557\n",
      "  batch 200 loss: 0.11734031178057194\n",
      "  batch 300 loss: 0.11169525913894177\n",
      "LOSS train 0.11169525913894177 valid 0.07540969383649386\n",
      "valid accuracy 97.69\n",
      "test accuracy 97.84\n",
      "EPOCH 7:\n",
      "  batch 100 loss: 0.09700823247432709\n",
      "  batch 200 loss: 0.10246674221009017\n",
      "  batch 300 loss: 0.09195230216719211\n",
      "LOSS train 0.09195230216719211 valid 0.07239155281523738\n",
      "valid accuracy 97.93\n",
      "test accuracy 97.83\n",
      "EPOCH 8:\n",
      "  batch 100 loss: 0.08032764117233455\n",
      "  batch 200 loss: 0.10152506986632943\n",
      "  batch 300 loss: 0.09698573283851147\n",
      "LOSS train 0.09698573283851147 valid 0.07120055081971179\n",
      "valid accuracy 97.81\n",
      "test accuracy 97.9\n",
      "EPOCH 9:\n",
      "  batch 100 loss: 0.0848085440741852\n",
      "  batch 200 loss: 0.08095987682230771\n",
      "  batch 300 loss: 0.08000023731961846\n",
      "LOSS train 0.08000023731961846 valid 0.0722476038963923\n",
      "valid accuracy 97.65\n",
      "test accuracy 97.7\n",
      "EPOCH 10:\n",
      "  batch 100 loss: 0.08063301384449005\n",
      "  batch 200 loss: 0.07716137636452913\n",
      "  batch 300 loss: 0.08449750194326043\n",
      "LOSS train 0.08449750194326043 valid 0.09167324916217948\n",
      "valid accuracy 97.23\n",
      "test accuracy 97.07\n",
      "EPOCH 11:\n",
      "  batch 100 loss: 0.07482321549672634\n",
      "  batch 200 loss: 0.06355048089288175\n",
      "  batch 300 loss: 0.07209202100522816\n",
      "LOSS train 0.07209202100522816 valid 0.0729437974941787\n",
      "valid accuracy 97.95\n",
      "test accuracy 97.98\n",
      "EPOCH 12:\n",
      "  batch 100 loss: 0.06788484807824716\n",
      "  batch 200 loss: 0.06905457830987871\n",
      "  batch 300 loss: 0.07437323406338692\n",
      "LOSS train 0.07437323406338692 valid 0.06762456683931235\n",
      "valid accuracy 97.99\n",
      "test accuracy 98.03\n",
      "EPOCH 13:\n",
      "  batch 100 loss: 0.06619166798423975\n",
      "  batch 200 loss: 0.06754143198952078\n",
      "  batch 300 loss: 0.0703897493518889\n",
      "LOSS train 0.0703897493518889 valid 0.07566885419095619\n",
      "valid accuracy 97.8\n",
      "test accuracy 97.8\n",
      "EPOCH 14:\n",
      "  batch 100 loss: 0.06106941914651543\n",
      "  batch 200 loss: 0.07423954157158733\n",
      "  batch 300 loss: 0.062357493015006184\n",
      "LOSS train 0.062357493015006184 valid 0.06242907249617031\n",
      "valid accuracy 98.12\n",
      "test accuracy 97.92\n",
      "EPOCH 15:\n",
      "  batch 100 loss: 0.06031845092307776\n",
      "  batch 200 loss: 0.06329722581896931\n",
      "  batch 300 loss: 0.0638298705080524\n",
      "LOSS train 0.0638298705080524 valid 0.0694690347543057\n",
      "valid accuracy 98.06\n",
      "test accuracy 98.09\n",
      "EPOCH 16:\n",
      "  batch 100 loss: 0.0604219179879874\n",
      "  batch 200 loss: 0.061479015364311634\n",
      "  batch 300 loss: 0.05719931811327115\n",
      "LOSS train 0.05719931811327115 valid 0.06355677034611566\n",
      "valid accuracy 98.24\n",
      "test accuracy 98.14\n",
      "EPOCH 17:\n",
      "  batch 100 loss: 0.049775899248197676\n",
      "  batch 200 loss: 0.05940535950474441\n",
      "  batch 300 loss: 0.05629428823012859\n",
      "LOSS train 0.05629428823012859 valid 0.06013078498359345\n",
      "valid accuracy 98.24\n",
      "test accuracy 98.25\n",
      "EPOCH 18:\n",
      "  batch 100 loss: 0.05332996237790212\n",
      "  batch 200 loss: 0.058526399792172015\n",
      "  batch 300 loss: 0.06012785663828254\n",
      "LOSS train 0.06012785663828254 valid 0.06306741434986456\n",
      "valid accuracy 98.11\n",
      "test accuracy 98.27\n",
      "EPOCH 19:\n",
      "  batch 100 loss: 0.05241032329387963\n",
      "  batch 200 loss: 0.05059674090938643\n",
      "  batch 300 loss: 0.05471914391499013\n",
      "LOSS train 0.05471914391499013 valid 0.07131482225760252\n",
      "valid accuracy 98.07\n",
      "test accuracy 98.04\n",
      "EPOCH 20:\n",
      "  batch 100 loss: 0.05312638936564326\n",
      "  batch 200 loss: 0.05133964088745415\n",
      "  batch 300 loss: 0.054922202681191266\n",
      "LOSS train 0.054922202681191266 valid 0.07513877733528049\n",
      "valid accuracy 97.94\n",
      "test accuracy 98.07\n",
      "EPOCH 21:\n",
      "  batch 100 loss: 0.05073812100104988\n",
      "  batch 200 loss: 0.05429991399636492\n",
      "  batch 300 loss: 0.05998327687848359\n",
      "LOSS train 0.05998327687848359 valid 0.06655047664527863\n",
      "valid accuracy 98.25\n",
      "test accuracy 98.13\n",
      "EPOCH 22:\n",
      "  batch 100 loss: 0.049692019852809605\n",
      "  batch 200 loss: 0.059093310846947134\n",
      "  batch 300 loss: 0.040823093742365014\n",
      "LOSS train 0.040823093742365014 valid 0.06514073532410673\n",
      "valid accuracy 98.07\n",
      "test accuracy 98.15\n",
      "EPOCH 23:\n",
      "  batch 100 loss: 0.049507634649053216\n",
      "  batch 200 loss: 0.04904815485700965\n",
      "  batch 300 loss: 0.04727461621630937\n",
      "LOSS train 0.04727461621630937 valid 0.08178594243665567\n",
      "valid accuracy 97.84\n",
      "test accuracy 97.91\n",
      "EPOCH 24:\n",
      "  batch 100 loss: 0.04654201140627265\n",
      "  batch 200 loss: 0.048959067284595224\n",
      "  batch 300 loss: 0.05090514112729579\n",
      "LOSS train 0.05090514112729579 valid 0.07859378433020055\n",
      "valid accuracy 97.82\n",
      "test accuracy 97.73\n",
      "EPOCH 25:\n",
      "  batch 100 loss: 0.04457653423771262\n",
      "  batch 200 loss: 0.044223677852423864\n",
      "  batch 300 loss: 0.0542392649827525\n",
      "LOSS train 0.0542392649827525 valid 0.06323495724229614\n",
      "valid accuracy 98.3\n",
      "test accuracy 98.46\n",
      "EPOCH 26:\n",
      "  batch 100 loss: 0.04146179715520702\n",
      "  batch 200 loss: 0.045523320361971854\n",
      "  batch 300 loss: 0.042644917140714826\n",
      "LOSS train 0.042644917140714826 valid 0.07913390319511483\n",
      "valid accuracy 97.91\n",
      "test accuracy 98.1\n",
      "EPOCH 27:\n",
      "  batch 100 loss: 0.03763563509157393\n",
      "  batch 200 loss: 0.035540251255733894\n",
      "  batch 300 loss: 0.04287597642512992\n",
      "LOSS train 0.04287597642512992 valid 0.06904615528433572\n",
      "valid accuracy 98.12\n",
      "test accuracy 98.39\n",
      "EPOCH 28:\n",
      "  batch 100 loss: 0.04135292174993083\n",
      "  batch 200 loss: 0.03727824864210561\n",
      "  batch 300 loss: 0.041218045844871085\n",
      "LOSS train 0.041218045844871085 valid 0.070480528900612\n",
      "valid accuracy 98.04\n",
      "test accuracy 98.16\n",
      "EPOCH 29:\n",
      "  batch 100 loss: 0.03380939879571088\n",
      "  batch 200 loss: 0.03852108184248209\n",
      "  batch 300 loss: 0.04689200576394796\n",
      "LOSS train 0.04689200576394796 valid 0.06368237865287103\n",
      "valid accuracy 98.3\n",
      "test accuracy 98.21\n",
      "EPOCH 30:\n",
      "  batch 100 loss: 0.034687632344430314\n",
      "  batch 200 loss: 0.03914214517688379\n",
      "  batch 300 loss: 0.039852126170881096\n",
      "LOSS train 0.039852126170881096 valid 0.0570227236640227\n",
      "valid accuracy 98.3\n",
      "test accuracy 98.36\n",
      "EPOCH 31:\n",
      "  batch 100 loss: 0.0377311465513776\n",
      "  batch 200 loss: 0.03428051494178362\n",
      "  batch 300 loss: 0.03959372198674828\n",
      "LOSS train 0.03959372198674828 valid 0.06402176313538434\n",
      "valid accuracy 98.36\n",
      "test accuracy 98.28\n",
      "EPOCH 32:\n",
      "  batch 100 loss: 0.04180623895023018\n",
      "  batch 200 loss: 0.044014936932362615\n",
      "  batch 300 loss: 0.045575919710099694\n",
      "LOSS train 0.045575919710099694 valid 0.06870366777352328\n",
      "valid accuracy 98.32\n",
      "test accuracy 98.42\n",
      "EPOCH 33:\n",
      "  batch 100 loss: 0.03632235163007863\n",
      "  batch 200 loss: 0.036534853610501156\n",
      "  batch 300 loss: 0.04109817703429144\n",
      "LOSS train 0.04109817703429144 valid 0.07067963153218167\n",
      "valid accuracy 98.23\n",
      "test accuracy 98.15\n",
      "EPOCH 34:\n",
      "  batch 100 loss: 0.04810538988851477\n",
      "  batch 200 loss: 0.04049936999799684\n",
      "  batch 300 loss: 0.035247934276703745\n",
      "LOSS train 0.035247934276703745 valid 0.06437961302721236\n",
      "valid accuracy 98.37\n",
      "test accuracy 98.38\n",
      "EPOCH 35:\n",
      "  batch 100 loss: 0.03284438405535184\n",
      "  batch 200 loss: 0.03719704365008511\n",
      "  batch 300 loss: 0.045650704386644064\n",
      "LOSS train 0.045650704386644064 valid 0.059156022513509265\n",
      "valid accuracy 98.41\n",
      "test accuracy 98.26\n",
      "EPOCH 36:\n",
      "  batch 100 loss: 0.03181955017556902\n",
      "  batch 200 loss: 0.03195407539402367\n",
      "  batch 300 loss: 0.030800600461661817\n",
      "LOSS train 0.030800600461661817 valid 0.06664477726533019\n",
      "valid accuracy 98.33\n",
      "test accuracy 98.22\n",
      "EPOCH 37:\n",
      "  batch 100 loss: 0.033591488111997025\n",
      "  batch 200 loss: 0.038122581596835514\n",
      "  batch 300 loss: 0.03328165641345549\n",
      "LOSS train 0.03328165641345549 valid 0.06994893087687183\n",
      "valid accuracy 98.26\n",
      "test accuracy 98.17\n",
      "EPOCH 38:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 100 loss: 0.029328753921436145\n",
      "  batch 200 loss: 0.028498774794861674\n",
      "  batch 300 loss: 0.03709884520270862\n",
      "LOSS train 0.03709884520270862 valid 0.062620608533933\n",
      "valid accuracy 98.46\n",
      "test accuracy 98.46\n",
      "EPOCH 39:\n",
      "  batch 100 loss: 0.03394110966299195\n",
      "  batch 200 loss: 0.028708453679573722\n",
      "  batch 300 loss: 0.030277160443365574\n",
      "LOSS train 0.030277160443365574 valid 0.07098924722876164\n",
      "valid accuracy 98.27\n",
      "test accuracy 98.14\n",
      "EPOCH 40:\n",
      "  batch 100 loss: 0.02796826745267026\n",
      "  batch 200 loss: 0.03522970259538852\n",
      "  batch 300 loss: 0.036509328905958684\n",
      "LOSS train 0.036509328905958684 valid 0.07410535240001369\n",
      "valid accuracy 98.21\n",
      "test accuracy 98.32\n",
      "EPOCH 41:\n",
      "  batch 100 loss: 0.036829827029141594\n",
      "  batch 200 loss: 0.03783220371464267\n",
      "  batch 300 loss: 0.03751438186969608\n",
      "LOSS train 0.03751438186969608 valid 0.07501611498455647\n",
      "valid accuracy 98.23\n",
      "test accuracy 98.46\n",
      "EPOCH 42:\n",
      "  batch 100 loss: 0.03334552592830733\n",
      "  batch 200 loss: 0.029827017809730023\n",
      "  batch 300 loss: 0.033691969537903786\n",
      "LOSS train 0.033691969537903786 valid 0.06892994109134461\n",
      "valid accuracy 98.35\n",
      "test accuracy 98.41\n",
      "EPOCH 43:\n",
      "  batch 100 loss: 0.03627307664239197\n",
      "  batch 200 loss: 0.032710194229730406\n",
      "  batch 300 loss: 0.03506634606397711\n",
      "LOSS train 0.03506634606397711 valid 0.08213419585689148\n",
      "valid accuracy 98.23\n",
      "test accuracy 98.05\n",
      "EPOCH 44:\n",
      "  batch 100 loss: 0.026344484608853237\n",
      "  batch 200 loss: 0.02322823120193789\n",
      "  batch 300 loss: 0.04080942752654664\n",
      "LOSS train 0.04080942752654664 valid 0.07051107393506961\n",
      "valid accuracy 98.25\n",
      "test accuracy 98.19\n",
      "EPOCH 45:\n",
      "  batch 100 loss: 0.030476895411557053\n",
      "  batch 200 loss: 0.028371252416109202\n",
      "  batch 300 loss: 0.03092816632299218\n",
      "LOSS train 0.03092816632299218 valid 0.08011531909556376\n",
      "valid accuracy 98.17\n",
      "test accuracy 98.16\n",
      "EPOCH 46:\n",
      "  batch 100 loss: 0.03276420605659951\n",
      "  batch 200 loss: 0.03371049519744702\n",
      "  batch 300 loss: 0.029871105689089746\n",
      "LOSS train 0.029871105689089746 valid 0.06862424144262\n",
      "valid accuracy 98.41\n",
      "test accuracy 98.26\n",
      "EPOCH 47:\n",
      "  batch 100 loss: 0.0281854327081237\n",
      "  batch 200 loss: 0.02387774318412994\n",
      "  batch 300 loss: 0.03345429941167822\n",
      "LOSS train 0.03345429941167822 valid 0.07065791753986467\n",
      "valid accuracy 98.25\n",
      "test accuracy 98.29\n",
      "EPOCH 48:\n",
      "  batch 100 loss: 0.028611968636978416\n",
      "  batch 200 loss: 0.03503792015493673\n",
      "  batch 300 loss: 0.03106280868873\n",
      "LOSS train 0.03106280868873 valid 0.06576762676411795\n",
      "valid accuracy 98.36\n",
      "test accuracy 98.28\n",
      "EPOCH 49:\n",
      "  batch 100 loss: 0.033641713188844735\n",
      "  batch 200 loss: 0.0313836012664251\n",
      "  batch 300 loss: 0.026880446641589516\n",
      "LOSS train 0.026880446641589516 valid 0.07105880658307313\n",
      "valid accuracy 98.28\n",
      "test accuracy 98.28\n",
      "EPOCH 50:\n",
      "  batch 100 loss: 0.022153127274068537\n",
      "  batch 200 loss: 0.028760293037630617\n",
      "  batch 300 loss: 0.030387729114445393\n",
      "LOSS train 0.030387729114445393 valid 0.06715954335618622\n",
      "valid accuracy 98.37\n",
      "test accuracy 98.25\n",
      "EPOCH 51:\n",
      "  batch 100 loss: 0.025673122883599717\n",
      "  batch 200 loss: 0.025094702914066146\n",
      "  batch 300 loss: 0.029870729624235537\n",
      "LOSS train 0.029870729624235537 valid 0.06522818138007637\n",
      "valid accuracy 98.46\n",
      "test accuracy 98.3\n",
      "EPOCH 52:\n",
      "  batch 100 loss: 0.024306141521665266\n",
      "  batch 200 loss: 0.027344489965762477\n",
      "  batch 300 loss: 0.03348860314377816\n",
      "LOSS train 0.03348860314377816 valid 0.06605604948929829\n",
      "valid accuracy 98.45\n",
      "test accuracy 98.32\n",
      "EPOCH 53:\n",
      "  batch 100 loss: 0.025139334571140353\n",
      "  batch 200 loss: 0.024006372565490892\n",
      "  batch 300 loss: 0.027031117419246586\n",
      "LOSS train 0.027031117419246586 valid 0.06811598294025546\n",
      "valid accuracy 98.44\n",
      "test accuracy 98.36\n",
      "EPOCH 54:\n",
      "  batch 100 loss: 0.02505870629887795\n",
      "  batch 200 loss: 0.03165586973831523\n",
      "  batch 300 loss: 0.02793755533231888\n",
      "LOSS train 0.02793755533231888 valid 0.0648378261359102\n",
      "valid accuracy 98.4\n",
      "test accuracy 98.3\n",
      "EPOCH 55:\n",
      "  batch 100 loss: 0.023177382784779182\n",
      "  batch 200 loss: 0.0216278194206825\n",
      "  batch 300 loss: 0.02620055623759981\n",
      "LOSS train 0.02620055623759981 valid 0.0717448888192192\n",
      "valid accuracy 98.38\n",
      "test accuracy 98.23\n",
      "EPOCH 56:\n",
      "  batch 100 loss: 0.023751501429978818\n",
      "  batch 200 loss: 0.03187312961745192\n",
      "  batch 300 loss: 0.02227535795260337\n",
      "LOSS train 0.02227535795260337 valid 0.06891617346411318\n",
      "valid accuracy 98.31\n",
      "test accuracy 98.08\n",
      "EPOCH 57:\n",
      "  batch 100 loss: 0.02766548955187318\n",
      "  batch 200 loss: 0.023124243099300657\n",
      "  batch 300 loss: 0.025966500830181757\n",
      "LOSS train 0.025966500830181757 valid 0.06322303601728327\n",
      "valid accuracy 98.45\n",
      "test accuracy 98.36\n",
      "EPOCH 58:\n",
      "  batch 100 loss: 0.020287752433432615\n",
      "  batch 200 loss: 0.025332479539938505\n",
      "  batch 300 loss: 0.029329545353539287\n",
      "LOSS train 0.029329545353539287 valid 0.060835800586781126\n",
      "valid accuracy 98.55\n",
      "test accuracy 98.31\n",
      "EPOCH 59:\n",
      "  batch 100 loss: 0.027755923691147474\n",
      "  batch 200 loss: 0.022485227252873302\n",
      "  batch 300 loss: 0.029352360070188297\n",
      "LOSS train 0.029352360070188297 valid 0.06853157307225184\n",
      "valid accuracy 98.51\n",
      "test accuracy 98.29\n",
      "EPOCH 60:\n",
      "  batch 100 loss: 0.02453465848142514\n",
      "  batch 200 loss: 0.021192451522510966\n",
      "  batch 300 loss: 0.02489975151285762\n",
      "LOSS train 0.02489975151285762 valid 0.06931400471517642\n",
      "valid accuracy 98.53\n",
      "test accuracy 98.42\n",
      "EPOCH 61:\n",
      "  batch 100 loss: 0.02279546080448199\n",
      "  batch 200 loss: 0.024884159205830658\n",
      "  batch 300 loss: 0.03520899855022435\n",
      "LOSS train 0.03520899855022435 valid 0.07786593859708762\n",
      "valid accuracy 98.42\n",
      "test accuracy 98.3\n",
      "EPOCH 62:\n",
      "  batch 100 loss: 0.021419328746851533\n",
      "  batch 200 loss: 0.021595445022467175\n",
      "  batch 300 loss: 0.02585366285762575\n",
      "LOSS train 0.02585366285762575 valid 0.07139996353509583\n",
      "valid accuracy 98.4\n",
      "test accuracy 98.34\n",
      "EPOCH 63:\n",
      "  batch 100 loss: 0.018154839128110323\n",
      "  batch 200 loss: 0.025445857544546015\n",
      "  batch 300 loss: 0.03184958427154925\n",
      "LOSS train 0.03184958427154925 valid 0.07196865103515514\n",
      "valid accuracy 98.48\n",
      "test accuracy 98.31\n",
      "EPOCH 64:\n",
      "  batch 100 loss: 0.025258942225045756\n",
      "  batch 200 loss: 0.026500127086474094\n",
      "  batch 300 loss: 0.02667576571024256\n",
      "LOSS train 0.02667576571024256 valid 0.07345177761158947\n",
      "valid accuracy 98.38\n",
      "test accuracy 98.38\n",
      "EPOCH 65:\n",
      "  batch 100 loss: 0.02628130183686153\n",
      "  batch 200 loss: 0.022798502624209505\n",
      "  batch 300 loss: 0.026796823923650664\n",
      "LOSS train 0.026796823923650664 valid 0.06198101180614432\n",
      "valid accuracy 98.71\n",
      "test accuracy 98.5\n",
      "EPOCH 66:\n",
      "  batch 100 loss: 0.024469857547956053\n",
      "  batch 200 loss: 0.020407924311293756\n",
      "  batch 300 loss: 0.023293822174250637\n",
      "LOSS train 0.023293822174250637 valid 0.0723794282409322\n",
      "valid accuracy 98.47\n",
      "test accuracy 98.34\n",
      "EPOCH 67:\n",
      "  batch 100 loss: 0.022639408054528756\n",
      "  batch 200 loss: 0.0217103303965996\n",
      "  batch 300 loss: 0.021359532153728652\n",
      "LOSS train 0.021359532153728652 valid 0.07818110293323392\n",
      "valid accuracy 98.32\n",
      "test accuracy 98.41\n",
      "EPOCH 68:\n",
      "  batch 100 loss: 0.01991394277487416\n",
      "  batch 200 loss: 0.025000286069425785\n",
      "  batch 300 loss: 0.027542680682527135\n",
      "LOSS train 0.027542680682527135 valid 0.07146855438429467\n",
      "valid accuracy 98.41\n",
      "test accuracy 98.42\n",
      "EPOCH 69:\n",
      "  batch 100 loss: 0.02276440992769494\n",
      "  batch 200 loss: 0.022060824141080956\n",
      "  batch 300 loss: 0.021278711134655168\n",
      "LOSS train 0.021278711134655168 valid 0.07955023209576177\n",
      "valid accuracy 98.18\n",
      "test accuracy 98.3\n",
      "EPOCH 70:\n",
      "  batch 100 loss: 0.01876465876292059\n",
      "  batch 200 loss: 0.020491457012976753\n",
      "  batch 300 loss: 0.020091815912746825\n",
      "LOSS train 0.020091815912746825 valid 0.07515935756435821\n",
      "valid accuracy 98.49\n",
      "test accuracy 98.39\n",
      "EPOCH 71:\n",
      "  batch 100 loss: 0.02219440363769536\n",
      "  batch 200 loss: 0.02468708360218443\n",
      "  batch 300 loss: 0.020887322040289293\n",
      "LOSS train 0.020887322040289293 valid 0.07595929787295305\n",
      "valid accuracy 98.42\n",
      "test accuracy 98.47\n",
      "EPOCH 72:\n",
      "  batch 100 loss: 0.01767109787062509\n",
      "  batch 200 loss: 0.019150770451087738\n",
      "  batch 300 loss: 0.022045546049030237\n",
      "LOSS train 0.022045546049030237 valid 0.07386180517356548\n",
      "valid accuracy 98.55\n",
      "test accuracy 98.43\n",
      "EPOCH 73:\n",
      "  batch 100 loss: 0.019678748494989122\n",
      "  batch 200 loss: 0.02349657135178859\n",
      "  batch 300 loss: 0.02125504790485138\n",
      "LOSS train 0.02125504790485138 valid 0.08055881051533809\n",
      "valid accuracy 98.39\n",
      "test accuracy 98.41\n",
      "EPOCH 74:\n",
      "  batch 100 loss: 0.022104885740336613\n",
      "  batch 200 loss: 0.020065504390495335\n",
      "  batch 300 loss: 0.021700819878824405\n",
      "LOSS train 0.021700819878824405 valid 0.07554297477391525\n",
      "valid accuracy 98.42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 98.34\n",
      "EPOCH 75:\n",
      "  batch 100 loss: 0.02003917104026186\n",
      "  batch 200 loss: 0.023090250130189817\n",
      "  batch 300 loss: 0.02303950760848238\n",
      "LOSS train 0.02303950760848238 valid 0.07469961963491913\n",
      "valid accuracy 98.49\n",
      "test accuracy 98.31\n",
      "EPOCH 76:\n",
      "  batch 100 loss: 0.01911245190705813\n",
      "  batch 200 loss: 0.021924961472468567\n",
      "  batch 300 loss: 0.01905864870022924\n",
      "LOSS train 0.01905864870022924 valid 0.07358545429488464\n",
      "valid accuracy 98.53\n",
      "test accuracy 98.47\n",
      "EPOCH 77:\n",
      "  batch 100 loss: 0.014544282587157795\n",
      "  batch 200 loss: 0.022047546976973535\n",
      "  batch 300 loss: 0.022347274386884237\n",
      "LOSS train 0.022347274386884237 valid 0.07121725977003159\n",
      "valid accuracy 98.46\n",
      "test accuracy 98.56\n",
      "EPOCH 78:\n",
      "  batch 100 loss: 0.023139860621304253\n",
      "  batch 200 loss: 0.01839955909177661\n",
      "  batch 300 loss: 0.017844020917182207\n",
      "LOSS train 0.017844020917182207 valid 0.07252942137471755\n",
      "valid accuracy 98.42\n",
      "test accuracy 98.54\n",
      "EPOCH 79:\n",
      "  batch 100 loss: 0.021638635029812576\n",
      "  batch 200 loss: 0.021547876373660985\n",
      "  batch 300 loss: 0.025031966372298483\n",
      "LOSS train 0.025031966372298483 valid 0.07080023122819776\n",
      "valid accuracy 98.46\n",
      "test accuracy 98.56\n",
      "EPOCH 80:\n",
      "  batch 100 loss: 0.018017300930077907\n",
      "  batch 200 loss: 0.012702505547058535\n",
      "  batch 300 loss: 0.0182642162234697\n",
      "LOSS train 0.0182642162234697 valid 0.06990284731396462\n",
      "valid accuracy 98.51\n",
      "test accuracy 98.55\n",
      "EPOCH 81:\n",
      "  batch 100 loss: 0.017523047421491356\n",
      "  batch 200 loss: 0.01638073274058115\n",
      "  batch 300 loss: 0.016055034164119207\n",
      "LOSS train 0.016055034164119207 valid 0.07505312454986203\n",
      "valid accuracy 98.5\n",
      "test accuracy 98.58\n",
      "EPOCH 82:\n",
      "  batch 100 loss: 0.018439992066341802\n",
      "  batch 200 loss: 0.016113375549321064\n",
      "  batch 300 loss: 0.018635887896743954\n",
      "LOSS train 0.018635887896743954 valid 0.08065738461908363\n",
      "valid accuracy 98.33\n",
      "test accuracy 98.47\n",
      "EPOCH 83:\n",
      "  batch 100 loss: 0.01969396355867502\n",
      "  batch 200 loss: 0.01607455047516851\n",
      "  batch 300 loss: 0.02069263008335838\n",
      "LOSS train 0.02069263008335838 valid 0.08045301487104956\n",
      "valid accuracy 98.42\n",
      "test accuracy 98.37\n",
      "EPOCH 84:\n",
      "  batch 100 loss: 0.021726759547309484\n",
      "  batch 200 loss: 0.02108345687520341\n",
      "  batch 300 loss: 0.021518707557115702\n",
      "LOSS train 0.021518707557115702 valid 0.07177150036102231\n",
      "valid accuracy 98.61\n",
      "test accuracy 98.42\n",
      "EPOCH 85:\n",
      "  batch 100 loss: 0.016256245414042497\n",
      "  batch 200 loss: 0.01860936595337989\n",
      "  batch 300 loss: 0.01906299627327826\n",
      "LOSS train 0.01906299627327826 valid 0.0698289955277998\n",
      "valid accuracy 98.64\n",
      "test accuracy 98.36\n",
      "EPOCH 86:\n",
      "  batch 100 loss: 0.017989883381596884\n",
      "  batch 200 loss: 0.018961883301280978\n",
      "  batch 300 loss: 0.017741331753531996\n",
      "LOSS train 0.017741331753531996 valid 0.0664832137763254\n",
      "valid accuracy 98.61\n",
      "test accuracy 98.56\n",
      "EPOCH 87:\n",
      "  batch 100 loss: 0.021263481162313837\n",
      "  batch 200 loss: 0.01390432148346008\n",
      "  batch 300 loss: 0.015737725470080476\n",
      "LOSS train 0.015737725470080476 valid 0.07075032407802506\n",
      "valid accuracy 98.46\n",
      "test accuracy 98.5\n",
      "EPOCH 88:\n",
      "  batch 100 loss: 0.015367950443323935\n",
      "  batch 200 loss: 0.01649086865703339\n",
      "  batch 300 loss: 0.024848119640009828\n",
      "LOSS train 0.024848119640009828 valid 0.07247867895204786\n",
      "valid accuracy 98.5\n",
      "test accuracy 98.27\n",
      "EPOCH 89:\n",
      "  batch 100 loss: 0.013872893559400835\n",
      "  batch 200 loss: 0.02264723921995028\n",
      "  batch 300 loss: 0.02046976823006844\n",
      "LOSS train 0.02046976823006844 valid 0.07064494819773061\n",
      "valid accuracy 98.51\n",
      "test accuracy 98.45\n",
      "EPOCH 90:\n",
      "  batch 100 loss: 0.01556749396295345\n",
      "  batch 200 loss: 0.016286687470364995\n",
      "  batch 300 loss: 0.012282458580266394\n",
      "LOSS train 0.012282458580266394 valid 0.09017954760142108\n",
      "valid accuracy 98.29\n",
      "test accuracy 98.36\n",
      "EPOCH 91:\n",
      "  batch 100 loss: 0.015713818986660043\n",
      "  batch 200 loss: 0.012899561987160269\n",
      "  batch 300 loss: 0.01213874247876447\n",
      "LOSS train 0.01213874247876447 valid 0.08205468785364027\n",
      "valid accuracy 98.31\n",
      "test accuracy 98.43\n",
      "EPOCH 92:\n",
      "  batch 100 loss: 0.016553585679621393\n",
      "  batch 200 loss: 0.015973627736966593\n",
      "  batch 300 loss: 0.020366044297625195\n",
      "LOSS train 0.020366044297625195 valid 0.08723026088234832\n",
      "valid accuracy 98.36\n",
      "test accuracy 98.45\n",
      "EPOCH 93:\n",
      "  batch 100 loss: 0.021113506966648855\n",
      "  batch 200 loss: 0.020224604766499395\n",
      "  batch 300 loss: 0.017240660746501817\n",
      "LOSS train 0.017240660746501817 valid 0.07977308871156737\n",
      "valid accuracy 98.35\n",
      "test accuracy 98.52\n",
      "EPOCH 94:\n",
      "  batch 100 loss: 0.024657060660715616\n",
      "  batch 200 loss: 0.017509309184933954\n",
      "  batch 300 loss: 0.029094688969562413\n",
      "LOSS train 0.029094688969562413 valid 0.08123212983158323\n",
      "valid accuracy 98.4\n",
      "test accuracy 98.58\n",
      "EPOCH 95:\n",
      "  batch 100 loss: 0.014123435416986468\n",
      "  batch 200 loss: 0.016111406663076196\n",
      "  batch 300 loss: 0.01840800808622589\n",
      "LOSS train 0.01840800808622589 valid 0.08535230011661549\n",
      "valid accuracy 98.53\n",
      "test accuracy 98.56\n",
      "EPOCH 96:\n",
      "  batch 100 loss: 0.014907077336247312\n",
      "  batch 200 loss: 0.016738580605942844\n",
      "  batch 300 loss: 0.022977121526782867\n",
      "LOSS train 0.022977121526782867 valid 0.08383576859000928\n",
      "valid accuracy 98.39\n",
      "test accuracy 98.46\n",
      "EPOCH 97:\n",
      "  batch 100 loss: 0.019982597264970536\n",
      "  batch 200 loss: 0.02188818659298704\n",
      "  batch 300 loss: 0.018636989440128673\n",
      "LOSS train 0.018636989440128673 valid 0.07391634908404579\n",
      "valid accuracy 98.34\n",
      "test accuracy 98.54\n",
      "EPOCH 98:\n",
      "  batch 100 loss: 0.01952559257959365\n",
      "  batch 200 loss: 0.014056815989424648\n",
      "  batch 300 loss: 0.017214730905870966\n",
      "LOSS train 0.017214730905870966 valid 0.07130659805151009\n",
      "valid accuracy 98.49\n",
      "test accuracy 98.53\n",
      "EPOCH 99:\n",
      "  batch 100 loss: 0.013616365066554862\n",
      "  batch 200 loss: 0.011856713637607754\n",
      "  batch 300 loss: 0.013865079514280296\n",
      "LOSS train 0.013865079514280296 valid 0.08239378273940963\n",
      "valid accuracy 98.59\n",
      "test accuracy 98.46\n",
      "EPOCH 100:\n",
      "  batch 100 loss: 0.013585748401292222\n",
      "  batch 200 loss: 0.01465766408532545\n",
      "  batch 300 loss: 0.01835596396913388\n",
      "LOSS train 0.01835596396913388 valid 0.07942378579302178\n",
      "valid accuracy 98.46\n",
      "test accuracy 98.34\n",
      "EPOCH 101:\n",
      "  batch 100 loss: 0.019402879522276636\n",
      "  batch 200 loss: 0.016873896433153276\n",
      "  batch 300 loss: 0.016954680130857012\n",
      "LOSS train 0.016954680130857012 valid 0.08600120484969678\n",
      "valid accuracy 98.43\n",
      "test accuracy 98.41\n",
      "EPOCH 102:\n",
      "  batch 100 loss: 0.01839269811513077\n",
      "  batch 200 loss: 0.014225941732765932\n",
      "  batch 300 loss: 0.015408537585462909\n",
      "LOSS train 0.015408537585462909 valid 0.07923330956547736\n",
      "valid accuracy 98.44\n",
      "test accuracy 98.55\n",
      "EPOCH 103:\n",
      "  batch 100 loss: 0.019551685201440704\n",
      "  batch 200 loss: 0.02133467821569866\n",
      "  batch 300 loss: 0.01194648865950512\n",
      "LOSS train 0.01194648865950512 valid 0.07788048050789945\n",
      "valid accuracy 98.56\n",
      "test accuracy 98.47\n",
      "EPOCH 104:\n",
      "  batch 100 loss: 0.01806301681055629\n",
      "  batch 200 loss: 0.017861490567920555\n",
      "  batch 300 loss: 0.016960011781711727\n",
      "LOSS train 0.016960011781711727 valid 0.08847234719594987\n",
      "valid accuracy 98.48\n",
      "test accuracy 98.49\n",
      "EPOCH 105:\n",
      "  batch 100 loss: 0.019470938135018514\n",
      "  batch 200 loss: 0.01064230547144689\n",
      "  batch 300 loss: 0.01703149543305699\n",
      "LOSS train 0.01703149543305699 valid 0.07144774046238023\n",
      "valid accuracy 98.65\n",
      "test accuracy 98.55\n",
      "EPOCH 106:\n",
      "  batch 100 loss: 0.018467415911127317\n",
      "  batch 200 loss: 0.012316624617651541\n",
      "  batch 300 loss: 0.017541492437048875\n",
      "LOSS train 0.017541492437048875 valid 0.07709689977021794\n",
      "valid accuracy 98.53\n",
      "test accuracy 98.55\n",
      "EPOCH 107:\n",
      "  batch 100 loss: 0.016023230317669005\n",
      "  batch 200 loss: 0.01664896949610693\n",
      "  batch 300 loss: 0.01882089709525644\n",
      "LOSS train 0.01882089709525644 valid 0.07944066145026664\n",
      "valid accuracy 98.5\n",
      "test accuracy 98.59\n",
      "EPOCH 108:\n",
      "  batch 100 loss: 0.012996167954470365\n",
      "  batch 200 loss: 0.01764777689996208\n",
      "  batch 300 loss: 0.016348323247948427\n",
      "LOSS train 0.016348323247948427 valid 0.07891577247467962\n",
      "valid accuracy 98.61\n",
      "test accuracy 98.49\n",
      "EPOCH 109:\n",
      "  batch 100 loss: 0.017275315188599052\n",
      "  batch 200 loss: 0.014677032272011274\n",
      "  batch 300 loss: 0.01731320256993058\n",
      "LOSS train 0.01731320256993058 valid 0.07654660350569337\n",
      "valid accuracy 98.45\n",
      "test accuracy 98.59\n",
      "EPOCH 110:\n",
      "  batch 100 loss: 0.009176253288717361\n",
      "  batch 200 loss: 0.013414574541511683\n",
      "  batch 300 loss: 0.012582366169899615\n",
      "LOSS train 0.012582366169899615 valid 0.07948839389990997\n",
      "valid accuracy 98.52\n",
      "test accuracy 98.41\n",
      "EPOCH 111:\n",
      "  batch 100 loss: 0.011890821875113033\n",
      "  batch 200 loss: 0.014552660502522486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 300 loss: 0.01600900016805099\n",
      "LOSS train 0.01600900016805099 valid 0.0694404675190601\n",
      "valid accuracy 98.67\n",
      "test accuracy 98.56\n",
      "EPOCH 112:\n",
      "  batch 100 loss: 0.016186204479909067\n",
      "  batch 200 loss: 0.012917774308316438\n",
      "  batch 300 loss: 0.014870574481019503\n",
      "LOSS train 0.014870574481019503 valid 0.07345635914311913\n",
      "valid accuracy 98.49\n",
      "test accuracy 98.54\n",
      "EPOCH 113:\n",
      "  batch 100 loss: 0.015251541463039758\n",
      "  batch 200 loss: 0.015681600571988384\n",
      "  batch 300 loss: 0.014008354702418728\n",
      "LOSS train 0.014008354702418728 valid 0.07328078302694256\n",
      "valid accuracy 98.65\n",
      "test accuracy 98.67\n",
      "EPOCH 114:\n",
      "  batch 100 loss: 0.013014754648138479\n",
      "  batch 200 loss: 0.01684218059739578\n",
      "  batch 300 loss: 0.01421483928710586\n",
      "LOSS train 0.01421483928710586 valid 0.07576480191567532\n",
      "valid accuracy 98.69\n",
      "test accuracy 98.49\n",
      "EPOCH 115:\n",
      "  batch 100 loss: 0.012098130180397675\n",
      "  batch 200 loss: 0.00826633638696876\n",
      "  batch 300 loss: 0.013180938225741557\n",
      "LOSS train 0.013180938225741557 valid 0.07329450164055329\n",
      "valid accuracy 98.6\n",
      "test accuracy 98.56\n",
      "EPOCH 116:\n",
      "  batch 100 loss: 0.01413589780429902\n",
      "  batch 200 loss: 0.013454984240161139\n",
      "  batch 300 loss: 0.018280049343957216\n",
      "LOSS train 0.018280049343957216 valid 0.08043644184160453\n",
      "valid accuracy 98.46\n",
      "test accuracy 98.59\n",
      "EPOCH 117:\n",
      "  batch 100 loss: 0.014474368358751235\n",
      "  batch 200 loss: 0.012688471615256276\n",
      "  batch 300 loss: 0.009171308348195453\n",
      "LOSS train 0.009171308348195453 valid 0.0803709904160579\n",
      "valid accuracy 98.48\n",
      "test accuracy 98.6\n",
      "EPOCH 118:\n",
      "  batch 100 loss: 0.011052050047665034\n",
      "  batch 200 loss: 0.01719400442936603\n",
      "  batch 300 loss: 0.009599958078197232\n",
      "LOSS train 0.009599958078197232 valid 0.08219773253581422\n",
      "valid accuracy 98.47\n",
      "test accuracy 98.55\n",
      "EPOCH 119:\n",
      "  batch 100 loss: 0.011184519715698116\n",
      "  batch 200 loss: 0.015233259133001411\n",
      "  batch 300 loss: 0.013627378447308729\n",
      "LOSS train 0.013627378447308729 valid 0.0811053299941152\n",
      "valid accuracy 98.56\n",
      "test accuracy 98.62\n",
      "EPOCH 120:\n",
      "  batch 100 loss: 0.012122602444478617\n",
      "  batch 200 loss: 0.01327676538916421\n",
      "  batch 300 loss: 0.013859149438576423\n",
      "LOSS train 0.013859149438576423 valid 0.07390965570756854\n",
      "valid accuracy 98.56\n",
      "test accuracy 98.56\n",
      "EPOCH 121:\n",
      "  batch 100 loss: 0.012884510498988675\n",
      "  batch 200 loss: 0.012240235537465196\n",
      "  batch 300 loss: 0.008632727668582448\n",
      "LOSS train 0.008632727668582448 valid 0.0848534495180732\n",
      "valid accuracy 98.32\n",
      "test accuracy 98.51\n",
      "EPOCH 122:\n",
      "  batch 100 loss: 0.013492435510365795\n",
      "  batch 200 loss: 0.012935923274699234\n",
      "  batch 300 loss: 0.017214460426903316\n",
      "LOSS train 0.017214460426903316 valid 0.07778382990907147\n",
      "valid accuracy 98.41\n",
      "test accuracy 98.64\n",
      "EPOCH 123:\n",
      "  batch 100 loss: 0.01499130073406377\n",
      "  batch 200 loss: 0.014192070288472677\n",
      "  batch 300 loss: 0.014769988143707452\n",
      "LOSS train 0.014769988143707452 valid 0.07758648364209346\n",
      "valid accuracy 98.47\n",
      "test accuracy 98.59\n",
      "EPOCH 124:\n",
      "  batch 100 loss: 0.017606855673802782\n",
      "  batch 200 loss: 0.010528367342953971\n",
      "  batch 300 loss: 0.008569971489432646\n",
      "LOSS train 0.008569971489432646 valid 0.08055815794445767\n",
      "valid accuracy 98.54\n",
      "test accuracy 98.62\n",
      "EPOCH 125:\n",
      "  batch 100 loss: 0.013161122357951171\n",
      "  batch 200 loss: 0.014455911529439617\n",
      "  batch 300 loss: 0.01443247789761699\n",
      "LOSS train 0.01443247789761699 valid 0.0847439402883837\n",
      "valid accuracy 98.45\n",
      "test accuracy 98.52\n",
      "EPOCH 126:\n",
      "  batch 100 loss: 0.01498488736400759\n",
      "  batch 200 loss: 0.012047979135646756\n",
      "  batch 300 loss: 0.01626734500961902\n",
      "LOSS train 0.01626734500961902 valid 0.08060493624011715\n",
      "valid accuracy 98.48\n",
      "test accuracy 98.56\n",
      "EPOCH 127:\n",
      "  batch 100 loss: 0.01977456707612873\n",
      "  batch 200 loss: 0.018993940338691572\n",
      "  batch 300 loss: 0.01737347127999783\n",
      "LOSS train 0.01737347127999783 valid 0.08357327251070285\n",
      "valid accuracy 98.51\n",
      "test accuracy 98.54\n",
      "EPOCH 128:\n",
      "  batch 100 loss: 0.008983563022820818\n",
      "  batch 200 loss: 0.010843037789718436\n",
      "  batch 300 loss: 0.01202159126706647\n",
      "LOSS train 0.01202159126706647 valid 0.09156419318301981\n",
      "valid accuracy 98.3\n",
      "test accuracy 98.51\n",
      "EPOCH 129:\n",
      "  batch 100 loss: 0.01383408779020101\n",
      "  batch 200 loss: 0.015135249507857225\n",
      "  batch 300 loss: 0.011526856968148423\n",
      "LOSS train 0.011526856968148423 valid 0.08670640847620097\n",
      "valid accuracy 98.41\n",
      "test accuracy 98.46\n",
      "EPOCH 130:\n",
      "  batch 100 loss: 0.01062472702586092\n",
      "  batch 200 loss: 0.012216754819019116\n",
      "  batch 300 loss: 0.01261390940725505\n",
      "LOSS train 0.01261390940725505 valid 0.08620079600748082\n",
      "valid accuracy 98.45\n",
      "test accuracy 98.52\n",
      "EPOCH 131:\n",
      "  batch 100 loss: 0.010139576753490474\n",
      "  batch 200 loss: 0.01318315714869641\n",
      "  batch 300 loss: 0.014675240133983606\n",
      "LOSS train 0.014675240133983606 valid 0.08148964020419723\n",
      "valid accuracy 98.53\n",
      "test accuracy 98.44\n",
      "EPOCH 132:\n",
      "  batch 100 loss: 0.01336795046671341\n",
      "  batch 200 loss: 0.008380168691101062\n",
      "  batch 300 loss: 0.01588454122784242\n",
      "LOSS train 0.01588454122784242 valid 0.07744610152406184\n",
      "valid accuracy 98.6\n",
      "test accuracy 98.47\n",
      "EPOCH 133:\n",
      "  batch 100 loss: 0.011453290558229128\n",
      "  batch 200 loss: 0.012331102290175977\n",
      "  batch 300 loss: 0.012188261832343415\n",
      "LOSS train 0.012188261832343415 valid 0.08316953831359293\n",
      "valid accuracy 98.61\n",
      "test accuracy 98.63\n",
      "EPOCH 134:\n",
      "  batch 100 loss: 0.015522380498546227\n",
      "  batch 200 loss: 0.013645765581422893\n",
      "  batch 300 loss: 0.0143354505532443\n",
      "LOSS train 0.0143354505532443 valid 0.08272277901635823\n",
      "valid accuracy 98.61\n",
      "test accuracy 98.57\n",
      "EPOCH 135:\n",
      "  batch 100 loss: 0.014644364651339857\n",
      "  batch 200 loss: 0.015208623104153957\n",
      "  batch 300 loss: 0.01027683843551131\n",
      "LOSS train 0.01027683843551131 valid 0.08164067764448214\n",
      "valid accuracy 98.47\n",
      "test accuracy 98.59\n",
      "EPOCH 136:\n",
      "  batch 100 loss: 0.009877323334567336\n",
      "  batch 200 loss: 0.012501670537567407\n",
      "  batch 300 loss: 0.017229117295701143\n",
      "LOSS train 0.017229117295701143 valid 0.08620216049263335\n",
      "valid accuracy 98.51\n",
      "test accuracy 98.53\n",
      "EPOCH 137:\n",
      "  batch 100 loss: 0.014080137360522257\n",
      "  batch 200 loss: 0.01803991768812921\n",
      "  batch 300 loss: 0.01942778738852212\n",
      "LOSS train 0.01942778738852212 valid 0.08280386386742308\n",
      "valid accuracy 98.44\n",
      "test accuracy 98.54\n",
      "EPOCH 138:\n",
      "  batch 100 loss: 0.012137415501654232\n",
      "  batch 200 loss: 0.012232787562004433\n",
      "  batch 300 loss: 0.014086494458115339\n",
      "LOSS train 0.014086494458115339 valid 0.08417530042070327\n",
      "valid accuracy 98.51\n",
      "test accuracy 98.53\n",
      "EPOCH 139:\n",
      "  batch 100 loss: 0.013099171493531686\n",
      "  batch 200 loss: 0.011857474356884268\n",
      "  batch 300 loss: 0.008597281913203005\n",
      "LOSS train 0.008597281913203005 valid 0.08537237034079802\n",
      "valid accuracy 98.43\n",
      "test accuracy 98.52\n",
      "EPOCH 140:\n",
      "  batch 100 loss: 0.009007319784889205\n",
      "  batch 200 loss: 0.010063926927869034\n",
      "  batch 300 loss: 0.008056960028642379\n",
      "LOSS train 0.008056960028642379 valid 0.08144513387410773\n",
      "valid accuracy 98.45\n",
      "test accuracy 98.55\n",
      "EPOCH 141:\n",
      "  batch 100 loss: 0.01118995888658901\n",
      "  batch 200 loss: 0.010530892546930772\n",
      "  batch 300 loss: 0.013249579641174023\n",
      "LOSS train 0.013249579641174023 valid 0.08240988246818026\n",
      "valid accuracy 98.47\n",
      "test accuracy 98.57\n",
      "EPOCH 142:\n",
      "  batch 100 loss: 0.012411609556011172\n",
      "  batch 200 loss: 0.01149651160803387\n",
      "  batch 300 loss: 0.011029785490453036\n",
      "LOSS train 0.011029785490453036 valid 0.08488835841586778\n",
      "valid accuracy 98.51\n",
      "test accuracy 98.48\n",
      "EPOCH 143:\n",
      "  batch 100 loss: 0.0121798340060559\n",
      "  batch 200 loss: 0.014666393525803869\n",
      "  batch 300 loss: 0.011474286818775\n",
      "LOSS train 0.011474286818775 valid 0.0763141166887031\n",
      "valid accuracy 98.55\n",
      "test accuracy 98.55\n",
      "EPOCH 144:\n",
      "  batch 100 loss: 0.012749714080237028\n",
      "  batch 200 loss: 0.012988103892485014\n",
      "  batch 300 loss: 0.012705222880504153\n",
      "LOSS train 0.012705222880504153 valid 0.08232760498126308\n",
      "valid accuracy 98.46\n",
      "test accuracy 98.52\n",
      "EPOCH 145:\n",
      "  batch 100 loss: 0.008704929389896279\n",
      "  batch 200 loss: 0.00789923534120362\n",
      "  batch 300 loss: 0.011387390390448218\n",
      "LOSS train 0.011387390390448218 valid 0.07635697053654762\n",
      "valid accuracy 98.61\n",
      "test accuracy 98.63\n",
      "EPOCH 146:\n",
      "  batch 100 loss: 0.010346099095295358\n",
      "  batch 200 loss: 0.009541618197941944\n",
      "  batch 300 loss: 0.007453248051938317\n",
      "LOSS train 0.007453248051938317 valid 0.08405271355214505\n",
      "valid accuracy 98.61\n",
      "test accuracy 98.47\n",
      "EPOCH 147:\n",
      "  batch 100 loss: 0.010646946140263935\n",
      "  batch 200 loss: 0.008575521206364557\n",
      "  batch 300 loss: 0.010002960695313163\n",
      "LOSS train 0.010002960695313163 valid 0.0869879619180832\n",
      "valid accuracy 98.54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 98.42\n",
      "EPOCH 148:\n",
      "  batch 100 loss: 0.008996613198061425\n",
      "  batch 200 loss: 0.0123586261580067\n",
      "  batch 300 loss: 0.007307582403232118\n",
      "LOSS train 0.007307582403232118 valid 0.08618234945569833\n",
      "valid accuracy 98.47\n",
      "test accuracy 98.57\n",
      "EPOCH 149:\n",
      "  batch 100 loss: 0.006688311322695881\n",
      "  batch 200 loss: 0.00888690698427922\n",
      "  batch 300 loss: 0.006651465916011148\n",
      "LOSS train 0.006651465916011148 valid 0.08296848952407725\n",
      "valid accuracy 98.53\n",
      "test accuracy 98.5\n",
      "EPOCH 150:\n",
      "  batch 100 loss: 0.007175610397257515\n",
      "  batch 200 loss: 0.010915760352618235\n",
      "  batch 300 loss: 0.00908010488495961\n",
      "LOSS train 0.00908010488495961 valid 0.07325460389698561\n",
      "valid accuracy 98.64\n",
      "test accuracy 98.58\n",
      "EPOCH 151:\n",
      "  batch 100 loss: 0.006759251057691245\n",
      "  batch 200 loss: 0.008001756799794748\n",
      "  batch 300 loss: 0.008828993505894688\n",
      "LOSS train 0.008828993505894688 valid 0.07741820166783431\n",
      "valid accuracy 98.7\n",
      "test accuracy 98.56\n",
      "EPOCH 152:\n",
      "  batch 100 loss: 0.007508191415978444\n",
      "  batch 200 loss: 0.00509299952127634\n",
      "  batch 300 loss: 0.007531884789020751\n",
      "LOSS train 0.007531884789020751 valid 0.08413986138171661\n",
      "valid accuracy 98.57\n",
      "test accuracy 98.59\n",
      "EPOCH 153:\n",
      "  batch 100 loss: 0.006546731036896744\n",
      "  batch 200 loss: 0.007611955130587375\n",
      "  batch 300 loss: 0.008029190425709203\n",
      "LOSS train 0.008029190425709203 valid 0.08417148271699354\n",
      "valid accuracy 98.52\n",
      "test accuracy 98.47\n",
      "EPOCH 154:\n",
      "  batch 100 loss: 0.00869262039068758\n",
      "  batch 200 loss: 0.009910619288034469\n",
      "  batch 300 loss: 0.012549553274072877\n",
      "LOSS train 0.012549553274072877 valid 0.08536577588984073\n",
      "valid accuracy 98.57\n",
      "test accuracy 98.52\n",
      "EPOCH 155:\n",
      "  batch 100 loss: 0.008664017868741212\n",
      "  batch 200 loss: 0.01062744019519414\n",
      "  batch 300 loss: 0.006109587311489122\n",
      "LOSS train 0.006109587311489122 valid 0.08460088278408438\n",
      "valid accuracy 98.6\n",
      "test accuracy 98.53\n",
      "EPOCH 156:\n",
      "  batch 100 loss: 0.011442183587048476\n",
      "  batch 200 loss: 0.007874994225171577\n",
      "  batch 300 loss: 0.010815253345026577\n",
      "LOSS train 0.010815253345026577 valid 0.0817952269196753\n",
      "valid accuracy 98.63\n",
      "test accuracy 98.57\n",
      "EPOCH 157:\n",
      "  batch 100 loss: 0.008868233244392058\n",
      "  batch 200 loss: 0.006995811194490784\n",
      "  batch 300 loss: 0.013817055005692963\n",
      "LOSS train 0.013817055005692963 valid 0.08281223635521764\n",
      "valid accuracy 98.59\n",
      "test accuracy 98.55\n",
      "EPOCH 158:\n",
      "  batch 100 loss: 0.010499763351067486\n",
      "  batch 200 loss: 0.010004523821378371\n",
      "  batch 300 loss: 0.010822328196020407\n",
      "LOSS train 0.010822328196020407 valid 0.08744381737783499\n",
      "valid accuracy 98.55\n",
      "test accuracy 98.45\n",
      "EPOCH 159:\n",
      "  batch 100 loss: 0.009863814535569872\n",
      "  batch 200 loss: 0.007222267819377066\n",
      "  batch 300 loss: 0.007256128287981483\n",
      "LOSS train 0.007256128287981483 valid 0.08565991478276316\n",
      "valid accuracy 98.49\n",
      "test accuracy 98.45\n",
      "EPOCH 160:\n",
      "  batch 100 loss: 0.010627508110337658\n",
      "  batch 200 loss: 0.009876965983449964\n",
      "  batch 300 loss: 0.010090311491185276\n",
      "LOSS train 0.010090311491185276 valid 0.08676714308076273\n",
      "valid accuracy 98.63\n",
      "test accuracy 98.51\n",
      "EPOCH 161:\n",
      "  batch 100 loss: 0.007830013783121785\n",
      "  batch 200 loss: 0.009539768763777373\n",
      "  batch 300 loss: 0.01410497745289831\n",
      "LOSS train 0.01410497745289831 valid 0.08365876065174603\n",
      "valid accuracy 98.69\n",
      "test accuracy 98.47\n",
      "EPOCH 162:\n",
      "  batch 100 loss: 0.010836596946146528\n",
      "  batch 200 loss: 0.005356431154605161\n",
      "  batch 300 loss: 0.010098314532629047\n",
      "LOSS train 0.010098314532629047 valid 0.0859464974824283\n",
      "valid accuracy 98.55\n",
      "test accuracy 98.47\n",
      "EPOCH 163:\n",
      "  batch 100 loss: 0.0069251820732233686\n",
      "  batch 200 loss: 0.008958138198642019\n",
      "  batch 300 loss: 0.007988460596661752\n",
      "LOSS train 0.007988460596661752 valid 0.08518414373643685\n",
      "valid accuracy 98.57\n",
      "test accuracy 98.42\n",
      "EPOCH 164:\n",
      "  batch 100 loss: 0.009160059804905814\n",
      "  batch 200 loss: 0.01253578952668363\n",
      "  batch 300 loss: 0.01091940183852671\n",
      "LOSS train 0.01091940183852671 valid 0.08555720706807952\n",
      "valid accuracy 98.52\n",
      "test accuracy 98.51\n",
      "EPOCH 165:\n",
      "  batch 100 loss: 0.01364588332728431\n",
      "  batch 200 loss: 0.00665537794438535\n",
      "  batch 300 loss: 0.01144852408752456\n",
      "LOSS train 0.01144852408752456 valid 0.08542648836737499\n",
      "valid accuracy 98.59\n",
      "test accuracy 98.54\n",
      "EPOCH 166:\n",
      "  batch 100 loss: 0.00873747023795147\n",
      "  batch 200 loss: 0.00862270536367987\n",
      "  batch 300 loss: 0.00904254136054078\n",
      "LOSS train 0.00904254136054078 valid 0.08514355869795773\n",
      "valid accuracy 98.62\n",
      "test accuracy 98.7\n",
      "EPOCH 167:\n",
      "  batch 100 loss: 0.014085247787750176\n",
      "  batch 200 loss: 0.010259619429714349\n",
      "  batch 300 loss: 0.011102915872256744\n",
      "LOSS train 0.011102915872256744 valid 0.10010196750134795\n",
      "valid accuracy 98.57\n",
      "test accuracy 98.47\n",
      "EPOCH 168:\n",
      "  batch 100 loss: 0.010677222068176633\n",
      "  batch 200 loss: 0.011219577565703957\n",
      "  batch 300 loss: 0.011630256542982807\n",
      "LOSS train 0.011630256542982807 valid 0.08194001600345416\n",
      "valid accuracy 98.59\n",
      "test accuracy 98.52\n",
      "EPOCH 169:\n",
      "  batch 100 loss: 0.0073606186940378395\n",
      "  batch 200 loss: 0.009284491209226076\n",
      "  batch 300 loss: 0.006270601234682545\n",
      "LOSS train 0.006270601234682545 valid 0.08749489805206072\n",
      "valid accuracy 98.62\n",
      "test accuracy 98.61\n",
      "EPOCH 170:\n",
      "  batch 100 loss: 0.011977844761611891\n",
      "  batch 200 loss: 0.008610610669639413\n",
      "  batch 300 loss: 0.009513094857647958\n",
      "LOSS train 0.009513094857647958 valid 0.08372504558088575\n",
      "valid accuracy 98.45\n",
      "test accuracy 98.45\n",
      "EPOCH 171:\n",
      "  batch 100 loss: 0.008180083452893996\n",
      "  batch 200 loss: 0.009911465860345742\n",
      "  batch 300 loss: 0.006923934626840946\n",
      "LOSS train 0.006923934626840946 valid 0.08464516270871023\n",
      "valid accuracy 98.6\n",
      "test accuracy 98.55\n",
      "EPOCH 172:\n",
      "  batch 100 loss: 0.01017072575302791\n",
      "  batch 200 loss: 0.005390046360045062\n",
      "  batch 300 loss: 0.007877771446991345\n",
      "LOSS train 0.007877771446991345 valid 0.0785443935360332\n",
      "valid accuracy 98.56\n",
      "test accuracy 98.59\n",
      "EPOCH 173:\n",
      "  batch 100 loss: 0.00613022521476978\n",
      "  batch 200 loss: 0.005838305128961565\n",
      "  batch 300 loss: 0.007796625157820927\n",
      "LOSS train 0.007796625157820927 valid 0.08529884232166728\n",
      "valid accuracy 98.57\n",
      "test accuracy 98.54\n",
      "EPOCH 174:\n",
      "  batch 100 loss: 0.004584442304385448\n",
      "  batch 200 loss: 0.00678988313426089\n",
      "  batch 300 loss: 0.005342317997312876\n",
      "LOSS train 0.005342317997312876 valid 0.0791440663081895\n",
      "valid accuracy 98.61\n",
      "test accuracy 98.57\n",
      "EPOCH 175:\n",
      "  batch 100 loss: 0.004852384338742013\n",
      "  batch 200 loss: 0.0052272153677608915\n",
      "  batch 300 loss: 0.007443969128330536\n",
      "LOSS train 0.007443969128330536 valid 0.07618899023195938\n",
      "valid accuracy 98.71\n",
      "test accuracy 98.65\n",
      "EPOCH 176:\n",
      "  batch 100 loss: 0.006344264828967994\n",
      "  batch 200 loss: 0.00862799963878615\n",
      "  batch 300 loss: 0.007040223912518968\n",
      "LOSS train 0.007040223912518968 valid 0.0883056690368627\n",
      "valid accuracy 98.49\n",
      "test accuracy 98.6\n",
      "EPOCH 177:\n",
      "  batch 100 loss: 0.008490256192106926\n",
      "  batch 200 loss: 0.009006452556134264\n",
      "  batch 300 loss: 0.0054267725232728025\n",
      "LOSS train 0.0054267725232728025 valid 0.08120221751131437\n",
      "valid accuracy 98.63\n",
      "test accuracy 98.62\n",
      "EPOCH 178:\n",
      "  batch 100 loss: 0.008986618836044045\n",
      "  batch 200 loss: 0.007572573029128762\n",
      "  batch 300 loss: 0.01030891007679287\n",
      "LOSS train 0.01030891007679287 valid 0.08609542376953995\n",
      "valid accuracy 98.6\n",
      "test accuracy 98.65\n",
      "EPOCH 179:\n",
      "  batch 100 loss: 0.007024981039270415\n",
      "  batch 200 loss: 0.008411286042088477\n",
      "  batch 300 loss: 0.00729870220994826\n",
      "LOSS train 0.00729870220994826 valid 0.07500301556049173\n",
      "valid accuracy 98.65\n",
      "test accuracy 98.56\n",
      "EPOCH 180:\n",
      "  batch 100 loss: 0.007587945303739616\n",
      "  batch 200 loss: 0.010154898412682769\n",
      "  batch 300 loss: 0.009571240787365695\n",
      "LOSS train 0.009571240787365695 valid 0.08275961771060736\n",
      "valid accuracy 98.64\n",
      "test accuracy 98.59\n",
      "EPOCH 181:\n",
      "  batch 100 loss: 0.006053709827343709\n",
      "  batch 200 loss: 0.010286075834172266\n",
      "  batch 300 loss: 0.011133518319037422\n",
      "LOSS train 0.011133518319037422 valid 0.08457603591569417\n",
      "valid accuracy 98.59\n",
      "test accuracy 98.55\n",
      "EPOCH 182:\n",
      "  batch 100 loss: 0.007264424504296585\n",
      "  batch 200 loss: 0.007900914771842054\n",
      "  batch 300 loss: 0.005410801195666864\n",
      "LOSS train 0.005410801195666864 valid 0.07583065330906216\n",
      "valid accuracy 98.61\n",
      "test accuracy 98.54\n",
      "EPOCH 183:\n",
      "  batch 100 loss: 0.013414144499033682\n",
      "  batch 200 loss: 0.008049813594871011\n",
      "  batch 300 loss: 0.004821401616106868\n",
      "LOSS train 0.004821401616106868 valid 0.0791544679451374\n",
      "valid accuracy 98.55\n",
      "test accuracy 98.52\n",
      "EPOCH 184:\n",
      "  batch 100 loss: 0.007646235741922283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 200 loss: 0.006467973547045176\n",
      "  batch 300 loss: 0.012658506603529532\n",
      "LOSS train 0.012658506603529532 valid 0.07980557456074719\n",
      "valid accuracy 98.61\n",
      "test accuracy 98.52\n",
      "EPOCH 185:\n",
      "  batch 100 loss: 0.009400744627877771\n",
      "  batch 200 loss: 0.007661861482285985\n",
      "  batch 300 loss: 0.012762390364996464\n",
      "LOSS train 0.012762390364996464 valid 0.08557477958642665\n",
      "valid accuracy 98.56\n",
      "test accuracy 98.62\n",
      "EPOCH 186:\n",
      "  batch 100 loss: 0.006781309877223976\n",
      "  batch 200 loss: 0.010787870608992308\n",
      "  batch 300 loss: 0.009593311570664582\n",
      "LOSS train 0.009593311570664582 valid 0.08175900824317989\n",
      "valid accuracy 98.57\n",
      "test accuracy 98.56\n",
      "EPOCH 187:\n",
      "  batch 100 loss: 0.004592447931719903\n",
      "  batch 200 loss: 0.012314644043989915\n",
      "  batch 300 loss: 0.010271608176358313\n",
      "LOSS train 0.010271608176358313 valid 0.08583847552654333\n",
      "valid accuracy 98.53\n",
      "test accuracy 98.49\n",
      "EPOCH 188:\n",
      "  batch 100 loss: 0.006968171849816827\n",
      "  batch 200 loss: 0.009669094219957515\n",
      "  batch 300 loss: 0.006960117576379616\n",
      "LOSS train 0.006960117576379616 valid 0.07926791385946245\n",
      "valid accuracy 98.52\n",
      "test accuracy 98.46\n",
      "EPOCH 189:\n",
      "  batch 100 loss: 0.0069527816416280076\n",
      "  batch 200 loss: 0.013473470021827439\n",
      "  batch 300 loss: 0.0060540232155719305\n",
      "LOSS train 0.0060540232155719305 valid 0.08306486353352205\n",
      "valid accuracy 98.53\n",
      "test accuracy 98.52\n",
      "EPOCH 190:\n",
      "  batch 100 loss: 0.010075568117122203\n",
      "  batch 200 loss: 0.009091178596120244\n",
      "  batch 300 loss: 0.00656552592688854\n",
      "LOSS train 0.00656552592688854 valid 0.08044931494142066\n",
      "valid accuracy 98.67\n",
      "test accuracy 98.58\n",
      "EPOCH 191:\n",
      "  batch 100 loss: 0.006625311207124014\n",
      "  batch 200 loss: 0.007790558252562505\n",
      "  batch 300 loss: 0.008189715476983111\n",
      "LOSS train 0.008189715476983111 valid 0.08083672217872856\n",
      "valid accuracy 98.51\n",
      "test accuracy 98.62\n",
      "EPOCH 192:\n",
      "  batch 100 loss: 0.008806957806399396\n",
      "  batch 200 loss: 0.008714974554163746\n",
      "  batch 300 loss: 0.007665054752159221\n",
      "LOSS train 0.007665054752159221 valid 0.0800150279773495\n",
      "valid accuracy 98.64\n",
      "test accuracy 98.51\n",
      "EPOCH 193:\n",
      "  batch 100 loss: 0.005977360467010442\n",
      "  batch 200 loss: 0.009108101050676965\n",
      "  batch 300 loss: 0.007725338151925598\n",
      "LOSS train 0.007725338151925598 valid 0.07569198181560247\n",
      "valid accuracy 98.68\n",
      "test accuracy 98.55\n",
      "EPOCH 194:\n",
      "  batch 100 loss: 0.0039531661816579345\n",
      "  batch 200 loss: 0.010118395571307417\n",
      "  batch 300 loss: 0.005781561669346047\n",
      "LOSS train 0.005781561669346047 valid 0.07928670091536213\n",
      "valid accuracy 98.7\n",
      "test accuracy 98.47\n",
      "EPOCH 195:\n",
      "  batch 100 loss: 0.0050197868169743745\n",
      "  batch 200 loss: 0.004200563511419659\n",
      "  batch 300 loss: 0.004454846937422161\n",
      "LOSS train 0.004454846937422161 valid 0.08336355648696876\n",
      "valid accuracy 98.62\n",
      "test accuracy 98.54\n",
      "EPOCH 196:\n",
      "  batch 100 loss: 0.008650471697537796\n",
      "  batch 200 loss: 0.009383436128327958\n",
      "  batch 300 loss: 0.004813575099610006\n",
      "LOSS train 0.004813575099610006 valid 0.07763031392761413\n",
      "valid accuracy 98.58\n",
      "test accuracy 98.57\n",
      "EPOCH 197:\n",
      "  batch 100 loss: 0.003913151789504354\n",
      "  batch 200 loss: 0.004720997015826924\n",
      "  batch 300 loss: 0.009644310835340094\n",
      "LOSS train 0.009644310835340094 valid 0.08385046480954031\n",
      "valid accuracy 98.54\n",
      "test accuracy 98.66\n",
      "EPOCH 198:\n",
      "  batch 100 loss: 0.006204720453038703\n",
      "  batch 200 loss: 0.0059230177510573865\n",
      "  batch 300 loss: 0.006279776202200082\n",
      "LOSS train 0.006279776202200082 valid 0.08088956058393726\n",
      "valid accuracy 98.6\n",
      "test accuracy 98.57\n",
      "EPOCH 199:\n",
      "  batch 100 loss: 0.008093863280330425\n",
      "  batch 200 loss: 0.005196141450867913\n",
      "  batch 300 loss: 0.005101861034749788\n",
      "LOSS train 0.005101861034749788 valid 0.07954875401836471\n",
      "valid accuracy 98.75\n",
      "test accuracy 98.7\n",
      "EPOCH 200:\n",
      "  batch 100 loss: 0.0037665900027366206\n",
      "  batch 200 loss: 0.006407820566932969\n",
      "  batch 300 loss: 0.005554971410158487\n",
      "LOSS train 0.005554971410158487 valid 0.08002138824653535\n",
      "valid accuracy 98.73\n",
      "test accuracy 98.74\n",
      "EPOCH 201:\n",
      "  batch 100 loss: 0.00543482518227961\n",
      "  batch 200 loss: 0.002546064537811503\n",
      "  batch 300 loss: 0.005174620654683224\n",
      "LOSS train 0.005174620654683224 valid 0.08682602407170519\n",
      "valid accuracy 98.6\n",
      "test accuracy 98.6\n",
      "EPOCH 202:\n",
      "  batch 100 loss: 0.007580895193292463\n",
      "  batch 200 loss: 0.00854235431235452\n",
      "  batch 300 loss: 0.006885241505246995\n",
      "LOSS train 0.006885241505246995 valid 0.0848194018550617\n",
      "valid accuracy 98.63\n",
      "test accuracy 98.7\n",
      "EPOCH 203:\n",
      "  batch 100 loss: 0.006755286807069751\n",
      "  batch 200 loss: 0.012723579107790784\n",
      "  batch 300 loss: 0.006560712857135513\n",
      "LOSS train 0.006560712857135513 valid 0.0818475340045565\n",
      "valid accuracy 98.56\n",
      "test accuracy 98.67\n",
      "EPOCH 204:\n",
      "  batch 100 loss: 0.007215480705924051\n",
      "  batch 200 loss: 0.007017603016508929\n",
      "  batch 300 loss: 0.008297569426621862\n",
      "LOSS train 0.008297569426621862 valid 0.08370508818267544\n",
      "valid accuracy 98.6\n",
      "test accuracy 98.72\n",
      "EPOCH 205:\n",
      "  batch 100 loss: 0.007225565087174459\n",
      "  batch 200 loss: 0.005053571932580781\n",
      "  batch 300 loss: 0.006037723883930966\n",
      "LOSS train 0.006037723883930966 valid 0.0807950573270386\n",
      "valid accuracy 98.61\n",
      "test accuracy 98.75\n",
      "EPOCH 206:\n",
      "  batch 100 loss: 0.004625931282994315\n",
      "  batch 200 loss: 0.005834320335120538\n",
      "  batch 300 loss: 0.004905145362414487\n",
      "LOSS train 0.004905145362414487 valid 0.08484466888705955\n",
      "valid accuracy 98.62\n",
      "test accuracy 98.57\n",
      "EPOCH 207:\n",
      "  batch 100 loss: 0.003182451947934908\n",
      "  batch 200 loss: 0.0044963070730580056\n",
      "  batch 300 loss: 0.005864626112579572\n",
      "LOSS train 0.005864626112579572 valid 0.0928750061218263\n",
      "valid accuracy 98.66\n",
      "test accuracy 98.59\n",
      "EPOCH 208:\n",
      "  batch 100 loss: 0.004680596593644282\n",
      "  batch 200 loss: 0.004212026209223722\n",
      "  batch 300 loss: 0.006093812375526113\n",
      "LOSS train 0.006093812375526113 valid 0.0831891942121564\n",
      "valid accuracy 98.73\n",
      "test accuracy 98.69\n",
      "EPOCH 209:\n",
      "  batch 100 loss: 0.006434026962683958\n",
      "  batch 200 loss: 0.003990546343337229\n",
      "  batch 300 loss: 0.005459453112323444\n",
      "LOSS train 0.005459453112323444 valid 0.08466626760521137\n",
      "valid accuracy 98.71\n",
      "test accuracy 98.61\n",
      "EPOCH 210:\n",
      "  batch 100 loss: 0.007480733817396441\n",
      "  batch 200 loss: 0.006321372280967808\n",
      "  batch 300 loss: 0.005185191035733112\n",
      "LOSS train 0.005185191035733112 valid 0.0906962396422352\n",
      "valid accuracy 98.68\n",
      "test accuracy 98.61\n",
      "EPOCH 211:\n",
      "  batch 100 loss: 0.0060733519850180075\n",
      "  batch 200 loss: 0.006083023529869606\n",
      "  batch 300 loss: 0.004001115924293117\n",
      "LOSS train 0.004001115924293117 valid 0.08442631241461664\n",
      "valid accuracy 98.67\n",
      "test accuracy 98.6\n",
      "EPOCH 212:\n",
      "  batch 100 loss: 0.009050716620399158\n",
      "  batch 200 loss: 0.010732042936840571\n",
      "  batch 300 loss: 0.008681085550114744\n",
      "LOSS train 0.008681085550114744 valid 0.08946622125777542\n",
      "valid accuracy 98.74\n",
      "test accuracy 98.58\n",
      "EPOCH 213:\n",
      "  batch 100 loss: 0.007109066388358088\n",
      "  batch 200 loss: 0.006351938109580147\n",
      "  batch 300 loss: 0.006627999623344181\n",
      "LOSS train 0.006627999623344181 valid 0.08550856811402925\n",
      "valid accuracy 98.75\n",
      "test accuracy 98.65\n",
      "EPOCH 214:\n",
      "  batch 100 loss: 0.0035382471374563805\n",
      "  batch 200 loss: 0.00604552551337008\n",
      "  batch 300 loss: 0.005266103820441117\n",
      "LOSS train 0.005266103820441117 valid 0.08765368185419292\n",
      "valid accuracy 98.66\n",
      "test accuracy 98.57\n",
      "EPOCH 215:\n",
      "  batch 100 loss: 0.007736579242952715\n",
      "  batch 200 loss: 0.006406481353324125\n",
      "  batch 300 loss: 0.005405717044138783\n",
      "LOSS train 0.005405717044138783 valid 0.08120009888428781\n",
      "valid accuracy 98.6\n",
      "test accuracy 98.69\n",
      "EPOCH 216:\n",
      "  batch 100 loss: 0.006318747184031395\n",
      "  batch 200 loss: 0.006882986490906546\n",
      "  batch 300 loss: 0.008393262201154812\n",
      "LOSS train 0.008393262201154812 valid 0.08643490809648524\n",
      "valid accuracy 98.54\n",
      "test accuracy 98.59\n",
      "EPOCH 217:\n",
      "  batch 100 loss: 0.0063591002422401744\n",
      "  batch 200 loss: 0.005234303644041134\n",
      "  batch 300 loss: 0.008809449585839388\n",
      "LOSS train 0.008809449585839388 valid 0.08573622718949415\n",
      "valid accuracy 98.64\n",
      "test accuracy 98.59\n",
      "EPOCH 218:\n",
      "  batch 100 loss: 0.007446654099641137\n",
      "  batch 200 loss: 0.005574202494777865\n",
      "  batch 300 loss: 0.005813161654630221\n",
      "LOSS train 0.005813161654630221 valid 0.08558747174568372\n",
      "valid accuracy 98.67\n",
      "test accuracy 98.54\n",
      "EPOCH 219:\n",
      "  batch 100 loss: 0.005621408696900545\n",
      "  batch 200 loss: 0.005220778789725386\n",
      "  batch 300 loss: 0.005850052728542039\n",
      "LOSS train 0.005850052728542039 valid 0.0899709741214234\n",
      "valid accuracy 98.61\n",
      "test accuracy 98.47\n",
      "EPOCH 220:\n",
      "  batch 100 loss: 0.004548670606501446\n",
      "  batch 200 loss: 0.0058969120532363205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 300 loss: 0.006297858143975646\n",
      "LOSS train 0.006297858143975646 valid 0.08370235701240054\n",
      "valid accuracy 98.64\n",
      "test accuracy 98.58\n",
      "EPOCH 221:\n",
      "  batch 100 loss: 0.0070876107293287305\n",
      "  batch 200 loss: 0.004928517980111451\n",
      "  batch 300 loss: 0.007997815450096937\n",
      "LOSS train 0.007997815450096937 valid 0.08240061417202497\n",
      "valid accuracy 98.66\n",
      "test accuracy 98.63\n",
      "EPOCH 222:\n",
      "  batch 100 loss: 0.005717249131934068\n",
      "  batch 200 loss: 0.0038735490493104407\n",
      "  batch 300 loss: 0.00351551860466202\n",
      "LOSS train 0.00351551860466202 valid 0.08492208964434177\n",
      "valid accuracy 98.61\n",
      "test accuracy 98.63\n",
      "EPOCH 223:\n",
      "  batch 100 loss: 0.005663363713584601\n",
      "  batch 200 loss: 0.004473832482352691\n",
      "  batch 300 loss: 0.004988176035203651\n",
      "LOSS train 0.004988176035203651 valid 0.08316023005139685\n",
      "valid accuracy 98.66\n",
      "test accuracy 98.64\n",
      "EPOCH 224:\n",
      "  batch 100 loss: 0.0035298145387787374\n",
      "  batch 200 loss: 0.005945992284724042\n",
      "  batch 300 loss: 0.003043221875066706\n",
      "LOSS train 0.003043221875066706 valid 0.0860056025728802\n",
      "valid accuracy 98.65\n",
      "test accuracy 98.62\n",
      "EPOCH 225:\n",
      "  batch 100 loss: 0.005557425662417472\n",
      "  batch 200 loss: 0.006062873322399583\n",
      "  batch 300 loss: 0.009068192027655755\n",
      "LOSS train 0.009068192027655755 valid 0.08835245295731715\n",
      "valid accuracy 98.61\n",
      "test accuracy 98.64\n",
      "EPOCH 226:\n",
      "  batch 100 loss: 0.00564645754456663\n",
      "  batch 200 loss: 0.005184843421537621\n",
      "  batch 300 loss: 0.008196541803588388\n",
      "LOSS train 0.008196541803588388 valid 0.08653446354743631\n",
      "valid accuracy 98.61\n",
      "test accuracy 98.62\n",
      "EPOCH 227:\n",
      "  batch 100 loss: 0.006032358871688075\n",
      "  batch 200 loss: 0.0035339478215755805\n",
      "  batch 300 loss: 0.007448722290399275\n",
      "LOSS train 0.007448722290399275 valid 0.09177367092667703\n",
      "valid accuracy 98.59\n",
      "test accuracy 98.48\n",
      "EPOCH 228:\n",
      "  batch 100 loss: 0.009953482581660183\n",
      "  batch 200 loss: 0.005060124016272312\n",
      "  batch 300 loss: 0.007030965369515343\n",
      "LOSS train 0.007030965369515343 valid 0.09114549484652085\n",
      "valid accuracy 98.58\n",
      "test accuracy 98.57\n",
      "EPOCH 229:\n",
      "  batch 100 loss: 0.0043663179484934745\n",
      "  batch 200 loss: 0.006840687813112538\n",
      "  batch 300 loss: 0.007700119093519788\n",
      "LOSS train 0.007700119093519788 valid 0.09078295791966089\n",
      "valid accuracy 98.54\n",
      "test accuracy 98.63\n",
      "EPOCH 230:\n",
      "  batch 100 loss: 0.005085506386053282\n",
      "  batch 200 loss: 0.004431369381837271\n",
      "  batch 300 loss: 0.005885459484684929\n",
      "LOSS train 0.005885459484684929 valid 0.082566683576931\n",
      "valid accuracy 98.7\n",
      "test accuracy 98.66\n",
      "EPOCH 231:\n",
      "  batch 100 loss: 0.005155341684430717\n",
      "  batch 200 loss: 0.004907997570857532\n",
      "  batch 300 loss: 0.007662998186446402\n",
      "LOSS train 0.007662998186446402 valid 0.09206113444834275\n",
      "valid accuracy 98.54\n",
      "test accuracy 98.65\n",
      "EPOCH 232:\n",
      "  batch 100 loss: 0.0080981854583918\n",
      "  batch 200 loss: 0.0036986082252767004\n",
      "  batch 300 loss: 0.004315324553279538\n",
      "LOSS train 0.004315324553279538 valid 0.08781093078441397\n",
      "valid accuracy 98.64\n",
      "test accuracy 98.67\n",
      "EPOCH 233:\n",
      "  batch 100 loss: 0.00687546776261371\n",
      "  batch 200 loss: 0.005368878333023019\n",
      "  batch 300 loss: 0.0049398227904177364\n",
      "LOSS train 0.0049398227904177364 valid 0.08723033298156943\n",
      "valid accuracy 98.68\n",
      "test accuracy 98.61\n",
      "EPOCH 234:\n",
      "  batch 100 loss: 0.004911105122765207\n",
      "  batch 200 loss: 0.003976355315700743\n",
      "  batch 300 loss: 0.004588846558500137\n",
      "LOSS train 0.004588846558500137 valid 0.08265613694308878\n",
      "valid accuracy 98.65\n",
      "test accuracy 98.55\n",
      "EPOCH 235:\n",
      "  batch 100 loss: 0.004002990304736187\n",
      "  batch 200 loss: 0.004142160022079579\n",
      "  batch 300 loss: 0.006409758291573837\n",
      "LOSS train 0.006409758291573837 valid 0.08495335627371557\n",
      "valid accuracy 98.58\n",
      "test accuracy 98.6\n",
      "EPOCH 236:\n",
      "  batch 100 loss: 0.005055712011880757\n",
      "  batch 200 loss: 0.00453872025487442\n",
      "  batch 300 loss: 0.005835416713472057\n",
      "LOSS train 0.005835416713472057 valid 0.0902444757252794\n",
      "valid accuracy 98.62\n",
      "test accuracy 98.61\n",
      "EPOCH 237:\n",
      "  batch 100 loss: 0.004668950852210969\n",
      "  batch 200 loss: 0.005559913290913379\n",
      "  batch 300 loss: 0.0038758051910735957\n",
      "LOSS train 0.0038758051910735957 valid 0.07957550986205356\n",
      "valid accuracy 98.63\n",
      "test accuracy 98.67\n",
      "EPOCH 238:\n",
      "  batch 100 loss: 0.0060497870681319905\n",
      "  batch 200 loss: 0.004822608283967753\n",
      "  batch 300 loss: 0.0075601932551637675\n",
      "LOSS train 0.0075601932551637675 valid 0.08602214553903058\n",
      "valid accuracy 98.63\n",
      "test accuracy 98.66\n",
      "EPOCH 239:\n",
      "  batch 100 loss: 0.004499361291556738\n",
      "  batch 200 loss: 0.004419974845965271\n",
      "  batch 300 loss: 0.006835900471498917\n",
      "LOSS train 0.006835900471498917 valid 0.08451650690181861\n",
      "valid accuracy 98.71\n",
      "test accuracy 98.52\n",
      "EPOCH 240:\n",
      "  batch 100 loss: 0.0029122357812887854\n",
      "  batch 200 loss: 0.004527634231505715\n",
      "  batch 300 loss: 0.006155405266216008\n",
      "LOSS train 0.006155405266216008 valid 0.08563092890515783\n",
      "valid accuracy 98.7\n",
      "test accuracy 98.54\n",
      "EPOCH 241:\n",
      "  batch 100 loss: 0.0046921922612420276\n",
      "  batch 200 loss: 0.005945402898919383\n",
      "  batch 300 loss: 0.003798079281289688\n",
      "LOSS train 0.003798079281289688 valid 0.08735878915874142\n",
      "valid accuracy 98.65\n",
      "test accuracy 98.58\n",
      "EPOCH 242:\n",
      "  batch 100 loss: 0.003213576731428418\n",
      "  batch 200 loss: 0.004560160118708012\n",
      "  batch 300 loss: 0.005793421646876027\n",
      "LOSS train 0.005793421646876027 valid 0.08117009792278872\n",
      "valid accuracy 98.71\n",
      "test accuracy 98.59\n",
      "EPOCH 243:\n",
      "  batch 100 loss: 0.0038986227157460007\n",
      "  batch 200 loss: 0.005055346352091874\n",
      "  batch 300 loss: 0.004399595556488691\n",
      "LOSS train 0.004399595556488691 valid 0.08317954428891046\n",
      "valid accuracy 98.63\n",
      "test accuracy 98.63\n",
      "EPOCH 244:\n",
      "  batch 100 loss: 0.004119802101944856\n",
      "  batch 200 loss: 0.004145126178944025\n",
      "  batch 300 loss: 0.006878541401485449\n",
      "LOSS train 0.006878541401485449 valid 0.08616279531613341\n",
      "valid accuracy 98.68\n",
      "test accuracy 98.56\n",
      "EPOCH 245:\n",
      "  batch 100 loss: 0.002741148705783303\n",
      "  batch 200 loss: 0.003748754612328611\n",
      "  batch 300 loss: 0.0035757875509656854\n",
      "LOSS train 0.0035757875509656854 valid 0.08606117896447128\n",
      "valid accuracy 98.64\n",
      "test accuracy 98.64\n",
      "EPOCH 246:\n",
      "  batch 100 loss: 0.0034765310984926855\n",
      "  batch 200 loss: 0.005070581903946731\n",
      "  batch 300 loss: 0.00477358440210935\n",
      "LOSS train 0.00477358440210935 valid 0.087585212892498\n",
      "valid accuracy 98.67\n",
      "test accuracy 98.65\n",
      "EPOCH 247:\n",
      "  batch 100 loss: 0.005291878382678377\n",
      "  batch 200 loss: 0.005416626429429243\n",
      "  batch 300 loss: 0.004277482068710387\n",
      "LOSS train 0.004277482068710387 valid 0.08731124512273174\n",
      "valid accuracy 98.55\n",
      "test accuracy 98.65\n",
      "EPOCH 248:\n",
      "  batch 100 loss: 0.00353640792127635\n",
      "  batch 200 loss: 0.0038453482986695064\n",
      "  batch 300 loss: 0.0038068283680661352\n",
      "LOSS train 0.0038068283680661352 valid 0.08447983620836463\n",
      "valid accuracy 98.66\n",
      "test accuracy 98.7\n",
      "EPOCH 249:\n",
      "  batch 100 loss: 0.004729405568689059\n",
      "  batch 200 loss: 0.0036438154404228127\n",
      "  batch 300 loss: 0.005387843611050016\n",
      "LOSS train 0.005387843611050016 valid 0.0803209208484595\n",
      "valid accuracy 98.66\n",
      "test accuracy 98.65\n",
      "EPOCH 250:\n",
      "  batch 100 loss: 0.005042190091612611\n",
      "  batch 200 loss: 0.003724623674457348\n",
      "  batch 300 loss: 0.004526010408974912\n",
      "LOSS train 0.004526010408974912 valid 0.08355915387620244\n",
      "valid accuracy 98.67\n",
      "test accuracy 98.71\n",
      "EPOCH 251:\n",
      "  batch 100 loss: 0.004850002925664683\n",
      "  batch 200 loss: 0.0040116934720643375\n",
      "  batch 300 loss: 0.003775387426996275\n",
      "LOSS train 0.003775387426996275 valid 0.0898973054881302\n",
      "valid accuracy 98.73\n",
      "test accuracy 98.54\n",
      "EPOCH 252:\n",
      "  batch 100 loss: 0.005298350306897106\n",
      "  batch 200 loss: 0.0049134983258242925\n",
      "  batch 300 loss: 0.005039424676889155\n",
      "LOSS train 0.005039424676889155 valid 0.08787963093393242\n",
      "valid accuracy 98.67\n",
      "test accuracy 98.57\n",
      "EPOCH 253:\n",
      "  batch 100 loss: 0.0037134572175256154\n",
      "  batch 200 loss: 0.0033886892142527356\n",
      "  batch 300 loss: 0.004643145564044566\n",
      "LOSS train 0.004643145564044566 valid 0.08983411179247877\n",
      "valid accuracy 98.62\n",
      "test accuracy 98.64\n",
      "EPOCH 254:\n",
      "  batch 100 loss: 0.0016838910424480957\n",
      "  batch 200 loss: 0.001978991276528781\n",
      "  batch 300 loss: 0.002761766122350604\n",
      "LOSS train 0.002761766122350604 valid 0.08405985123845514\n",
      "valid accuracy 98.53\n",
      "test accuracy 98.57\n",
      "EPOCH 255:\n",
      "  batch 100 loss: 0.0034423441007593782\n",
      "  batch 200 loss: 0.004788800580445951\n",
      "  batch 300 loss: 0.006031564671137062\n",
      "LOSS train 0.006031564671137062 valid 0.08745986218887486\n",
      "valid accuracy 98.69\n",
      "test accuracy 98.63\n",
      "EPOCH 256:\n",
      "  batch 100 loss: 0.004245066168457896\n",
      "  batch 200 loss: 0.004178813326582827\n",
      "  batch 300 loss: 0.0022900406934060415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 0.0022900406934060415 valid 0.09077377827378758\n",
      "valid accuracy 98.61\n",
      "test accuracy 98.57\n",
      "EPOCH 257:\n",
      "  batch 100 loss: 0.0022830757957905236\n",
      "  batch 200 loss: 0.006073883611012149\n",
      "  batch 300 loss: 0.003866810099997142\n",
      "LOSS train 0.003866810099997142 valid 0.0849690222523488\n",
      "valid accuracy 98.58\n",
      "test accuracy 98.74\n",
      "EPOCH 258:\n",
      "  batch 100 loss: 0.0024578055067070183\n",
      "  batch 200 loss: 0.004222710781979084\n",
      "  batch 300 loss: 0.002430374193601299\n",
      "LOSS train 0.002430374193601299 valid 0.09054384926721928\n",
      "valid accuracy 98.56\n",
      "test accuracy 98.61\n",
      "EPOCH 259:\n",
      "  batch 100 loss: 0.002346128415997626\n",
      "  batch 200 loss: 0.0033118039368218886\n",
      "  batch 300 loss: 0.00391843434886539\n",
      "LOSS train 0.00391843434886539 valid 0.08705359834978112\n",
      "valid accuracy 98.55\n",
      "test accuracy 98.64\n",
      "EPOCH 260:\n",
      "  batch 100 loss: 0.004071679178735081\n",
      "  batch 200 loss: 0.00496543627643689\n",
      "  batch 300 loss: 0.00270204956924168\n",
      "LOSS train 0.00270204956924168 valid 0.08450870728014112\n",
      "valid accuracy 98.7\n",
      "test accuracy 98.6\n",
      "EPOCH 261:\n",
      "  batch 100 loss: 0.006168544143385688\n",
      "  batch 200 loss: 0.0029815811594007613\n",
      "  batch 300 loss: 0.005560143741572788\n",
      "LOSS train 0.005560143741572788 valid 0.0869677962254046\n",
      "valid accuracy 98.69\n",
      "test accuracy 98.63\n",
      "EPOCH 262:\n",
      "  batch 100 loss: 0.003890188739651421\n",
      "  batch 200 loss: 0.002839201198517234\n",
      "  batch 300 loss: 0.0037008120286915157\n",
      "LOSS train 0.0037008120286915157 valid 0.086444909469017\n",
      "valid accuracy 98.62\n",
      "test accuracy 98.57\n",
      "EPOCH 263:\n",
      "  batch 100 loss: 0.0029157772093549284\n",
      "  batch 200 loss: 0.0068375046861640955\n",
      "  batch 300 loss: 0.0046628844606999564\n",
      "LOSS train 0.0046628844606999564 valid 0.08995290197487486\n",
      "valid accuracy 98.6\n",
      "test accuracy 98.58\n",
      "EPOCH 264:\n",
      "  batch 100 loss: 0.0032632721357816765\n",
      "  batch 200 loss: 0.0026712823044938716\n",
      "  batch 300 loss: 0.0061470405142347315\n",
      "LOSS train 0.0061470405142347315 valid 0.09034033939813618\n",
      "valid accuracy 98.63\n",
      "test accuracy 98.61\n",
      "EPOCH 265:\n",
      "  batch 100 loss: 0.0026087739117827626\n",
      "  batch 200 loss: 0.0020887711486745533\n",
      "  batch 300 loss: 0.005311758811911034\n",
      "LOSS train 0.005311758811911034 valid 0.08605153135990977\n",
      "valid accuracy 98.69\n",
      "test accuracy 98.66\n",
      "EPOCH 266:\n",
      "  batch 100 loss: 0.0034392965140656886\n",
      "  batch 200 loss: 0.0020997983967684063\n",
      "  batch 300 loss: 0.0032802854248548383\n",
      "LOSS train 0.0032802854248548383 valid 0.08482335944670093\n",
      "valid accuracy 98.64\n",
      "test accuracy 98.67\n",
      "EPOCH 267:\n",
      "  batch 100 loss: 0.005623628784383641\n",
      "  batch 200 loss: 0.0013487186554788978\n",
      "  batch 300 loss: 0.004857242073082944\n",
      "LOSS train 0.004857242073082944 valid 0.08454926724649524\n",
      "valid accuracy 98.74\n",
      "test accuracy 98.65\n",
      "EPOCH 268:\n",
      "  batch 100 loss: 0.00395749401759069\n",
      "  batch 200 loss: 0.0023806406566755102\n",
      "  batch 300 loss: 0.002446802667591861\n",
      "LOSS train 0.002446802667591861 valid 0.08564032810386667\n",
      "valid accuracy 98.7\n",
      "test accuracy 98.47\n",
      "EPOCH 269:\n",
      "  batch 100 loss: 0.0025946886685753157\n",
      "  batch 200 loss: 0.004016245373861054\n",
      "  batch 300 loss: 0.004988378785375289\n",
      "LOSS train 0.004988378785375289 valid 0.08365583946621732\n",
      "valid accuracy 98.71\n",
      "test accuracy 98.5\n",
      "EPOCH 270:\n",
      "  batch 100 loss: 0.004677978025571505\n",
      "  batch 200 loss: 0.004610184769248349\n",
      "  batch 300 loss: 0.0028234056283821473\n",
      "LOSS train 0.0028234056283821473 valid 0.08297461969617341\n",
      "valid accuracy 98.67\n",
      "test accuracy 98.54\n",
      "EPOCH 271:\n",
      "  batch 100 loss: 0.004549784140144766\n",
      "  batch 200 loss: 0.0024414896119731642\n",
      "  batch 300 loss: 0.004444362962136666\n",
      "LOSS train 0.004444362962136666 valid 0.08669709951693906\n",
      "valid accuracy 98.56\n",
      "test accuracy 98.59\n",
      "EPOCH 272:\n",
      "  batch 100 loss: 0.0049149560004414635\n",
      "  batch 200 loss: 0.004841975679821857\n",
      "  batch 300 loss: 0.003990479840350929\n",
      "LOSS train 0.003990479840350929 valid 0.0908034407386534\n",
      "valid accuracy 98.51\n",
      "test accuracy 98.57\n",
      "EPOCH 273:\n",
      "  batch 100 loss: 0.00282366772124675\n",
      "  batch 200 loss: 0.0019265913723978657\n",
      "  batch 300 loss: 0.003225866439214542\n",
      "LOSS train 0.003225866439214542 valid 0.08978833339822366\n",
      "valid accuracy 98.61\n",
      "test accuracy 98.6\n",
      "EPOCH 274:\n",
      "  batch 100 loss: 0.002937175608187843\n",
      "  batch 200 loss: 0.0036216438355432955\n",
      "  batch 300 loss: 0.0037956282026445367\n",
      "LOSS train 0.0037956282026445367 valid 0.0928833069073431\n",
      "valid accuracy 98.56\n",
      "test accuracy 98.56\n",
      "EPOCH 275:\n",
      "  batch 100 loss: 0.00488976395287402\n",
      "  batch 200 loss: 0.0035509778009865656\n",
      "  batch 300 loss: 0.0035119926345967656\n",
      "LOSS train 0.0035119926345967656 valid 0.08792817144100659\n",
      "valid accuracy 98.65\n",
      "test accuracy 98.64\n",
      "EPOCH 276:\n",
      "  batch 100 loss: 0.002864830194066137\n",
      "  batch 200 loss: 0.006368709993777202\n",
      "  batch 300 loss: 0.0046627586586632215\n",
      "LOSS train 0.0046627586586632215 valid 0.08776083528304977\n",
      "valid accuracy 98.56\n",
      "test accuracy 98.52\n",
      "EPOCH 277:\n",
      "  batch 100 loss: 0.00402190492092231\n",
      "  batch 200 loss: 0.00567014950865314\n",
      "  batch 300 loss: 0.005615779161335582\n",
      "LOSS train 0.005615779161335582 valid 0.08914520035131594\n",
      "valid accuracy 98.62\n",
      "test accuracy 98.62\n",
      "EPOCH 278:\n",
      "  batch 100 loss: 0.005250796242751257\n",
      "  batch 200 loss: 0.002053015361543089\n",
      "  batch 300 loss: 0.002625932733210519\n",
      "LOSS train 0.002625932733210519 valid 0.09042750501879554\n",
      "valid accuracy 98.66\n",
      "test accuracy 98.57\n",
      "EPOCH 279:\n",
      "  batch 100 loss: 0.0019392388546016727\n",
      "  batch 200 loss: 0.004957479489925119\n",
      "  batch 300 loss: 0.004975769611660468\n",
      "LOSS train 0.004975769611660468 valid 0.08653048232621056\n",
      "valid accuracy 98.57\n",
      "test accuracy 98.67\n",
      "EPOCH 280:\n",
      "  batch 100 loss: 0.005769871056477669\n",
      "  batch 200 loss: 0.0040283312405591685\n",
      "  batch 300 loss: 0.004727307340606473\n",
      "LOSS train 0.004727307340606473 valid 0.08633253087743867\n",
      "valid accuracy 98.56\n",
      "test accuracy 98.56\n",
      "EPOCH 281:\n",
      "  batch 100 loss: 0.002218331004407883\n",
      "  batch 200 loss: 0.004907940299906955\n",
      "  batch 300 loss: 0.004503004133341264\n",
      "LOSS train 0.004503004133341264 valid 0.09160196510418855\n",
      "valid accuracy 98.59\n",
      "test accuracy 98.47\n",
      "EPOCH 282:\n",
      "  batch 100 loss: 0.003627304911200966\n",
      "  batch 200 loss: 0.003389521255887473\n",
      "  batch 300 loss: 0.003016126936806103\n",
      "LOSS train 0.003016126936806103 valid 0.09347654190910405\n",
      "valid accuracy 98.51\n",
      "test accuracy 98.49\n",
      "EPOCH 283:\n",
      "  batch 100 loss: 0.004362991192102186\n",
      "  batch 200 loss: 0.0015884629858527433\n",
      "  batch 300 loss: 0.002687581524374991\n",
      "LOSS train 0.002687581524374991 valid 0.08577954496466858\n",
      "valid accuracy 98.66\n",
      "test accuracy 98.6\n",
      "EPOCH 284:\n",
      "  batch 100 loss: 0.004357151804176169\n",
      "  batch 200 loss: 0.00309002122954837\n",
      "  batch 300 loss: 0.0036003511771326656\n",
      "LOSS train 0.0036003511771326656 valid 0.09029078856164682\n",
      "valid accuracy 98.54\n",
      "test accuracy 98.53\n",
      "EPOCH 285:\n",
      "  batch 100 loss: 0.0032528396316642903\n",
      "  batch 200 loss: 0.002987932405926017\n",
      "  batch 300 loss: 0.004525111783243005\n",
      "LOSS train 0.004525111783243005 valid 0.09145375756405821\n",
      "valid accuracy 98.51\n",
      "test accuracy 98.59\n",
      "EPOCH 286:\n",
      "  batch 100 loss: 0.004470304674413655\n",
      "  batch 200 loss: 0.003005087212042099\n",
      "  batch 300 loss: 0.007497460643979253\n",
      "LOSS train 0.007497460643979253 valid 0.08804555862930331\n",
      "valid accuracy 98.55\n",
      "test accuracy 98.66\n",
      "EPOCH 287:\n",
      "  batch 100 loss: 0.00391619691714368\n",
      "  batch 200 loss: 0.0029094722802729222\n",
      "  batch 300 loss: 0.007285599572994777\n",
      "LOSS train 0.007285599572994777 valid 0.08827111421044403\n",
      "valid accuracy 98.58\n",
      "test accuracy 98.68\n",
      "EPOCH 288:\n",
      "  batch 100 loss: 0.0035563593351270128\n",
      "  batch 200 loss: 0.002779569853641988\n",
      "  batch 300 loss: 0.005954535841196957\n",
      "LOSS train 0.005954535841196957 valid 0.0889262660444451\n",
      "valid accuracy 98.57\n",
      "test accuracy 98.64\n",
      "EPOCH 289:\n",
      "  batch 100 loss: 0.00474405316759757\n",
      "  batch 200 loss: 0.005452193826170841\n",
      "  batch 300 loss: 0.004518076590163673\n",
      "LOSS train 0.004518076590163673 valid 0.09279017520085169\n",
      "valid accuracy 98.61\n",
      "test accuracy 98.64\n",
      "EPOCH 290:\n",
      "  batch 100 loss: 0.002076007944238807\n",
      "  batch 200 loss: 0.0035580852583598243\n",
      "  batch 300 loss: 0.0036238931346415626\n",
      "LOSS train 0.0036238931346415626 valid 0.09211059369920709\n",
      "valid accuracy 98.65\n",
      "test accuracy 98.6\n",
      "EPOCH 291:\n",
      "  batch 100 loss: 0.004053164694982172\n",
      "  batch 200 loss: 0.0031355668873189304\n",
      "  batch 300 loss: 0.0032390683191082558\n",
      "LOSS train 0.0032390683191082558 valid 0.09172176918465053\n",
      "valid accuracy 98.66\n",
      "test accuracy 98.61\n",
      "EPOCH 292:\n",
      "  batch 100 loss: 0.0018910147499762787\n",
      "  batch 200 loss: 0.0037103453189300238\n",
      "  batch 300 loss: 0.0031089639683395376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 0.0031089639683395376 valid 0.08741864288748646\n",
      "valid accuracy 98.63\n",
      "test accuracy 98.62\n",
      "EPOCH 293:\n",
      "  batch 100 loss: 0.0015397402107017654\n",
      "  batch 200 loss: 0.004013825914084918\n",
      "  batch 300 loss: 0.0022920359426251125\n",
      "LOSS train 0.0022920359426251125 valid 0.091062734931375\n",
      "valid accuracy 98.61\n",
      "test accuracy 98.67\n",
      "EPOCH 294:\n",
      "  batch 100 loss: 0.004112448734644261\n",
      "  batch 200 loss: 0.002143917722789155\n",
      "  batch 300 loss: 0.0032157401150027455\n",
      "LOSS train 0.0032157401150027455 valid 0.0869808951074981\n",
      "valid accuracy 98.59\n",
      "test accuracy 98.63\n",
      "EPOCH 295:\n",
      "  batch 100 loss: 0.0031534567975737104\n",
      "  batch 200 loss: 0.004698273049840083\n",
      "  batch 300 loss: 0.004142177395186764\n",
      "LOSS train 0.004142177395186764 valid 0.09286024568362199\n",
      "valid accuracy 98.56\n",
      "test accuracy 98.59\n",
      "EPOCH 296:\n",
      "  batch 100 loss: 0.0041458624199516335\n",
      "  batch 200 loss: 0.002320456666852877\n",
      "  batch 300 loss: 0.0028299525227797064\n",
      "LOSS train 0.0028299525227797064 valid 0.0916754729198227\n",
      "valid accuracy 98.61\n",
      "test accuracy 98.58\n",
      "EPOCH 297:\n",
      "  batch 100 loss: 0.003079991750845572\n",
      "  batch 200 loss: 0.002572677135577237\n",
      "  batch 300 loss: 0.002494800483149682\n",
      "LOSS train 0.002494800483149682 valid 0.09219822857368301\n",
      "valid accuracy 98.63\n",
      "test accuracy 98.56\n",
      "EPOCH 298:\n",
      "  batch 100 loss: 0.0018527718231178803\n",
      "  batch 200 loss: 0.003938395844247608\n",
      "  batch 300 loss: 0.0027085058540259866\n",
      "LOSS train 0.0027085058540259866 valid 0.09094594706292337\n",
      "valid accuracy 98.59\n",
      "test accuracy 98.66\n",
      "EPOCH 299:\n",
      "  batch 100 loss: 0.0011310817921815807\n",
      "  batch 200 loss: 0.002785518375052618\n",
      "  batch 300 loss: 0.004582744472833582\n",
      "LOSS train 0.004582744472833582 valid 0.08906520792635679\n",
      "valid accuracy 98.7\n",
      "test accuracy 98.68\n",
      "EPOCH 300:\n",
      "  batch 100 loss: 0.002684370130044016\n",
      "  batch 200 loss: 0.0019482588346413899\n",
      "  batch 300 loss: 0.002870239654694444\n",
      "LOSS train 0.002870239654694444 valid 0.09555201046279325\n",
      "valid accuracy 98.62\n",
      "test accuracy 98.52\n"
     ]
    }
   ],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/MNIST_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "val_loss = []\n",
    "val_accuracy = []\n",
    "\n",
    "test_loss = []\n",
    "test_accuracy = []\n",
    "\n",
    "EPOCHS = 300\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "num_vbatches = int(len(validation_set) / batch_size) + 1\n",
    "vdenom = 2 ** num_vbatches - 1\n",
    "\n",
    "num_tbatches = int(len(test_data) / batch_size) + 1\n",
    "tdenom = 2 ** num_tbatches - 1\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.998)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number, writer)\n",
    "\n",
    "    # We don't need gradients on to do reporting\n",
    "    model.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    correct = 0\n",
    "    for i, vdata in enumerate(validation_loader):\n",
    "        vinputs, vlabels = vdata\n",
    "        vinputs = vinputs.view(-1, 784)\n",
    "        voutputs = model(vinputs)\n",
    "        _, predicted = torch.max(voutputs, 1)\n",
    "        correct += torch.sum(vlabels == predicted)\n",
    "        vloss = ce_loss(voutputs, vlabels)\n",
    "        running_vloss += vloss.item()\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    val_loss.append(avg_vloss)\n",
    "    val_accuracy.append(100 * float(correct)/ len(validation_set))\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "    print('valid accuracy {}'.format(val_accuracy[-1]))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = './models/SGD-dropout/model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    # Update the learning rate\n",
    "    scheduler.step()\n",
    "    \n",
    "    ########## NOT SURE HOW TO CHANGE MOMENTUM LINEARLY ##########################\n",
    "    \n",
    "    running_tloss = 0.0\n",
    "    correct = 0\n",
    "\n",
    "    for i, tdata in enumerate(test_loader):\n",
    "        tinputs, tlabels = tdata\n",
    "        tinputs = tinputs.view(-1, 784)\n",
    "        toutputs = model(tinputs)\n",
    "        _, predicted = torch.max(toutputs, 1)\n",
    "        correct += torch.sum(tlabels == predicted)\n",
    "        tloss = ce_loss(voutputs, vlabels)\n",
    "        running_tloss += tloss.item()\n",
    "\n",
    "    avg_tloss = running_tloss / (i + 1)\n",
    "\n",
    "    test_loss.append(avg_tloss)\n",
    "    test_accuracy.append(100 * float(correct)/ len(test_data))\n",
    "    print('test accuracy {}'.format(test_accuracy[-1]))\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "data['test-loss'] = test_loss\n",
    "data['test-accuracy'] = test_accuracy\n",
    "data['validation-loss'] = val_loss\n",
    "data['validation-accuracy'] = val_accuracy\n",
    "\n",
    "with open('./models/SGD-dropout/test-loss', 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "miTxSzr8jk1R"
   },
   "source": [
    "# Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uaDECpUpjmuw",
    "outputId": "805c8ddf-4252-4a31-83af-acdc31735237"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS test 4.9586291424930096e-05\n",
      "Number of correct predictions 9852\n",
      "Accuracy: 98.52\n"
     ]
    }
   ],
   "source": [
    "model.train(False)\n",
    "\n",
    "running_tloss = 0.0\n",
    "correct = 0\n",
    "\n",
    "for i, tdata in enumerate(test_loader):\n",
    "    tinputs, tlabels = tdata\n",
    "    tinputs = tinputs.view(-1, 784)\n",
    "    toutputs = model(tinputs)\n",
    "    _, predicted = torch.max(toutputs, 1)\n",
    "    correct += torch.sum(tlabels == predicted)\n",
    "    tloss = ce_loss(voutputs, vlabels)\n",
    "    running_tloss += tloss.item()\n",
    "\n",
    "avg_tloss = running_tloss / (i + 1)\n",
    "print('LOSS test {}'.format(avg_tloss))\n",
    "print(\"Number of correct predictions {}\".format(correct))\n",
    "\n",
    "print(\"Accuracy: {:.2f}\".format(100 * float(correct)/ len(test_data)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './models/SGD-dropout/model_{}_{}'.format(timestamp, epoch_number)\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "OHh7JGU9K_ZE"
   },
   "outputs": [],
   "source": [
    "# for step in range(3000):\n",
    "#     pre = model(x)\n",
    "#     ce = ce_loss(pre, y)\n",
    "#     kl = kl_loss(model)\n",
    "#     cost = ce + kl_weight*kl\n",
    "    \n",
    "#     optimizer.zero_grad()\n",
    "#     cost.backward()\n",
    "#     optimizer.step()\n",
    "    \n",
    "# _, predicted = torch.max(pre.data, 1)\n",
    "# total = y.size(0)\n",
    "# correct = (predicted == y).sum()\n",
    "# print('- Accuracy: %f %%' % (100 * float(correct) / total))\n",
    "# print('- CE : %2.2f, KL : %2.2f' % (ce.item(), kl.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HUKaRHNaLWPq"
   },
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "cA6ecKiZLBwG"
   },
   "outputs": [],
   "source": [
    "# def draw_plot(predicted) :\n",
    "#     fig = plt.figure(figsize = (16, 5))\n",
    "\n",
    "#     ax1 = fig.add_subplot(1, 2, 1)\n",
    "#     ax2 = fig.add_subplot(1, 2, 2)\n",
    "\n",
    "#     z1_plot = ax1.scatter(X[:, 0], X[:, 1], c = Y)\n",
    "#     z2_plot = ax2.scatter(X[:, 0], X[:, 1], c = predicted)\n",
    "\n",
    "#     plt.colorbar(z1_plot,ax=ax1)\n",
    "#     plt.colorbar(z2_plot,ax=ax2)\n",
    "\n",
    "#     ax1.set_title(\"REAL\")\n",
    "#     ax2.set_title(\"PREDICT\")\n",
    "\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "JDz-mKj0LGZz"
   },
   "outputs": [],
   "source": [
    "# pre = model(x)\n",
    "# _, predicted = torch.max(pre.data, 1)\n",
    "# draw_plot(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "gUnK7u3ZLJBy"
   },
   "outputs": [],
   "source": [
    "# Bayesian Neural Network will return different outputs even if inputs are same.\n",
    "# In other words, different plots will be shown every time forward method is called.\n",
    "# pre = model(x)\n",
    "# _, predicted = torch.max(pre.data, 1)\n",
    "# draw_plot(predicted)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
